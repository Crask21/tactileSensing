[07/22 16:17:28] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:22:32] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:23:58] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:23:58] vclr INFO: length of training dataset: 48119
[07/22 16:25:20] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:25:21] vclr INFO: length of training dataset: 48119
[07/22 16:26:02] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:26:03] vclr INFO: length of training dataset: 48119
[07/22 16:27:19] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:27:19] vclr INFO: length of training dataset: 48119
[07/22 16:27:21] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 16:31:23] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:31:24] vclr INFO: length of training dataset: 48119
[07/22 16:31:25] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 16:31:25] vclr INFO: DistributedDataParallel enabled.
[07/22 16:31:25] vclr INFO: Distributed Enabled
[07/22 16:31:25] vclr INFO: Training
[07/22 16:33:32] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:33:33] vclr INFO: length of training dataset: 48119
[07/22 16:33:34] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 16:33:34] vclr INFO: DistributedDataParallel enabled.
[07/22 16:33:34] vclr INFO: Distributed Enabled
[07/22 16:33:34] vclr INFO: Training
[07/22 16:34:38] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:34:38] vclr INFO: length of training dataset: 48119
[07/22 16:38:30] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:38:31] vclr INFO: length of training dataset: 48119
[07/22 16:38:32] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 16:38:32] vclr INFO: DistributedDataParallel enabled.
[07/22 16:38:32] vclr INFO: Distributed Enabled
[07/22 16:38:32] vclr INFO: Training
[07/22 16:42:34] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:42:35] vclr INFO: length of training dataset: 48119
[07/22 16:42:36] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 16:42:36] vclr INFO: DistributedDataParallel enabled.
[07/22 16:42:36] vclr INFO: Distributed Enabled
[07/22 16:42:36] vclr INFO: Training
[07/22 16:48:27] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 16:48:27] vclr INFO: length of training dataset: 48119
[07/22 16:48:28] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 16:48:28] vclr INFO: DistributedDataParallel enabled.
[07/22 16:48:28] vclr INFO: Distributed Enabled
[07/22 16:48:28] vclr INFO: Training
[07/22 17:40:31] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 17:40:31] vclr INFO: length of training dataset: 35087
[07/22 17:40:33] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 17:40:33] vclr INFO: DistributedDataParallel enabled.
[07/22 17:40:33] vclr INFO: Distributed Enabled
[07/22 17:40:33] vclr INFO: Training
[07/22 17:42:48] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 17:42:48] vclr INFO: length of training dataset: 35087
[07/22 17:42:49] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 17:42:50] vclr INFO: DistributedDataParallel enabled.
[07/22 17:42:50] vclr INFO: Distributed Enabled
[07/22 17:42:50] vclr INFO: Training
[07/22 17:45:24] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 17:45:24] vclr INFO: length of training dataset: 35087
[07/22 17:45:25] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 17:45:25] vclr INFO: DistributedDataParallel enabled.
[07/22 17:45:25] vclr INFO: Distributed Enabled
[07/22 17:45:25] vclr INFO: Training
[07/22 17:48:39] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 17:48:40] vclr INFO: length of training dataset: 35087
[07/22 17:48:41] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 17:48:41] vclr INFO: DistributedDataParallel enabled.
[07/22 17:48:41] vclr INFO: Distributed Enabled
[07/22 17:48:41] vclr INFO: Training
[07/22 17:53:37] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 17:53:38] vclr INFO: length of training dataset: 35087
[07/22 17:53:39] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 17:53:39] vclr INFO: DistributedDataParallel enabled.
[07/22 17:53:39] vclr INFO: Distributed Enabled
[07/22 17:53:39] vclr INFO: Training
[07/22 17:54:26] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 17:54:26] vclr INFO: length of training dataset: 35087
[07/22 17:54:27] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 17:54:27] vclr INFO: Distributed Enabled
[07/22 17:54:27] vclr INFO: Training
[07/22 18:10:53] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 18:10:54] vclr INFO: length of training dataset: 35087
[07/22 18:10:55] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 18:10:55] vclr INFO: Training
[07/22 18:17:53] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 18:17:53] vclr INFO: length of training dataset: 16730
[07/22 18:17:56] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 18:17:56] vclr INFO: Training
[07/22 18:29:28] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/22 18:29:28] vclr INFO: length of training dataset: 16730
[07/22 18:29:31] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/22 18:29:32] vclr INFO: Training
[07/22 18:35:28] vclr INFO: Train: [  1]/[   0/1045] BT=355.966/355.966 Loss=10.869 5.493 0.333 1.517 3.526/10.869
[07/22 18:36:20] vclr INFO: Train: [  1]/[  10/1045] BT=4.663/37.144 Loss=10.949 5.189 0.594 1.436 3.730/11.110
[07/22 18:37:11] vclr INFO: Train: [  1]/[  20/1045] BT=5.487/21.845 Loss=11.864 5.759 0.392 1.463 4.250/11.161
[07/22 18:37:59] vclr INFO: Train: [  1]/[  30/1045] BT=4.553/16.361 Loss=11.879 5.475 0.602 1.434 4.369/11.145
[07/22 18:38:47] vclr INFO: Train: [  1]/[  40/1045] BT=4.737/13.550 Loss=10.976 5.382 0.521 1.436 3.636/11.207
[07/22 18:39:34] vclr INFO: Train: [  1]/[  50/1045] BT=4.543/11.815 Loss=11.669 5.861 0.490 1.479 3.840/11.257
[07/22 18:40:21] vclr INFO: Train: [  1]/[  60/1045] BT=4.670/10.637 Loss=10.982 5.407 0.426 1.409 3.740/11.214
[07/22 18:41:05] vclr INFO: Train: [  1]/[  70/1045] BT=4.359/9.770 Loss=10.347 4.813 0.778 1.423 3.333/11.182
[07/22 18:41:52] vclr INFO: Train: [  1]/[  80/1045] BT=4.422/9.142 Loss=11.686 6.005 0.334 1.405 3.941/11.193
[07/22 18:42:41] vclr INFO: Train: [  1]/[  90/1045] BT=4.694/8.671 Loss=10.362 5.063 0.385 1.493 3.421/11.177
[07/22 18:43:27] vclr INFO: Train: [  1]/[ 100/1045] BT=4.356/8.265 Loss=11.540 5.904 0.302 1.456 3.878/11.164
[07/22 18:44:13] vclr INFO: Train: [  1]/[ 110/1045] BT=4.439/7.938 Loss=12.035 6.048 0.767 1.505 3.714/11.164
[07/22 18:44:58] vclr INFO: Train: [  1]/[ 120/1045] BT=4.001/7.655 Loss=11.627 5.160 0.432 1.421 4.615/11.176
[07/22 18:45:43] vclr INFO: Train: [  1]/[ 130/1045] BT=4.376/7.413 Loss=11.260 5.608 0.523 1.422 3.707/11.183
[07/22 18:46:27] vclr INFO: Train: [  1]/[ 140/1045] BT=4.562/7.198 Loss=10.936 5.253 0.380 1.407 3.895/11.201
[07/22 18:47:08] vclr INFO: Train: [  1]/[ 150/1045] BT=4.404/6.994 Loss=11.190 5.296 0.673 1.465 3.756/11.228
[07/22 18:47:50] vclr INFO: Train: [  1]/[ 160/1045] BT=4.153/6.819 Loss=11.438 5.576 0.622 1.430 3.810/11.231
[07/22 18:48:33] vclr INFO: Train: [  1]/[ 170/1045] BT=4.252/6.673 Loss=11.659 5.882 0.396 1.299 4.082/11.235
[07/22 18:49:16] vclr INFO: Train: [  1]/[ 180/1045] BT=4.666/6.541 Loss=11.923 5.791 0.565 1.461 4.107/11.251
[07/22 18:49:57] vclr INFO: Train: [  1]/[ 190/1045] BT=3.870/6.417 Loss=11.807 5.919 0.308 1.414 4.165/11.270
[07/22 18:50:40] vclr INFO: Train: [  1]/[ 200/1045] BT=4.500/6.309 Loss=11.713 5.667 0.458 1.430 4.158/11.295
[07/22 18:51:24] vclr INFO: Train: [  1]/[ 210/1045] BT=4.386/6.218 Loss=11.959 6.208 0.394 1.323 4.034/11.327
[07/22 18:52:06] vclr INFO: Train: [  1]/[ 220/1045] BT=4.412/6.127 Loss=12.359 6.221 0.461 1.466 4.211/11.339
[07/22 18:52:50] vclr INFO: Train: [  1]/[ 230/1045] BT=4.550/6.054 Loss=11.953 6.382 0.340 1.438 3.794/11.372
[07/22 18:53:35] vclr INFO: Train: [  1]/[ 240/1045] BT=4.834/5.990 Loss=12.101 6.074 0.417 1.395 4.215/11.387
[07/22 18:54:21] vclr INFO: Train: [  1]/[ 250/1045] BT=4.593/5.933 Loss=11.728 5.957 0.446 1.374 3.951/11.412
[07/22 18:55:06] vclr INFO: Train: [  1]/[ 260/1045] BT=4.480/5.877 Loss=11.623 5.849 0.530 1.347 3.896/11.429
[07/22 18:55:52] vclr INFO: Train: [  1]/[ 270/1045] BT=5.079/5.833 Loss=12.063 6.222 0.326 1.371 4.144/11.445
[07/22 18:56:40] vclr INFO: Train: [  1]/[ 280/1045] BT=5.201/5.796 Loss=12.523 6.611 0.309 1.387 4.216/11.467
[07/22 18:57:29] vclr INFO: Train: [  1]/[ 290/1045] BT=4.628/5.764 Loss=12.600 5.941 0.447 1.445 4.767/11.476
[07/22 18:58:20] vclr INFO: Train: [  1]/[ 300/1045] BT=5.113/5.741 Loss=12.966 6.542 0.321 1.402 4.701/11.508
[07/22 18:59:09] vclr INFO: Train: [  1]/[ 310/1045] BT=5.295/5.716 Loss=12.709 6.349 0.308 1.464 4.588/11.528
[07/22 19:00:00] vclr INFO: Train: [  1]/[ 320/1045] BT=4.880/5.694 Loss=12.596 6.416 0.492 1.372 4.316/11.543
[07/22 19:00:51] vclr INFO: Train: [  1]/[ 330/1045] BT=5.328/5.677 Loss=12.172 6.131 0.531 1.410 4.099/11.565
[07/26 20:04:46] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/26 20:04:46] vclr INFO: length of training dataset: 16730
[07/26 20:04:49] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/26 20:04:50] vclr INFO: Training
[07/26 20:10:16] vclr INFO: Train: [  1]/[   0/1045] BT=326.748/326.748 Loss=9.749 4.276 0.897 1.474 3.102/9.749
[07/26 20:11:02] vclr INFO: Train: [  1]/[  10/1045] BT=4.498/33.878 Loss=11.271 5.616 0.469 1.399 3.787/11.494
[07/26 20:11:43] vclr INFO: Train: [  1]/[  20/1045] BT=3.936/19.678 Loss=11.549 5.802 0.330 1.480 3.938/11.367
[07/26 20:12:27] vclr INFO: Train: [  1]/[  30/1045] BT=3.367/14.758 Loss=10.253 4.872 0.487 1.394 3.499/11.276
[07/26 20:13:09] vclr INFO: Train: [  1]/[  40/1045] BT=4.137/12.177 Loss=10.727 5.557 0.378 1.392 3.401/11.226
[07/26 20:13:49] vclr INFO: Train: [  1]/[  50/1045] BT=4.133/10.567 Loss=10.802 5.446 0.437 1.428 3.491/11.232
[07/26 20:14:33] vclr INFO: Train: [  1]/[  60/1045] BT=3.873/9.555 Loss=10.741 5.538 0.357 1.426 3.419/11.240
[07/26 20:15:17] vclr INFO: Train: [  1]/[  70/1045] BT=4.129/8.829 Loss=10.570 5.105 0.652 1.380 3.433/11.219
[07/26 20:15:58] vclr INFO: Train: [  1]/[  80/1045] BT=4.510/8.249 Loss=11.553 5.534 0.616 1.376 4.027/11.202
[07/26 20:16:40] vclr INFO: Train: [  1]/[  90/1045] BT=4.244/7.807 Loss=10.851 5.365 0.478 1.416 3.592/11.212
[07/26 20:17:23] vclr INFO: Train: [  1]/[ 100/1045] BT=3.914/7.456 Loss=10.887 5.228 0.460 1.465 3.733/11.225
[07/26 20:18:05] vclr INFO: Train: [  1]/[ 110/1045] BT=3.807/7.168 Loss=11.686 5.514 0.480 1.333 4.359/11.224
[07/26 20:18:48] vclr INFO: Train: [  1]/[ 120/1045] BT=3.981/6.924 Loss=11.279 5.730 0.393 1.374 3.781/11.241
[07/26 20:19:31] vclr INFO: Train: [  1]/[ 130/1045] BT=4.906/6.730 Loss=12.100 5.931 0.470 1.413 4.285/11.264
[07/26 20:20:13] vclr INFO: Train: [  1]/[ 140/1045] BT=3.816/6.548 Loss=11.478 5.798 0.347 1.309 4.024/11.284
[07/26 20:20:55] vclr INFO: Train: [  1]/[ 150/1045] BT=4.316/6.391 Loss=11.102 5.468 0.516 1.400 3.719/11.295
[07/26 20:21:37] vclr INFO: Train: [  1]/[ 160/1045] BT=3.831/6.254 Loss=11.234 5.665 0.399 1.349 3.822/11.292
[07/26 20:22:21] vclr INFO: Train: [  1]/[ 170/1045] BT=4.341/6.148 Loss=11.894 5.944 0.490 1.380 4.080/11.308
[07/26 20:23:04] vclr INFO: Train: [  1]/[ 180/1045] BT=3.649/6.045 Loss=12.320 6.318 0.404 1.387 4.210/11.325
[07/26 20:23:47] vclr INFO: Train: [  1]/[ 190/1045] BT=4.056/5.956 Loss=11.534 5.976 0.361 1.393 3.803/11.331
[07/26 20:24:30] vclr INFO: Train: [  1]/[ 200/1045] BT=4.169/5.872 Loss=11.665 5.658 0.528 1.472 4.006/11.350
[07/26 20:25:11] vclr INFO: Train: [  1]/[ 210/1045] BT=4.152/5.790 Loss=11.476 5.837 0.478 1.367 3.795/11.359
[07/26 20:25:55] vclr INFO: Train: [  1]/[ 220/1045] BT=4.608/5.727 Loss=12.540 6.431 0.419 1.383 4.306/11.388
[07/26 20:26:37] vclr INFO: Train: [  1]/[ 230/1045] BT=3.913/5.660 Loss=11.612 5.707 0.522 1.443 3.939/11.411
[07/26 20:27:23] vclr INFO: Train: [  1]/[ 240/1045] BT=4.612/5.614 Loss=11.309 5.460 0.392 1.450 4.006/11.433
[07/26 20:28:09] vclr INFO: Train: [  1]/[ 250/1045] BT=5.251/5.576 Loss=11.469 5.708 0.453 1.364 3.944/11.459
[07/26 20:28:53] vclr INFO: Train: [  1]/[ 260/1045] BT=4.139/5.530 Loss=11.503 5.520 0.519 1.380 4.084/11.477
[07/26 20:29:39] vclr INFO: Train: [  1]/[ 270/1045] BT=4.202/5.495 Loss=11.701 5.779 0.462 1.431 4.028/11.491
[07/26 20:30:22] vclr INFO: Train: [  1]/[ 280/1045] BT=4.428/5.453 Loss=11.145 5.461 0.495 1.348 3.840/11.496
[07/26 20:31:08] vclr INFO: Train: [  1]/[ 290/1045] BT=3.905/5.425 Loss=11.536 5.691 0.437 1.336 4.072/11.509
[07/26 20:31:55] vclr INFO: Train: [  1]/[ 300/1045] BT=4.601/5.400 Loss=12.083 6.480 0.355 1.341 3.906/11.526
[07/26 20:32:37] vclr INFO: Train: [  1]/[ 310/1045] BT=4.898/5.361 Loss=11.953 5.834 0.661 1.381 4.077/11.541
[07/26 20:33:22] vclr INFO: Train: [  1]/[ 320/1045] BT=5.051/5.336 Loss=11.909 5.903 0.441 1.434 4.132/11.564
[07/26 20:34:08] vclr INFO: Train: [  1]/[ 330/1045] BT=3.665/5.312 Loss=12.383 6.406 0.317 1.460 4.200/11.587
[07/26 20:34:50] vclr INFO: Train: [  1]/[ 340/1045] BT=4.581/5.279 Loss=11.373 5.728 0.436 1.356 3.854/11.601
[07/26 21:11:39] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/26 21:11:39] vclr INFO: length of training dataset: 16728
[07/26 21:11:45] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/26 21:11:46] vclr INFO: Training
[07/26 21:19:39] vclr INFO: Train: [  1]/[   0/1045] BT=473.564/473.564 Loss=9.696 4.579 0.517 1.411 3.190/9.696
[07/26 21:20:39] vclr INFO: Train: [  1]/[  10/1045] BT=4.720/48.452 Loss=11.210 5.625 0.384 1.255 3.947/11.518
[07/26 22:39:12] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/26 22:39:12] vclr INFO: length of training dataset: 4592
[07/26 22:39:16] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/26 22:39:16] vclr INFO: Training
[07/26 22:45:42] vclr INFO: Train: [  1]/[   0/ 287] BT=386.385/386.385 Loss=9.962 4.891 0.626 1.458 2.988/9.962
[07/26 22:46:22] vclr INFO: Train: [  1]/[  10/ 287] BT=3.569/38.757 Loss=11.526 5.188 0.682 1.421 4.235/11.738
[07/26 22:47:02] vclr INFO: Train: [  1]/[  20/ 287] BT=4.529/22.207 Loss=12.224 6.063 0.303 1.399 4.458/11.773
[07/26 22:47:41] vclr INFO: Train: [  1]/[  30/ 287] BT=3.563/16.284 Loss=12.252 6.203 0.226 1.393 4.430/11.802
[07/26 22:48:19] vclr INFO: Train: [  1]/[  40/ 287] BT=3.717/13.242 Loss=11.639 5.547 0.291 1.551 4.251/11.785
[07/26 22:48:56] vclr INFO: Train: [  1]/[  50/ 287] BT=3.598/11.368 Loss=12.200 6.247 0.329 1.384 4.241/11.724
[07/26 22:49:33] vclr INFO: Train: [  1]/[  60/ 287] BT=3.680/10.111 Loss=10.808 5.301 0.447 1.383 3.677/11.700
[07/26 22:50:08] vclr INFO: Train: [  1]/[  70/ 287] BT=3.380/9.185 Loss=10.551 5.012 0.538 1.511 3.490/11.687
[07/26 22:50:46] vclr INFO: Train: [  1]/[  80/ 287] BT=3.400/8.515 Loss=11.492 5.523 0.556 1.505 3.908/11.689
[07/26 22:51:24] vclr INFO: Train: [  1]/[  90/ 287] BT=3.660/8.003 Loss=12.019 6.032 0.458 1.420 4.108/11.695
[07/26 22:52:00] vclr INFO: Train: [  1]/[ 100/ 287] BT=3.378/7.560 Loss=12.010 6.112 0.448 1.351 4.099/11.709
[07/26 22:52:35] vclr INFO: Train: [  1]/[ 110/ 287] BT=3.433/7.200 Loss=12.389 6.045 0.564 1.497 4.283/11.743
[07/26 22:53:14] vclr INFO: Train: [  1]/[ 120/ 287] BT=3.372/6.924 Loss=11.958 6.017 0.436 1.338 4.167/11.772
[07/26 22:53:52] vclr INFO: Train: [  1]/[ 130/ 287] BT=3.728/6.687 Loss=12.489 6.268 0.527 1.553 4.141/11.786
[07/26 22:54:29] vclr INFO: Train: [  1]/[ 140/ 287] BT=3.910/6.476 Loss=11.657 5.506 0.452 1.482 4.218/11.825
[07/26 22:55:05] vclr INFO: Train: [  1]/[ 150/ 287] BT=3.842/6.285 Loss=12.281 6.159 0.436 1.471 4.216/11.873
[07/26 22:55:41] vclr INFO: Train: [  1]/[ 160/ 287] BT=3.581/6.118 Loss=12.318 5.963 0.564 1.482 4.309/11.904
[07/26 22:56:18] vclr INFO: Train: [  1]/[ 170/ 287] BT=3.642/5.976 Loss=12.990 6.210 0.544 1.479 4.757/11.971
[07/26 22:56:55] vclr INFO: Train: [  1]/[ 180/ 287] BT=3.920/5.848 Loss=12.547 6.299 0.449 1.491 4.309/12.010
[07/26 22:57:30] vclr INFO: Train: [  1]/[ 190/ 287] BT=3.276/5.728 Loss=12.454 6.524 0.374 1.438 4.118/12.034
[07/26 22:58:07] vclr INFO: Train: [  1]/[ 200/ 287] BT=3.899/5.626 Loss=12.552 6.013 0.511 1.315 4.714/12.077
[07/26 22:58:45] vclr INFO: Train: [  1]/[ 210/ 287] BT=3.796/5.543 Loss=12.166 5.827 0.476 1.474 4.389/12.104
[07/26 22:59:22] vclr INFO: Train: [  1]/[ 220/ 287] BT=3.909/5.458 Loss=12.986 6.055 0.536 1.538 4.858/12.130
[07/26 22:59:59] vclr INFO: Train: [  1]/[ 230/ 287] BT=3.760/5.381 Loss=13.176 6.974 0.298 1.206 4.698/12.169
[07/26 23:00:35] vclr INFO: Train: [  1]/[ 240/ 287] BT=3.845/5.308 Loss=12.894 6.394 0.425 1.415 4.659/12.221
[07/26 23:01:11] vclr INFO: Train: [  1]/[ 250/ 287] BT=3.478/5.239 Loss=12.213 5.814 0.466 1.365 4.567/12.257
[07/26 23:01:46] vclr INFO: Train: [  1]/[ 260/ 287] BT=3.568/5.172 Loss=12.110 5.931 0.398 1.419 4.361/12.280
[07/26 23:02:19] vclr INFO: Train: [  1]/[ 270/ 287] BT=3.485/5.105 Loss=12.899 6.362 0.395 1.437 4.705/12.298
[07/26 23:02:53] vclr INFO: Train: [  1]/[ 280/ 287] BT=3.426/5.042 Loss=13.741 6.826 0.346 1.497 5.071/12.319
[07/26 23:03:42] vclr INFO: epoch 1, total time 1466.26, loss=12.340874828112666
[07/26 23:03:42] vclr INFO: ==> Saving...
[07/26 23:06:58] vclr INFO: Train: [  2]/[   0/ 287] BT=190.541/190.541 Loss=13.012 6.454 0.367 1.360 4.831/13.012
[07/26 23:07:31] vclr INFO: Train: [  2]/[  10/ 287] BT=3.359/20.296 Loss=12.410 6.343 0.442 1.351 4.273/13.033
[07/26 23:08:04] vclr INFO: Train: [  2]/[  20/ 287] BT=3.152/12.222 Loss=13.083 6.678 0.447 1.504 4.455/13.096
[07/26 23:08:36] vclr INFO: Train: [  2]/[  30/ 287] BT=3.291/9.299 Loss=13.538 6.637 0.599 1.514 4.788/13.037
[07/26 23:09:11] vclr INFO: Train: [  2]/[  40/ 287] BT=3.254/7.881 Loss=13.050 6.269 0.535 1.506 4.740/13.078
[07/26 23:09:45] vclr INFO: Train: [  2]/[  50/ 287] BT=3.481/7.003 Loss=13.152 6.478 0.500 1.406 4.769/13.155
[07/26 23:10:20] vclr INFO: Train: [  2]/[  60/ 287] BT=3.175/6.431 Loss=12.976 6.622 0.363 1.336 4.656/13.188
[07/26 23:10:51] vclr INFO: Train: [  2]/[  70/ 287] BT=3.114/5.966 Loss=13.187 6.571 0.393 1.457 4.765/13.222
[07/26 23:11:24] vclr INFO: Train: [  2]/[  80/ 287] BT=3.590/5.639 Loss=14.138 7.415 0.286 1.530 4.907/13.286
[07/26 23:11:57] vclr INFO: Train: [  2]/[  90/ 287] BT=3.117/5.379 Loss=13.929 6.917 0.334 1.432 5.245/13.348
[07/26 23:12:29] vclr INFO: Train: [  2]/[ 100/ 287] BT=3.499/5.165 Loss=12.783 6.057 0.543 1.436 4.747/13.410
[07/26 23:13:01] vclr INFO: Train: [  2]/[ 110/ 287] BT=3.842/4.986 Loss=13.806 6.743 0.444 1.525 5.093/13.437
[07/26 23:13:36] vclr INFO: Train: [  2]/[ 120/ 287] BT=3.158/4.864 Loss=14.165 7.388 0.370 1.333 5.074/13.471
[07/26 23:14:09] vclr INFO: Train: [  2]/[ 130/ 287] BT=2.489/4.747 Loss=13.909 6.630 0.477 1.446 5.356/13.491
[07/26 23:14:40] vclr INFO: Train: [  2]/[ 140/ 287] BT=2.595/4.626 Loss=14.498 7.148 0.546 1.592 5.212/13.526
[07/26 23:15:15] vclr INFO: Train: [  2]/[ 150/ 287] BT=3.788/4.555 Loss=13.487 6.544 0.489 1.367 5.087/13.553
[07/26 23:15:48] vclr INFO: Train: [  2]/[ 160/ 287] BT=2.900/4.478 Loss=14.016 7.304 0.287 1.383 5.041/13.565
[07/26 23:16:21] vclr INFO: Train: [  2]/[ 170/ 287] BT=3.553/4.409 Loss=13.086 6.373 0.480 1.326 4.906/13.577
[07/26 23:16:55] vclr INFO: Train: [  2]/[ 180/ 287] BT=3.334/4.350 Loss=14.362 7.066 0.368 1.357 5.571/13.595
[07/26 23:17:27] vclr INFO: Train: [  2]/[ 190/ 287] BT=3.730/4.292 Loss=13.648 6.859 0.220 1.539 5.030/13.614
[07/26 23:18:00] vclr INFO: Train: [  2]/[ 200/ 287] BT=3.113/4.242 Loss=12.679 6.222 0.452 1.321 4.683/13.627
[07/26 23:18:35] vclr INFO: Train: [  2]/[ 210/ 287] BT=3.324/4.208 Loss=13.336 6.427 0.441 1.425 5.044/13.629
[07/26 23:19:08] vclr INFO: Train: [  2]/[ 220/ 287] BT=2.955/4.167 Loss=14.032 6.987 0.492 1.384 5.169/13.636
[07/26 23:19:39] vclr INFO: Train: [  2]/[ 230/ 287] BT=3.162/4.118 Loss=12.914 6.563 0.421 1.245 4.686/13.641
[07/26 23:20:11] vclr INFO: Train: [  2]/[ 240/ 287] BT=3.605/4.080 Loss=13.355 6.314 0.460 1.635 4.947/13.651
[07/26 23:20:43] vclr INFO: Train: [  2]/[ 250/ 287] BT=2.908/4.048 Loss=13.738 6.641 0.626 1.441 5.030/13.663
[07/26 23:21:18] vclr INFO: Train: [  2]/[ 260/ 287] BT=3.163/4.024 Loss=14.637 7.291 0.424 1.624 5.297/13.677
[07/26 23:21:51] vclr INFO: Train: [  2]/[ 270/ 287] BT=3.043/3.999 Loss=13.596 6.552 0.339 1.492 5.213/13.687
[07/26 23:22:25] vclr INFO: Train: [  2]/[ 280/ 287] BT=3.584/3.979 Loss=13.114 6.758 0.377 1.320 4.659/13.702
[07/26 23:22:49] vclr INFO: epoch 2, total time 1146.35, loss=13.709848782742066
[07/26 23:22:49] vclr INFO: ==> Saving...
[07/26 23:23:47] vclr INFO: Train: [  3]/[   0/ 287] BT=56.437/56.437 Loss=14.247 7.305 0.314 1.587 5.042/14.247
[07/26 23:24:23] vclr INFO: Train: [  3]/[  10/ 287] BT=3.536/8.323 Loss=14.413 6.820 0.678 1.365 5.549/14.044
[07/26 23:24:57] vclr INFO: Train: [  3]/[  20/ 287] BT=3.249/5.982 Loss=13.537 6.782 0.358 1.523 4.875/14.013
[07/26 23:25:31] vclr INFO: Train: [  3]/[  30/ 287] BT=3.551/5.171 Loss=13.884 6.845 0.533 1.642 4.864/13.996
[07/26 23:26:06] vclr INFO: Train: [  3]/[  40/ 287] BT=3.894/4.747 Loss=14.462 7.278 0.503 1.488 5.193/13.978
[07/26 23:26:43] vclr INFO: Train: [  3]/[  50/ 287] BT=3.667/4.551 Loss=14.438 7.136 0.476 1.614 5.212/13.985
[07/26 23:27:20] vclr INFO: Train: [  3]/[  60/ 287] BT=3.673/4.407 Loss=14.051 6.724 0.543 1.649 5.134/13.999
[07/26 23:27:57] vclr INFO: Train: [  3]/[  70/ 287] BT=3.309/4.312 Loss=14.431 6.888 0.553 1.573 5.417/14.021
[07/26 23:28:34] vclr INFO: Train: [  3]/[  80/ 287] BT=4.152/4.238 Loss=14.143 6.796 0.516 1.542 5.290/14.028
[07/26 23:29:10] vclr INFO: Train: [  3]/[  90/ 287] BT=2.858/4.160 Loss=14.357 7.155 0.400 1.577 5.225/14.064
[07/26 23:29:47] vclr INFO: Train: [  3]/[ 100/ 287] BT=4.338/4.119 Loss=14.403 7.080 0.408 1.447 5.467/14.063
[07/26 23:30:24] vclr INFO: Train: [  3]/[ 110/ 287] BT=3.816/4.082 Loss=14.045 6.889 0.402 1.310 5.444/14.095
[07/26 23:30:59] vclr INFO: Train: [  3]/[ 120/ 287] BT=3.488/4.029 Loss=14.428 7.222 0.513 1.289 5.404/14.106
[07/26 23:31:34] vclr INFO: Train: [  3]/[ 130/ 287] BT=3.191/3.993 Loss=14.610 7.156 0.434 1.426 5.595/14.121
[07/26 23:32:08] vclr INFO: Train: [  3]/[ 140/ 287] BT=3.484/3.953 Loss=15.118 6.945 0.705 1.603 5.865/14.122
[07/26 23:32:42] vclr INFO: Train: [  3]/[ 150/ 287] BT=4.074/3.916 Loss=14.267 6.796 0.521 1.501 5.448/14.153
[07/26 23:33:18] vclr INFO: Train: [  3]/[ 160/ 287] BT=3.911/3.894 Loss=13.555 6.475 0.486 1.533 5.061/14.153
[07/26 23:33:52] vclr INFO: Train: [  3]/[ 170/ 287] BT=3.550/3.866 Loss=14.423 7.247 0.466 1.411 5.299/14.171
[07/26 23:34:25] vclr INFO: Train: [  3]/[ 180/ 287] BT=3.435/3.837 Loss=14.323 7.084 0.476 1.329 5.434/14.177
[07/26 23:34:59] vclr INFO: Train: [  3]/[ 190/ 287] BT=3.369/3.811 Loss=13.708 6.709 0.464 1.556 4.978/14.189
[07/26 23:35:35] vclr INFO: Train: [  3]/[ 200/ 287] BT=3.966/3.801 Loss=14.307 7.165 0.425 1.451 5.266/14.204
[07/26 23:36:11] vclr INFO: Train: [  3]/[ 210/ 287] BT=3.822/3.793 Loss=14.163 6.922 0.399 1.557 5.285/14.206
[07/26 23:36:46] vclr INFO: Train: [  3]/[ 220/ 287] BT=3.181/3.779 Loss=14.555 7.574 0.315 1.375 5.291/14.206
[07/26 23:37:22] vclr INFO: Train: [  3]/[ 230/ 287] BT=3.699/3.772 Loss=14.938 7.578 0.482 1.743 5.134/14.214
[07/26 23:37:56] vclr INFO: Train: [  3]/[ 240/ 287] BT=3.301/3.757 Loss=13.852 6.734 0.633 1.377 5.108/14.227
[07/26 23:38:32] vclr INFO: Train: [  3]/[ 250/ 287] BT=3.621/3.751 Loss=14.629 7.349 0.309 1.534 5.437/14.240
[07/26 23:39:09] vclr INFO: Train: [  3]/[ 260/ 287] BT=3.454/3.748 Loss=14.686 7.570 0.348 1.292 5.476/14.257
[07/26 23:39:45] vclr INFO: Train: [  3]/[ 270/ 287] BT=3.253/3.741 Loss=14.345 7.122 0.492 1.370 5.361/14.260
[07/26 23:40:20] vclr INFO: Train: [  3]/[ 280/ 287] BT=3.602/3.732 Loss=14.833 7.253 0.526 1.531 5.524/14.268
[07/26 23:40:45] vclr INFO: epoch 3, total time 1076.49, loss=14.270727267248706
[07/26 23:40:45] vclr INFO: ==> Saving...
[07/26 23:41:44] vclr INFO: Train: [  4]/[   0/ 287] BT=57.572/57.572 Loss=14.458 7.107 0.510 1.385 5.455/14.458
[07/26 23:42:22] vclr INFO: Train: [  4]/[  10/ 287] BT=4.083/8.654 Loss=14.922 7.170 0.372 1.914 5.466/14.436
[07/26 23:42:59] vclr INFO: Train: [  4]/[  20/ 287] BT=3.851/6.293 Loss=12.997 6.447 0.434 1.272 4.844/14.238
[07/26 23:43:37] vclr INFO: Train: [  4]/[  30/ 287] BT=4.783/5.487 Loss=13.813 6.528 0.542 1.505 5.238/14.296
[07/26 23:44:13] vclr INFO: Train: [  4]/[  40/ 287] BT=3.603/5.026 Loss=14.960 7.352 0.487 1.494 5.627/14.354
[07/26 23:44:51] vclr INFO: Train: [  4]/[  50/ 287] BT=3.750/4.790 Loss=14.784 7.066 0.554 1.722 5.443/14.362
[07/26 23:45:28] vclr INFO: Train: [  4]/[  60/ 287] BT=3.242/4.617 Loss=14.145 6.788 0.481 1.478 5.399/14.337
[07/26 23:46:06] vclr INFO: Train: [  4]/[  70/ 287] BT=3.913/4.500 Loss=15.027 7.121 0.635 1.695 5.576/14.362
[07/26 23:46:42] vclr INFO: Train: [  4]/[  80/ 287] BT=4.145/4.389 Loss=14.909 7.052 0.530 1.920 5.407/14.364
[07/26 23:47:19] vclr INFO: Train: [  4]/[  90/ 287] BT=3.666/4.305 Loss=15.125 7.485 0.829 1.291 5.520/14.395
[07/26 23:47:54] vclr INFO: Train: [  4]/[ 100/ 287] BT=3.120/4.227 Loss=14.526 7.267 0.543 1.559 5.157/14.413
[07/26 23:48:28] vclr INFO: Train: [  4]/[ 110/ 287] BT=3.480/4.156 Loss=15.163 7.678 0.375 1.519 5.591/14.409
[07/26 23:49:05] vclr INFO: Train: [  4]/[ 120/ 287] BT=3.844/4.115 Loss=13.958 7.095 0.428 1.369 5.066/14.420
[07/26 23:49:42] vclr INFO: Train: [  4]/[ 130/ 287] BT=3.600/4.085 Loss=14.313 7.200 0.476 1.380 5.258/14.430
[07/26 23:50:16] vclr INFO: Train: [  4]/[ 140/ 287] BT=3.125/4.037 Loss=15.523 7.359 0.621 1.708 5.835/14.440
[07/26 23:50:50] vclr INFO: Train: [  4]/[ 150/ 287] BT=3.276/3.996 Loss=14.230 6.908 0.619 1.372 5.331/14.421
[07/26 23:51:28] vclr INFO: Train: [  4]/[ 160/ 287] BT=3.193/3.982 Loss=13.955 7.093 0.434 1.456 4.973/14.396
[07/26 23:52:07] vclr INFO: Train: [  4]/[ 170/ 287] BT=5.719/3.977 Loss=14.094 6.894 0.288 1.585 5.328/14.390
[07/26 23:52:43] vclr INFO: Train: [  4]/[ 180/ 287] BT=3.536/3.956 Loss=14.789 7.063 0.555 1.608 5.562/14.389
[07/26 23:53:21] vclr INFO: Train: [  4]/[ 190/ 287] BT=3.581/3.946 Loss=14.339 6.850 0.570 1.535 5.384/14.402
[07/26 23:53:57] vclr INFO: Train: [  4]/[ 200/ 287] BT=3.522/3.933 Loss=14.035 6.771 0.308 1.940 5.016/14.400
[07/26 23:54:30] vclr INFO: Train: [  4]/[ 210/ 287] BT=3.433/3.902 Loss=14.191 6.998 0.412 1.455 5.327/14.410
[07/26 23:55:05] vclr INFO: Train: [  4]/[ 220/ 287] BT=3.436/3.882 Loss=14.317 7.041 0.355 1.543 5.377/14.428
[07/26 23:55:41] vclr INFO: Train: [  4]/[ 230/ 287] BT=3.072/3.872 Loss=14.377 7.186 0.267 1.760 5.165/14.428
[07/26 23:56:16] vclr INFO: Train: [  4]/[ 240/ 287] BT=2.975/3.855 Loss=14.881 7.211 0.592 1.534 5.545/14.437
[07/26 23:56:51] vclr INFO: Train: [  4]/[ 250/ 287] BT=3.344/3.840 Loss=14.302 6.902 0.334 1.237 5.830/14.441
[07/26 23:57:24] vclr INFO: Train: [  4]/[ 260/ 287] BT=3.086/3.822 Loss=13.767 6.767 0.540 1.825 4.635/14.449
[07/26 23:57:56] vclr INFO: Train: [  4]/[ 270/ 287] BT=3.288/3.799 Loss=14.295 7.113 0.335 1.585 5.261/14.450
[07/26 23:58:32] vclr INFO: Train: [  4]/[ 280/ 287] BT=3.844/3.789 Loss=15.755 7.878 0.344 1.498 6.035/14.448
[07/26 23:58:54] vclr INFO: epoch 4, total time 1088.56, loss=14.452850777097696
[07/26 23:58:54] vclr INFO: ==> Saving...
[07/26 23:59:48] vclr INFO: Train: [  5]/[   0/ 287] BT=53.203/53.203 Loss=14.043 6.690 0.600 1.604 5.149/14.043
[07/27 00:00:22] vclr INFO: Train: [  5]/[  10/ 287] BT=3.854/7.874 Loss=13.473 6.651 0.459 1.345 5.018/14.276
[07/27 00:00:57] vclr INFO: Train: [  5]/[  20/ 287] BT=3.577/5.828 Loss=15.297 7.381 0.380 1.899 5.637/14.500
[07/27 00:01:34] vclr INFO: Train: [  5]/[  30/ 287] BT=4.117/5.144 Loss=13.801 6.803 0.424 1.413 5.162/14.404
[07/27 00:02:07] vclr INFO: Train: [  5]/[  40/ 287] BT=3.273/4.691 Loss=13.851 6.565 0.467 1.777 5.042/14.400
[07/27 00:02:40] vclr INFO: Train: [  5]/[  50/ 287] BT=3.838/4.407 Loss=13.231 6.349 0.638 1.303 4.941/14.330
[07/27 00:03:15] vclr INFO: Train: [  5]/[  60/ 287] BT=3.333/4.264 Loss=12.989 6.336 0.494 1.302 4.857/14.333
[07/27 00:03:49] vclr INFO: Train: [  5]/[  70/ 287] BT=3.851/4.142 Loss=14.966 7.291 0.397 1.469 5.809/14.368
[07/27 00:04:26] vclr INFO: Train: [  5]/[  80/ 287] BT=3.559/4.084 Loss=14.626 7.287 0.323 1.832 5.183/14.407
[07/27 00:05:01] vclr INFO: Train: [  5]/[  90/ 287] BT=3.506/4.017 Loss=14.232 7.015 0.337 1.500 5.380/14.410
[07/27 00:05:38] vclr INFO: Train: [  5]/[ 100/ 287] BT=3.439/3.986 Loss=15.124 7.323 0.449 1.795 5.558/14.419
[07/27 00:06:11] vclr INFO: Train: [  5]/[ 110/ 287] BT=3.359/3.928 Loss=15.258 7.657 0.485 1.438 5.678/14.437
[07/27 00:06:44] vclr INFO: Train: [  5]/[ 120/ 287] BT=3.557/3.877 Loss=13.091 6.417 0.476 1.361 4.837/14.404
[07/27 00:07:20] vclr INFO: Train: [  5]/[ 130/ 287] BT=3.894/3.855 Loss=14.206 6.705 0.615 1.940 4.946/14.394
[07/27 00:07:55] vclr INFO: Train: [  5]/[ 140/ 287] BT=3.697/3.833 Loss=14.176 6.828 0.513 1.677 5.158/14.400
[07/27 00:08:29] vclr INFO: Train: [  5]/[ 150/ 287] BT=3.253/3.804 Loss=14.569 7.004 0.409 2.037 5.119/14.384
[07/27 00:09:04] vclr INFO: Train: [  5]/[ 160/ 287] BT=3.008/3.784 Loss=15.241 7.703 0.502 1.766 5.270/14.392
[07/27 00:09:38] vclr INFO: Train: [  5]/[ 170/ 287] BT=3.334/3.763 Loss=14.365 7.001 0.591 1.634 5.140/14.374
[07/27 00:10:13] vclr INFO: Train: [  5]/[ 180/ 287] BT=3.212/3.744 Loss=15.396 7.214 0.525 1.826 5.831/14.374
[07/27 00:10:46] vclr INFO: Train: [  5]/[ 190/ 287] BT=3.212/3.721 Loss=14.538 6.841 0.369 1.753 5.576/14.369
[07/27 00:11:20] vclr INFO: Train: [  5]/[ 200/ 287] BT=3.534/3.708 Loss=14.633 7.015 0.478 1.583 5.557/14.359
[07/27 00:11:55] vclr INFO: Train: [  5]/[ 210/ 287] BT=3.539/3.698 Loss=14.076 6.928 0.398 1.528 5.222/14.358
[07/27 00:12:31] vclr INFO: Train: [  5]/[ 220/ 287] BT=3.353/3.692 Loss=13.101 6.531 0.361 1.429 4.779/14.322
[07/27 00:13:04] vclr INFO: Train: [  5]/[ 230/ 287] BT=3.003/3.676 Loss=13.984 6.841 0.453 1.658 5.032/14.308
[07/27 00:13:40] vclr INFO: Train: [  5]/[ 240/ 287] BT=3.127/3.671 Loss=14.138 7.198 0.409 1.237 5.294/14.304
[07/27 00:14:15] vclr INFO: Train: [  5]/[ 250/ 287] BT=3.139/3.666 Loss=15.328 7.412 0.516 1.904 5.496/14.295
[07/27 00:14:46] vclr INFO: Train: [  5]/[ 260/ 287] BT=2.807/3.644 Loss=14.130 6.780 0.456 1.659 5.235/14.289
[07/27 00:15:21] vclr INFO: Train: [  5]/[ 270/ 287] BT=3.782/3.637 Loss=14.149 6.694 0.428 1.940 5.087/14.293
[07/27 00:15:55] vclr INFO: Train: [  5]/[ 280/ 287] BT=2.978/3.629 Loss=14.919 7.872 0.496 1.319 5.232/14.305
[07/27 00:16:18] vclr INFO: epoch 5, total time 1044.04, loss=14.307848641265975
[07/27 00:16:18] vclr INFO: ==> Saving...
[07/27 00:17:16] vclr INFO: Train: [  6]/[   0/ 287] BT=56.084/56.084 Loss=13.566 6.641 0.405 1.500 5.020/13.566
[07/27 00:17:55] vclr INFO: Train: [  6]/[  10/ 287] BT=3.499/8.649 Loss=14.361 6.958 0.340 1.520 5.543/14.371
[07/27 00:18:34] vclr INFO: Train: [  6]/[  20/ 287] BT=4.009/6.399 Loss=14.267 7.310 0.391 1.538 5.028/14.264
[07/27 00:19:12] vclr INFO: Train: [  6]/[  30/ 287] BT=3.984/5.560 Loss=14.355 7.321 0.304 1.568 5.162/14.194
[07/27 00:19:51] vclr INFO: Train: [  6]/[  40/ 287] BT=3.463/5.154 Loss=13.509 6.773 0.321 1.415 5.000/14.089
[07/27 00:20:27] vclr INFO: Train: [  6]/[  50/ 287] BT=3.275/4.853 Loss=13.687 6.751 0.523 1.569 4.844/14.009
[07/27 00:21:03] vclr INFO: Train: [  6]/[  60/ 287] BT=3.266/4.642 Loss=14.515 6.911 0.544 1.585 5.476/14.027
[07/27 00:21:40] vclr INFO: Train: [  6]/[  70/ 287] BT=3.969/4.519 Loss=14.425 6.979 0.539 1.453 5.454/14.071
[07/27 00:22:19] vclr INFO: Train: [  6]/[  80/ 287] BT=3.694/4.443 Loss=14.010 6.941 0.513 1.375 5.181/14.042
[07/27 00:22:58] vclr INFO: Train: [  6]/[  90/ 287] BT=3.877/4.380 Loss=15.323 7.776 0.410 1.527 5.610/14.042
[07/27 00:23:35] vclr INFO: Train: [  6]/[ 100/ 287] BT=3.539/4.315 Loss=14.721 7.015 0.359 1.392 5.954/14.035
[07/27 00:24:16] vclr INFO: Train: [  6]/[ 110/ 287] BT=3.634/4.291 Loss=14.862 7.075 0.465 1.878 5.444/14.043
[07/27 00:24:58] vclr INFO: Train: [  6]/[ 120/ 287] BT=4.293/4.283 Loss=13.723 6.612 0.617 1.528 4.966/14.050
[07/27 00:25:40] vclr INFO: Train: [  6]/[ 130/ 287] BT=4.205/4.278 Loss=14.053 6.462 0.459 2.210 4.922/14.055
[07/27 00:26:17] vclr INFO: Train: [  6]/[ 140/ 287] BT=4.037/4.234 Loss=14.325 6.357 0.825 1.969 5.174/14.054
[07/27 00:26:52] vclr INFO: Train: [  6]/[ 150/ 287] BT=4.158/4.189 Loss=13.997 6.924 0.387 1.370 5.315/14.043
[07/27 00:27:29] vclr INFO: Train: [  6]/[ 160/ 287] BT=3.793/4.156 Loss=13.536 6.786 0.246 1.388 5.116/14.048
[07/27 00:28:08] vclr INFO: Train: [  6]/[ 170/ 287] BT=3.877/4.145 Loss=14.408 6.840 0.418 1.826 5.323/14.037
[07/27 00:28:45] vclr INFO: Train: [  6]/[ 180/ 287] BT=3.703/4.117 Loss=14.087 7.169 0.464 1.211 5.243/14.056
[07/27 00:29:23] vclr INFO: Train: [  6]/[ 190/ 287] BT=3.872/4.102 Loss=13.844 6.795 0.408 1.761 4.879/14.048
[07/27 00:29:59] vclr INFO: Train: [  6]/[ 200/ 287] BT=3.640/4.075 Loss=12.998 6.151 0.509 1.585 4.753/14.027
[07/27 00:30:35] vclr INFO: Train: [  6]/[ 210/ 287] BT=3.758/4.053 Loss=14.240 7.017 0.500 1.554 5.169/14.016
[07/27 00:31:11] vclr INFO: Train: [  6]/[ 220/ 287] BT=3.348/4.036 Loss=13.496 6.430 0.342 1.729 4.995/14.023
[07/27 00:31:50] vclr INFO: Train: [  6]/[ 230/ 287] BT=3.804/4.026 Loss=13.842 7.097 0.217 1.313 5.216/14.023
[07/27 00:32:26] vclr INFO: Train: [  6]/[ 240/ 287] BT=3.900/4.012 Loss=13.249 6.517 0.420 1.442 4.871/14.003
[07/27 00:33:03] vclr INFO: Train: [  6]/[ 250/ 287] BT=3.174/3.997 Loss=13.949 6.302 0.448 1.905 5.295/14.002
[07/27 00:33:39] vclr INFO: Train: [  6]/[ 260/ 287] BT=3.357/3.984 Loss=13.227 6.637 0.408 1.435 4.747/13.992
[07/27 00:34:19] vclr INFO: Train: [  6]/[ 270/ 287] BT=4.001/3.983 Loss=13.824 7.056 0.267 1.224 5.276/13.976
[07/27 00:34:56] vclr INFO: Train: [  6]/[ 280/ 287] BT=4.165/3.974 Loss=13.259 6.714 0.324 1.513 4.707/13.970
[07/27 00:35:21] vclr INFO: epoch 6, total time 1143.03, loss=13.963944906972426
[07/27 00:35:21] vclr INFO: ==> Saving...
[07/27 00:36:18] vclr INFO: Train: [  7]/[   0/ 287] BT=54.976/54.976 Loss=13.303 6.881 0.381 1.351 4.691/13.303
[07/27 00:36:57] vclr INFO: Train: [  7]/[  10/ 287] BT=3.768/8.562 Loss=13.814 6.945 0.398 1.829 4.642/13.484
[07/27 00:37:45] vclr INFO: Train: [  7]/[  20/ 287] BT=5.038/6.733 Loss=12.835 6.280 0.311 1.418 4.825/13.443
[07/27 00:38:34] vclr INFO: Train: [  7]/[  30/ 287] BT=4.361/6.149 Loss=13.782 6.918 0.319 1.298 5.247/13.579
[07/27 00:39:23] vclr INFO: Train: [  7]/[  40/ 287] BT=4.817/5.855 Loss=12.611 5.994 0.705 1.469 4.444/13.516
[07/27 00:40:14] vclr INFO: Train: [  7]/[  50/ 287] BT=5.003/5.691 Loss=13.980 6.572 0.391 1.817 5.200/13.509
[07/27 00:41:04] vclr INFO: Train: [  7]/[  60/ 287] BT=5.449/5.588 Loss=14.701 7.133 0.437 1.606 5.525/13.488
[07/27 00:41:55] vclr INFO: Train: [  7]/[  70/ 287] BT=5.141/5.522 Loss=13.775 6.785 0.423 1.455 5.112/13.528
[07/27 00:42:39] vclr INFO: Train: [  7]/[  80/ 287] BT=5.080/5.384 Loss=13.538 6.634 0.418 1.583 4.902/13.510
[07/27 00:43:32] vclr INFO: Train: [  7]/[  90/ 287] BT=5.196/5.369 Loss=12.146 6.016 0.492 1.349 4.290/13.510
[07/27 00:44:20] vclr INFO: Train: [  7]/[ 100/ 287] BT=5.481/5.312 Loss=13.496 6.623 0.338 1.710 4.824/13.540
[07/27 00:45:06] vclr INFO: Train: [  7]/[ 110/ 287] BT=4.996/5.248 Loss=13.422 6.693 0.632 1.159 4.939/13.531
[07/27 00:45:54] vclr INFO: Train: [  7]/[ 120/ 287] BT=5.193/5.211 Loss=13.798 6.844 0.261 1.700 4.992/13.541
[07/27 00:46:37] vclr INFO: Train: [  7]/[ 130/ 287] BT=4.625/5.143 Loss=13.505 6.465 0.481 1.699 4.860/13.548
[07/27 00:47:20] vclr INFO: Train: [  7]/[ 140/ 287] BT=4.701/5.084 Loss=14.474 7.132 0.314 1.661 5.366/13.551
[07/27 00:48:05] vclr INFO: Train: [  7]/[ 150/ 287] BT=5.050/5.042 Loss=13.910 6.938 0.348 1.456 5.167/13.537
[07/27 00:48:53] vclr INFO: Train: [  7]/[ 160/ 287] BT=5.303/5.029 Loss=15.042 6.687 0.745 2.369 5.241/13.549
[07/27 00:49:38] vclr INFO: Train: [  7]/[ 170/ 287] BT=4.461/4.999 Loss=13.598 6.717 0.399 1.417 5.065/13.542
[07/27 00:50:24] vclr INFO: Train: [  7]/[ 180/ 287] BT=4.316/4.975 Loss=12.644 6.168 0.476 1.313 4.688/13.539
[07/27 00:51:14] vclr INFO: Train: [  7]/[ 190/ 287] BT=4.992/4.977 Loss=14.362 6.961 0.826 1.551 5.025/13.543
[07/27 00:52:02] vclr INFO: Train: [  7]/[ 200/ 287] BT=4.441/4.967 Loss=12.540 5.779 0.511 1.636 4.614/13.530
[07/27 00:52:48] vclr INFO: Train: [  7]/[ 210/ 287] BT=4.624/4.952 Loss=12.633 6.338 0.498 1.469 4.328/13.503
[07/27 00:53:32] vclr INFO: Train: [  7]/[ 220/ 287] BT=4.608/4.927 Loss=13.902 6.995 0.259 1.754 4.894/13.503
[07/27 00:54:19] vclr INFO: Train: [  7]/[ 230/ 287] BT=4.935/4.917 Loss=13.488 6.594 0.545 1.490 4.859/13.491
[07/27 00:55:01] vclr INFO: Train: [  7]/[ 240/ 287] BT=4.022/4.888 Loss=13.092 6.611 0.374 1.362 4.745/13.488
[07/27 00:55:48] vclr INFO: Train: [  7]/[ 250/ 287] BT=5.228/4.878 Loss=15.128 7.003 0.466 2.261 5.399/13.491
[07/27 00:56:35] vclr INFO: Train: [  7]/[ 260/ 287] BT=4.331/4.872 Loss=13.962 6.098 0.432 2.352 5.080/13.496
[07/27 00:57:23] vclr INFO: Train: [  7]/[ 270/ 287] BT=4.614/4.871 Loss=13.834 6.957 0.354 1.428 5.095/13.503
[07/27 00:58:12] vclr INFO: Train: [  7]/[ 280/ 287] BT=4.672/4.872 Loss=11.895 5.808 0.515 1.413 4.159/13.494
[07/27 00:58:45] vclr INFO: epoch 7, total time 1403.81, loss=13.500387391146882
[07/27 00:58:45] vclr INFO: ==> Saving...
[07/27 00:59:43] vclr INFO: Train: [  8]/[   0/ 287] BT=56.008/56.008 Loss=13.566 6.995 0.363 1.331 4.876/13.566
[07/27 01:00:38] vclr INFO: Train: [  8]/[  10/ 287] BT=5.801/10.159 Loss=13.925 6.599 0.510 1.993 4.823/13.751
[07/27 01:01:34] vclr INFO: Train: [  8]/[  20/ 287] BT=5.584/7.973 Loss=13.179 6.284 0.290 1.813 4.791/13.577
[07/27 01:02:28] vclr INFO: Train: [  8]/[  30/ 287] BT=6.201/7.146 Loss=12.983 6.430 0.462 1.716 4.375/13.517
[07/27 01:03:22] vclr INFO: Train: [  8]/[  40/ 287] BT=5.371/6.708 Loss=13.726 6.455 0.473 1.957 4.842/13.541
[07/27 01:04:20] vclr INFO: Train: [  8]/[  50/ 287] BT=6.011/6.529 Loss=14.235 6.373 0.772 1.702 5.388/13.533
[07/27 01:05:12] vclr INFO: Train: [  8]/[  60/ 287] BT=4.943/6.320 Loss=12.116 5.844 0.504 1.487 4.282/13.469
[07/27 01:06:06] vclr INFO: Train: [  8]/[  70/ 287] BT=5.189/6.191 Loss=13.187 6.382 0.555 1.597 4.653/13.429
[07/27 01:07:02] vclr INFO: Train: [  8]/[  80/ 287] BT=6.489/6.110 Loss=13.284 6.371 0.697 1.396 4.820/13.382
[07/27 01:07:57] vclr INFO: Train: [  8]/[  90/ 287] BT=5.216/6.045 Loss=12.784 6.345 0.439 1.455 4.544/13.353
[07/27 01:08:57] vclr INFO: Train: [  8]/[ 100/ 287] BT=6.777/6.040 Loss=14.464 7.114 0.502 1.700 5.149/13.344
[07/27 01:09:53] vclr INFO: Train: [  8]/[ 110/ 287] BT=6.007/6.002 Loss=13.138 6.414 0.575 1.394 4.755/13.358
[07/27 01:10:47] vclr INFO: Train: [  8]/[ 120/ 287] BT=5.174/5.954 Loss=13.151 6.473 0.398 1.696 4.584/13.374
[07/27 01:11:43] vclr INFO: Train: [  8]/[ 130/ 287] BT=4.838/5.925 Loss=13.427 6.609 0.440 1.612 4.767/13.354
[07/27 01:12:41] vclr INFO: Train: [  8]/[ 140/ 287] BT=5.940/5.915 Loss=12.599 5.962 0.447 1.474 4.715/13.345
[07/27 01:13:34] vclr INFO: Train: [  8]/[ 150/ 287] BT=5.998/5.879 Loss=12.734 6.324 0.435 1.454 4.521/13.331
[07/27 01:14:32] vclr INFO: Train: [  8]/[ 160/ 287] BT=6.052/5.870 Loss=15.014 7.409 0.586 1.913 5.105/13.344
[07/27 01:15:32] vclr INFO: Train: [  8]/[ 170/ 287] BT=5.434/5.876 Loss=14.146 6.969 0.651 1.502 5.025/13.345
[07/27 01:16:25] vclr INFO: Train: [  8]/[ 180/ 287] BT=5.349/5.844 Loss=13.157 6.519 0.516 1.570 4.553/13.352
[07/27 01:17:20] vclr INFO: Train: [  8]/[ 190/ 287] BT=5.221/5.828 Loss=13.168 6.642 0.383 1.482 4.660/13.351
[07/27 01:18:13] vclr INFO: Train: [  8]/[ 200/ 287] BT=5.437/5.801 Loss=13.478 6.176 0.543 1.827 4.933/13.354
[07/27 01:19:06] vclr INFO: Train: [  8]/[ 210/ 287] BT=5.383/5.777 Loss=12.856 6.317 0.435 1.472 4.631/13.343
[07/27 01:20:01] vclr INFO: Train: [  8]/[ 220/ 287] BT=5.349/5.767 Loss=12.817 6.090 0.437 1.590 4.699/13.345
[07/27 01:20:56] vclr INFO: Train: [  8]/[ 230/ 287] BT=5.593/5.753 Loss=14.239 6.611 0.884 1.733 5.011/13.351
[07/27 01:21:51] vclr INFO: Train: [  8]/[ 240/ 287] BT=5.836/5.744 Loss=13.291 6.572 0.361 1.397 4.961/13.344
[07/27 01:22:45] vclr INFO: Train: [  8]/[ 250/ 287] BT=5.124/5.732 Loss=14.192 7.347 0.135 1.628 5.081/13.341
[07/27 01:23:38] vclr INFO: Train: [  8]/[ 260/ 287] BT=5.124/5.714 Loss=13.722 7.018 0.334 1.744 4.627/13.329
[07/27 01:24:31] vclr INFO: Train: [  8]/[ 270/ 287] BT=5.033/5.698 Loss=12.956 6.190 0.613 1.551 4.602/13.316
[07/27 01:25:27] vclr INFO: Train: [  8]/[ 280/ 287] BT=6.588/5.695 Loss=12.543 5.893 0.468 1.506 4.676/13.313
[07/27 01:26:04] vclr INFO: epoch 8, total time 1639.43, loss=13.3124929055935
[07/27 01:26:04] vclr INFO: ==> Saving...
[07/27 01:27:31] vclr INFO: Train: [  9]/[   0/ 287] BT=85.191/85.191 Loss=12.205 6.004 0.399 1.470 4.332/12.205
[07/27 01:28:28] vclr INFO: Train: [  9]/[  10/ 287] BT=5.540/12.963 Loss=12.385 6.102 0.457 1.346 4.480/13.050
[07/27 01:29:21] vclr INFO: Train: [  9]/[  20/ 287] BT=5.001/9.312 Loss=13.863 6.680 0.424 1.587 5.171/13.095
[07/27 01:30:13] vclr INFO: Train: [  9]/[  30/ 287] BT=5.263/7.990 Loss=12.302 6.108 0.373 1.443 4.379/13.057
[07/27 01:31:05] vclr INFO: Train: [  9]/[  40/ 287] BT=5.325/7.309 Loss=13.094 6.697 0.379 1.262 4.755/13.039
[07/27 01:32:01] vclr INFO: Train: [  9]/[  50/ 287] BT=5.931/6.979 Loss=13.979 7.142 0.339 1.358 5.139/13.063
[07/27 01:33:01] vclr INFO: Train: [  9]/[  60/ 287] BT=5.675/6.806 Loss=13.628 6.485 0.320 1.780 5.043/13.027
[07/27 01:33:54] vclr INFO: Train: [  9]/[  70/ 287] BT=4.040/6.607 Loss=12.924 6.077 0.680 1.488 4.680/13.018
[07/27 01:34:46] vclr INFO: Train: [  9]/[  80/ 287] BT=4.988/6.424 Loss=13.005 6.415 0.508 1.545 4.538/13.057
[07/27 01:35:39] vclr INFO: Train: [  9]/[  90/ 287] BT=5.363/6.299 Loss=12.243 5.888 0.329 1.650 4.376/13.090
[07/27 01:36:32] vclr INFO: Train: [  9]/[ 100/ 287] BT=5.234/6.199 Loss=13.051 6.029 0.359 1.505 5.157/13.106
[07/27 01:37:28] vclr INFO: Train: [  9]/[ 110/ 287] BT=5.496/6.148 Loss=13.694 6.543 0.383 1.515 5.253/13.110
[07/27 01:38:25] vclr INFO: Train: [  9]/[ 120/ 287] BT=4.886/6.113 Loss=12.412 6.357 0.267 1.193 4.595/13.104
[07/27 01:39:18] vclr INFO: Train: [  9]/[ 130/ 287] BT=5.771/6.047 Loss=13.251 6.305 0.440 1.577 4.929/13.091
[07/27 01:40:10] vclr INFO: Train: [  9]/[ 140/ 287] BT=5.544/5.990 Loss=13.436 6.610 0.429 1.408 4.989/13.113
[07/27 01:41:05] vclr INFO: Train: [  9]/[ 150/ 287] BT=5.763/5.955 Loss=13.216 6.248 0.366 1.625 4.977/13.131
[07/27 01:42:03] vclr INFO: Train: [  9]/[ 160/ 287] BT=5.216/5.946 Loss=12.606 6.173 0.338 1.486 4.609/13.113
[07/27 01:42:58] vclr INFO: Train: [  9]/[ 170/ 287] BT=5.214/5.921 Loss=13.739 6.225 0.467 2.105 4.942/13.125
[07/27 01:43:51] vclr INFO: Train: [  9]/[ 180/ 287] BT=5.307/5.890 Loss=12.742 6.249 0.323 1.510 4.659/13.102
[07/27 01:44:43] vclr INFO: Train: [  9]/[ 190/ 287] BT=5.121/5.851 Loss=13.679 7.197 0.326 1.379 4.778/13.101
[07/27 01:45:38] vclr INFO: Train: [  9]/[ 200/ 287] BT=6.524/5.835 Loss=14.036 6.649 0.382 2.049 4.956/13.099
[07/27 01:46:31] vclr INFO: Train: [  9]/[ 210/ 287] BT=6.458/5.808 Loss=13.142 6.253 0.631 1.495 4.764/13.096
[07/27 01:47:30] vclr INFO: Train: [  9]/[ 220/ 287] BT=5.376/5.811 Loss=14.174 7.020 0.445 1.997 4.711/13.098
[07/27 01:48:27] vclr INFO: Train: [  9]/[ 230/ 287] BT=5.805/5.808 Loss=12.751 6.418 0.389 1.472 4.472/13.098
[07/27 01:49:24] vclr INFO: Train: [  9]/[ 240/ 287] BT=6.565/5.802 Loss=13.727 6.647 0.346 1.539 5.195/13.100
[07/27 01:50:21] vclr INFO: Train: [  9]/[ 250/ 287] BT=5.653/5.797 Loss=14.211 6.771 0.416 2.303 4.722/13.102
[07/27 01:51:10] vclr INFO: Train: [  9]/[ 260/ 287] BT=5.148/5.764 Loss=12.454 6.106 0.508 1.483 4.358/13.105
[07/27 01:52:02] vclr INFO: Train: [  9]/[ 270/ 287] BT=6.250/5.743 Loss=13.087 6.357 0.616 1.581 4.533/13.101
[07/27 01:52:52] vclr INFO: Train: [  9]/[ 280/ 287] BT=4.481/5.718 Loss=13.120 6.223 0.355 1.975 4.568/13.106
[07/27 01:53:20] vclr INFO: epoch 9, total time 1636.30, loss=13.117392483488608
[07/27 01:53:20] vclr INFO: ==> Saving...
[07/27 01:54:22] vclr INFO: Train: [ 10]/[   0/ 287] BT=59.049/59.049 Loss=12.433 5.923 0.446 1.817 4.247/12.433
[07/27 01:55:09] vclr INFO: Train: [ 10]/[  10/ 287] BT=4.816/9.638 Loss=13.634 7.159 0.214 1.572 4.689/13.465
[07/27 01:55:54] vclr INFO: Train: [ 10]/[  20/ 287] BT=4.472/7.220 Loss=14.030 7.049 0.207 1.590 5.183/13.302
[07/27 01:56:39] vclr INFO: Train: [ 10]/[  30/ 287] BT=4.429/6.346 Loss=13.267 6.665 0.372 1.631 4.599/13.212
[07/27 01:57:26] vclr INFO: Train: [ 10]/[  40/ 287] BT=4.371/5.924 Loss=13.830 6.454 0.442 1.808 5.125/13.215
[07/27 01:58:11] vclr INFO: Train: [ 10]/[  50/ 287] BT=4.566/5.645 Loss=13.135 6.593 0.369 1.194 4.979/13.209
[07/27 01:58:55] vclr INFO: Train: [ 10]/[  60/ 287] BT=3.603/5.450 Loss=13.600 6.289 0.512 2.245 4.554/13.231
[07/27 01:59:39] vclr INFO: Train: [ 10]/[  70/ 287] BT=3.939/5.302 Loss=13.061 6.339 0.481 1.654 4.587/13.231
[07/27 02:00:25] vclr INFO: Train: [ 10]/[  80/ 287] BT=4.677/5.214 Loss=12.742 6.190 0.655 1.409 4.488/13.217
[07/27 02:01:11] vclr INFO: Train: [ 10]/[  90/ 287] BT=4.193/5.150 Loss=12.925 6.557 0.371 1.547 4.449/13.187
[07/27 02:01:57] vclr INFO: Train: [ 10]/[ 100/ 287] BT=4.459/5.090 Loss=12.559 6.209 0.318 1.595 4.437/13.176
[07/27 02:02:42] vclr INFO: Train: [ 10]/[ 110/ 287] BT=4.274/5.041 Loss=11.928 5.817 0.488 1.261 4.362/13.164
[07/27 02:03:29] vclr INFO: Train: [ 10]/[ 120/ 287] BT=4.643/5.011 Loss=13.075 6.409 0.369 1.722 4.574/13.137
[07/27 02:04:13] vclr INFO: Train: [ 10]/[ 130/ 287] BT=4.854/4.968 Loss=12.264 6.109 0.330 1.311 4.515/13.127
[07/27 02:04:57] vclr INFO: Train: [ 10]/[ 140/ 287] BT=4.571/4.921 Loss=14.372 7.403 0.333 1.381 5.256/13.129
[07/27 02:05:43] vclr INFO: Train: [ 10]/[ 150/ 287] BT=4.713/4.903 Loss=12.743 6.602 0.303 1.328 4.509/13.124
[07/27 02:06:28] vclr INFO: Train: [ 10]/[ 160/ 287] BT=4.592/4.876 Loss=13.962 7.337 0.251 1.399 4.975/13.118
[07/27 02:07:13] vclr INFO: Train: [ 10]/[ 170/ 287] BT=4.578/4.854 Loss=12.568 6.121 0.427 1.575 4.445/13.107
[07/27 02:07:57] vclr INFO: Train: [ 10]/[ 180/ 287] BT=4.437/4.828 Loss=12.452 6.201 0.291 1.486 4.473/13.104
[07/27 02:08:41] vclr INFO: Train: [ 10]/[ 190/ 287] BT=4.263/4.806 Loss=12.354 6.350 0.330 1.282 4.393/13.095
[07/27 02:09:26] vclr INFO: Train: [ 10]/[ 200/ 287] BT=4.435/4.795 Loss=13.092 6.523 0.306 1.556 4.707/13.078
[07/27 02:10:12] vclr INFO: Train: [ 10]/[ 210/ 287] BT=4.725/4.785 Loss=12.874 6.117 0.443 1.453 4.861/13.078
[07/27 02:10:57] vclr INFO: Train: [ 10]/[ 220/ 287] BT=4.708/4.771 Loss=12.836 6.002 0.407 1.432 4.995/13.075
[07/27 02:11:43] vclr INFO: Train: [ 10]/[ 230/ 287] BT=4.509/4.762 Loss=12.568 6.063 0.265 1.920 4.320/13.061
[07/27 02:12:26] vclr INFO: Train: [ 10]/[ 240/ 287] BT=4.621/4.745 Loss=12.668 6.343 0.286 1.417 4.621/13.052
[07/27 02:13:11] vclr INFO: Train: [ 10]/[ 250/ 287] BT=4.308/4.735 Loss=12.216 5.512 0.560 1.759 4.384/13.045
[07/27 02:13:57] vclr INFO: Train: [ 10]/[ 260/ 287] BT=4.694/4.730 Loss=12.358 6.059 0.506 1.303 4.491/13.026
[07/27 02:14:42] vclr INFO: Train: [ 10]/[ 270/ 287] BT=4.608/4.722 Loss=13.333 6.839 0.260 1.514 4.720/13.012
[07/27 02:15:27] vclr INFO: Train: [ 10]/[ 280/ 287] BT=4.613/4.714 Loss=14.376 6.925 0.401 2.113 4.937/13.010
[07/27 02:15:57] vclr INFO: epoch 10, total time 1356.91, loss=13.001918081622506
[07/27 02:15:57] vclr INFO: ==> Saving...
[07/27 02:17:09] vclr INFO: Train: [ 11]/[   0/ 287] BT=64.006/64.006 Loss=13.294 6.614 0.604 1.538 4.538/13.294
[07/27 02:17:54] vclr INFO: Train: [ 11]/[  10/ 287] BT=4.488/9.934 Loss=12.384 6.025 0.450 1.591 4.318/12.893
[07/27 02:18:40] vclr INFO: Train: [ 11]/[  20/ 287] BT=4.203/7.390 Loss=13.245 6.512 0.500 1.567 4.665/13.028
[07/27 02:19:26] vclr INFO: Train: [ 11]/[  30/ 287] BT=4.882/6.476 Loss=12.031 5.754 0.472 1.497 4.308/12.970
[07/27 02:20:10] vclr INFO: Train: [ 11]/[  40/ 287] BT=5.027/5.974 Loss=12.991 6.797 0.248 1.450 4.496/13.007
[07/27 02:20:53] vclr INFO: Train: [ 11]/[  50/ 287] BT=4.038/5.654 Loss=13.261 6.899 0.342 1.371 4.649/13.027
[07/27 02:21:38] vclr INFO: Train: [ 11]/[  60/ 287] BT=4.255/5.464 Loss=13.739 7.207 0.323 1.306 4.904/13.009
[07/27 02:22:22] vclr INFO: Train: [ 11]/[  70/ 287] BT=4.137/5.315 Loss=12.641 6.484 0.418 1.337 4.403/13.013
[07/27 02:23:07] vclr INFO: Train: [ 11]/[  80/ 287] BT=4.372/5.213 Loss=13.658 6.129 0.490 1.898 5.140/13.072
[07/27 02:23:54] vclr INFO: Train: [ 11]/[  90/ 287] BT=4.878/5.152 Loss=12.079 5.893 0.411 1.500 4.274/13.076
[07/27 02:24:38] vclr INFO: Train: [ 11]/[ 100/ 287] BT=4.871/5.081 Loss=12.857 6.186 0.600 1.397 4.674/13.039
[07/27 02:25:23] vclr INFO: Train: [ 11]/[ 110/ 287] BT=4.212/5.028 Loss=13.161 6.529 0.359 1.454 4.819/13.031
[07/27 02:26:11] vclr INFO: Train: [ 11]/[ 120/ 287] BT=4.890/5.009 Loss=12.364 5.683 0.361 1.446 4.874/13.010
[07/27 02:26:57] vclr INFO: Train: [ 11]/[ 130/ 287] BT=4.413/4.980 Loss=13.824 6.793 0.240 1.749 5.042/12.979
[07/27 02:27:44] vclr INFO: Train: [ 11]/[ 140/ 287] BT=4.720/4.956 Loss=12.441 6.118 0.379 1.227 4.717/12.971
[07/27 02:28:31] vclr INFO: Train: [ 11]/[ 150/ 287] BT=3.902/4.938 Loss=12.544 6.434 0.317 1.379 4.414/12.964
[07/27 02:29:16] vclr INFO: Train: [ 11]/[ 160/ 287] BT=4.631/4.912 Loss=13.341 6.870 0.446 1.467 4.558/12.958
[07/27 02:30:00] vclr INFO: Train: [ 11]/[ 170/ 287] BT=5.011/4.882 Loss=14.044 7.031 0.492 1.404 5.118/12.964
[07/27 02:30:45] vclr INFO: Train: [ 11]/[ 180/ 287] BT=4.413/4.863 Loss=13.596 6.949 0.308 1.363 4.976/12.943
[07/27 02:31:31] vclr INFO: Train: [ 11]/[ 190/ 287] BT=4.635/4.846 Loss=12.542 6.031 0.385 1.637 4.488/12.946
[07/27 02:32:16] vclr INFO: Train: [ 11]/[ 200/ 287] BT=4.498/4.830 Loss=12.686 6.189 0.308 1.889 4.300/12.957
[07/27 02:33:01] vclr INFO: Train: [ 11]/[ 210/ 287] BT=4.347/4.814 Loss=13.061 6.303 0.399 1.465 4.894/12.969
[07/27 02:33:46] vclr INFO: Train: [ 11]/[ 220/ 287] BT=4.549/4.801 Loss=12.871 6.416 0.591 1.373 4.491/12.963
[07/27 02:34:33] vclr INFO: Train: [ 11]/[ 230/ 287] BT=4.918/4.795 Loss=14.537 7.015 0.326 1.657 5.538/12.971
[07/27 02:35:17] vclr INFO: Train: [ 11]/[ 240/ 287] BT=4.227/4.780 Loss=13.840 7.052 0.203 1.467 5.118/12.972
[07/27 02:36:04] vclr INFO: Train: [ 11]/[ 250/ 287] BT=4.246/4.775 Loss=12.337 5.758 0.613 1.533 4.432/12.979
[07/27 02:36:52] vclr INFO: Train: [ 11]/[ 260/ 287] BT=4.889/4.777 Loss=13.062 6.629 0.333 1.708 4.391/12.972
[07/27 02:37:37] vclr INFO: Train: [ 11]/[ 270/ 287] BT=4.581/4.768 Loss=12.885 6.567 0.464 1.387 4.466/12.965
[07/27 02:38:22] vclr INFO: Train: [ 11]/[ 280/ 287] BT=4.705/4.757 Loss=12.368 6.009 0.488 1.463 4.408/12.956
[07/27 02:38:53] vclr INFO: epoch 11, total time 1375.39, loss=12.963668092202642
[07/27 02:38:53] vclr INFO: ==> Saving...
[07/27 02:39:51] vclr INFO: Train: [ 12]/[   0/ 287] BT=56.737/56.737 Loss=12.122 6.093 0.291 1.446 4.292/12.122
[07/27 02:40:36] vclr INFO: Train: [ 12]/[  10/ 287] BT=4.313/9.260 Loss=12.666 6.365 0.561 1.214 4.526/12.562
[07/27 02:41:23] vclr INFO: Train: [ 12]/[  20/ 287] BT=4.586/7.077 Loss=12.868 6.433 0.488 1.423 4.524/12.589
[07/27 02:42:08] vclr INFO: Train: [ 12]/[  30/ 287] BT=4.568/6.248 Loss=12.242 5.793 0.453 1.612 4.384/12.614
[07/27 02:42:53] vclr INFO: Train: [ 12]/[  40/ 287] BT=4.762/5.824 Loss=13.086 6.502 0.317 1.397 4.870/12.698
[07/27 02:43:37] vclr INFO: Train: [ 12]/[  50/ 287] BT=4.071/5.540 Loss=12.375 6.128 0.373 1.469 4.405/12.713
[07/27 02:44:22] vclr INFO: Train: [ 12]/[  60/ 287] BT=4.778/5.371 Loss=13.145 6.571 0.361 1.615 4.597/12.763
[07/27 02:45:05] vclr INFO: Train: [ 12]/[  70/ 287] BT=4.491/5.217 Loss=13.085 6.489 0.388 1.411 4.797/12.769
[07/27 02:45:49] vclr INFO: Train: [ 12]/[  80/ 287] BT=5.099/5.115 Loss=13.092 6.452 0.519 1.631 4.490/12.750
[07/27 02:46:33] vclr INFO: Train: [ 12]/[  90/ 287] BT=4.539/5.040 Loss=12.898 6.216 0.433 1.611 4.637/12.781
[07/27 02:47:19] vclr INFO: Train: [ 12]/[ 100/ 287] BT=4.453/5.000 Loss=12.791 6.069 0.368 1.932 4.421/12.797
[07/27 02:48:05] vclr INFO: Train: [ 12]/[ 110/ 287] BT=6.458/4.966 Loss=13.383 6.567 0.473 1.823 4.519/12.827
[07/27 02:48:55] vclr INFO: Train: [ 12]/[ 120/ 287] BT=5.270/4.963 Loss=14.343 6.896 0.269 2.657 4.521/12.896
[07/27 02:49:40] vclr INFO: Train: [ 12]/[ 130/ 287] BT=4.296/4.933 Loss=12.125 6.070 0.306 1.512 4.237/12.907
[07/27 02:50:28] vclr INFO: Train: [ 12]/[ 140/ 287] BT=4.712/4.920 Loss=12.829 6.087 0.301 1.809 4.631/12.908
[07/27 02:51:18] vclr INFO: Train: [ 12]/[ 150/ 287] BT=5.230/4.925 Loss=12.942 6.273 0.342 1.880 4.447/12.907
[07/27 02:52:06] vclr INFO: Train: [ 12]/[ 160/ 287] BT=5.115/4.919 Loss=12.737 5.991 0.451 1.534 4.761/12.900
[07/27 02:52:52] vclr INFO: Train: [ 12]/[ 170/ 287] BT=4.907/4.902 Loss=13.235 6.684 0.380 1.480 4.691/12.907
[07/27 02:53:40] vclr INFO: Train: [ 12]/[ 180/ 287] BT=4.439/4.895 Loss=12.157 5.962 0.308 1.138 4.749/12.912
[07/27 02:54:29] vclr INFO: Train: [ 12]/[ 190/ 287] BT=4.182/4.896 Loss=12.865 6.347 0.343 1.664 4.511/12.931
[07/27 02:55:21] vclr INFO: Train: [ 12]/[ 200/ 287] BT=4.748/4.908 Loss=13.120 6.374 0.458 1.740 4.548/12.935
[07/27 02:56:09] vclr INFO: Train: [ 12]/[ 210/ 287] BT=5.375/4.903 Loss=13.408 6.675 0.470 1.366 4.896/12.935
[07/27 02:56:56] vclr INFO: Train: [ 12]/[ 220/ 287] BT=5.513/4.896 Loss=12.745 6.138 0.392 1.759 4.456/12.937
[07/27 02:57:47] vclr INFO: Train: [ 12]/[ 230/ 287] BT=4.854/4.905 Loss=12.944 6.323 0.356 1.465 4.800/12.940
[07/27 02:58:33] vclr INFO: Train: [ 12]/[ 240/ 287] BT=4.836/4.891 Loss=13.938 6.423 0.499 1.900 5.116/12.954
[07/27 02:59:22] vclr INFO: Train: [ 12]/[ 250/ 287] BT=5.160/4.891 Loss=13.047 6.683 0.281 1.532 4.551/12.962
[07/27 03:00:12] vclr INFO: Train: [ 12]/[ 260/ 287] BT=5.233/4.896 Loss=12.889 6.562 0.466 1.210 4.650/12.954
[07/27 03:01:01] vclr INFO: Train: [ 12]/[ 270/ 287] BT=4.875/4.896 Loss=13.383 6.558 0.789 1.449 4.586/12.950
[07/27 03:01:49] vclr INFO: Train: [ 12]/[ 280/ 287] BT=4.522/4.892 Loss=12.892 6.159 0.382 2.054 4.298/12.947
[07/27 03:02:19] vclr INFO: epoch 12, total time 1406.87, loss=12.941959939351896
[07/27 03:02:19] vclr INFO: ==> Saving...
[07/27 03:03:28] vclr INFO: Train: [ 13]/[   0/ 287] BT=65.969/65.969 Loss=13.016 6.257 0.364 1.688 4.707/13.016
[07/27 03:04:17] vclr INFO: Train: [ 13]/[  10/ 287] BT=4.323/10.479 Loss=13.286 6.708 0.401 1.888 4.289/13.246
[07/27 03:05:04] vclr INFO: Train: [ 13]/[  20/ 287] BT=4.206/7.691 Loss=12.699 6.088 0.374 2.011 4.226/13.153
[07/27 03:05:49] vclr INFO: Train: [ 13]/[  30/ 287] BT=4.564/6.691 Loss=13.177 6.276 0.325 1.707 4.868/13.144
[07/27 03:06:37] vclr INFO: Train: [ 13]/[  40/ 287] BT=4.622/6.227 Loss=11.677 5.884 0.412 1.155 4.226/13.037
[07/27 03:07:25] vclr INFO: Train: [ 13]/[  50/ 287] BT=4.752/5.943 Loss=13.315 5.961 0.357 2.330 4.667/12.936
[07/27 03:08:17] vclr INFO: Train: [ 13]/[  60/ 287] BT=5.252/5.812 Loss=13.508 6.551 0.411 1.258 5.287/12.982
[07/27 03:09:06] vclr INFO: Train: [ 13]/[  70/ 287] BT=4.790/5.692 Loss=12.988 6.300 0.494 1.493 4.701/12.971
[07/27 03:09:54] vclr INFO: Train: [ 13]/[  80/ 287] BT=4.880/5.581 Loss=13.358 6.932 0.205 1.468 4.753/12.948
[07/27 03:10:41] vclr INFO: Train: [ 13]/[  90/ 287] BT=4.748/5.480 Loss=12.918 6.875 0.229 1.375 4.438/12.940
[07/27 03:11:31] vclr INFO: Train: [ 13]/[ 100/ 287] BT=4.786/5.434 Loss=12.535 6.321 0.274 1.417 4.524/12.924
[07/27 03:12:20] vclr INFO: Train: [ 13]/[ 110/ 287] BT=4.267/5.390 Loss=13.304 6.593 0.242 1.458 5.010/12.913
[07/27 03:13:11] vclr INFO: Train: [ 13]/[ 120/ 287] BT=5.307/5.361 Loss=12.580 5.912 0.372 1.758 4.538/12.903
[07/27 03:14:02] vclr INFO: Train: [ 13]/[ 130/ 287] BT=5.426/5.341 Loss=13.048 6.751 0.181 1.430 4.685/12.898
[07/27 03:14:51] vclr INFO: Train: [ 13]/[ 140/ 287] BT=5.499/5.313 Loss=13.660 6.478 0.189 2.150 4.842/12.893
[07/27 03:15:41] vclr INFO: Train: [ 13]/[ 150/ 287] BT=4.942/5.293 Loss=13.481 6.272 0.637 1.583 4.989/12.899
[07/27 03:16:29] vclr INFO: Train: [ 13]/[ 160/ 287] BT=5.365/5.260 Loss=13.304 6.394 0.415 1.414 5.081/12.887
[07/27 03:17:17] vclr INFO: Train: [ 13]/[ 170/ 287] BT=4.591/5.232 Loss=12.581 5.992 0.545 1.421 4.624/12.890
[07/27 03:18:08] vclr INFO: Train: [ 13]/[ 180/ 287] BT=5.440/5.228 Loss=12.120 5.769 0.472 1.523 4.356/12.884
[07/27 03:18:56] vclr INFO: Train: [ 13]/[ 190/ 287] BT=4.050/5.202 Loss=13.176 6.165 0.356 2.356 4.299/12.898
[07/27 03:19:43] vclr INFO: Train: [ 13]/[ 200/ 287] BT=4.317/5.179 Loss=13.101 6.148 0.543 1.905 4.506/12.905
[07/27 03:20:32] vclr INFO: Train: [ 13]/[ 210/ 287] BT=4.574/5.163 Loss=12.320 6.055 0.390 1.416 4.460/12.884
[07/27 03:21:19] vclr INFO: Train: [ 13]/[ 220/ 287] BT=4.411/5.146 Loss=13.455 6.292 0.612 2.022 4.529/12.877
[07/27 03:22:06] vclr INFO: Train: [ 13]/[ 230/ 287] BT=4.456/5.126 Loss=12.168 5.835 0.519 1.096 4.719/12.873
[07/27 03:22:55] vclr INFO: Train: [ 13]/[ 240/ 287] BT=5.167/5.115 Loss=12.663 5.862 0.341 1.654 4.806/12.884
[07/27 03:23:42] vclr INFO: Train: [ 13]/[ 250/ 287] BT=4.449/5.101 Loss=12.314 6.042 0.390 1.394 4.488/12.895
[07/27 03:24:30] vclr INFO: Train: [ 13]/[ 260/ 287] BT=4.731/5.088 Loss=13.228 6.615 0.347 1.667 4.600/12.887
[07/27 03:25:20] vclr INFO: Train: [ 13]/[ 270/ 287] BT=5.174/5.084 Loss=13.204 6.216 0.268 1.981 4.739/12.886
[07/27 03:26:08] vclr INFO: Train: [ 13]/[ 280/ 287] BT=4.720/5.076 Loss=12.776 6.100 0.414 1.652 4.611/12.891
[07/27 03:26:42] vclr INFO: epoch 13, total time 1462.53, loss=12.893570175569648
[07/27 03:26:42] vclr INFO: ==> Saving...
[07/27 03:28:22] vclr INFO: Train: [ 14]/[   0/ 287] BT=96.635/96.635 Loss=12.481 5.879 0.523 1.627 4.452/12.481
[07/27 03:29:10] vclr INFO: Train: [ 14]/[  10/ 287] BT=4.940/13.207 Loss=13.008 6.300 0.493 1.461 4.754/12.699
[07/27 03:30:00] vclr INFO: Train: [ 14]/[  20/ 287] BT=5.130/9.266 Loss=13.096 6.417 0.404 1.560 4.715/12.649
[07/27 03:30:49] vclr INFO: Train: [ 14]/[  30/ 287] BT=5.237/7.853 Loss=12.488 5.993 0.353 1.398 4.744/12.710
[07/27 03:31:38] vclr INFO: Train: [ 14]/[  40/ 287] BT=5.121/7.143 Loss=12.471 6.040 0.591 1.561 4.278/12.711
[07/27 03:32:25] vclr INFO: Train: [ 14]/[  50/ 287] BT=4.937/6.668 Loss=12.125 6.246 0.233 1.345 4.300/12.707
[07/27 03:33:14] vclr INFO: Train: [ 14]/[  60/ 287] BT=4.960/6.376 Loss=12.514 6.017 0.339 1.639 4.520/12.711
[07/27 03:34:03] vclr INFO: Train: [ 14]/[  70/ 287] BT=4.809/6.162 Loss=12.966 6.364 0.392 1.629 4.580/12.729
[07/27 03:34:51] vclr INFO: Train: [ 14]/[  80/ 287] BT=4.819/5.996 Loss=14.685 7.253 0.314 2.066 5.052/12.750
[07/27 03:35:48] vclr INFO: Train: [ 14]/[  90/ 287] BT=5.760/5.968 Loss=13.363 6.676 0.337 1.857 4.494/12.814
[07/27 03:36:49] vclr INFO: Train: [ 14]/[ 100/ 287] BT=5.343/5.981 Loss=12.185 5.774 0.395 1.449 4.566/12.786
[07/27 03:37:52] vclr INFO: Train: [ 14]/[ 110/ 287] BT=6.181/6.012 Loss=12.784 6.111 0.536 1.497 4.640/12.773
[07/27 03:38:54] vclr INFO: Train: [ 14]/[ 120/ 287] BT=5.606/6.020 Loss=13.145 6.736 0.209 1.231 4.968/12.756
[07/27 03:39:57] vclr INFO: Train: [ 14]/[ 130/ 287] BT=5.849/6.045 Loss=13.102 6.845 0.221 1.434 4.601/12.759
[07/27 03:40:59] vclr INFO: Train: [ 14]/[ 140/ 287] BT=6.648/6.055 Loss=13.032 5.896 0.324 1.979 4.833/12.746
[07/27 03:42:01] vclr INFO: Train: [ 14]/[ 150/ 287] BT=6.514/6.065 Loss=11.760 5.960 0.349 1.326 4.123/12.747
[07/27 03:43:01] vclr INFO: Train: [ 14]/[ 160/ 287] BT=5.867/6.063 Loss=12.868 6.142 0.386 1.630 4.710/12.748
[07/27 03:44:04] vclr INFO: Train: [ 14]/[ 170/ 287] BT=6.762/6.077 Loss=13.127 6.632 0.288 1.427 4.779/12.750
[07/27 03:45:07] vclr INFO: Train: [ 14]/[ 180/ 287] BT=7.058/6.086 Loss=12.683 5.939 0.504 1.774 4.466/12.736
[07/27 03:46:08] vclr INFO: Train: [ 14]/[ 190/ 287] BT=6.315/6.088 Loss=12.864 6.215 0.418 1.655 4.576/12.728
[07/27 03:47:08] vclr INFO: Train: [ 14]/[ 200/ 287] BT=6.153/6.085 Loss=12.361 6.121 0.553 1.386 4.301/12.738
[07/27 03:48:10] vclr INFO: Train: [ 14]/[ 210/ 287] BT=6.513/6.089 Loss=12.807 6.109 0.450 1.672 4.576/12.759
[07/27 03:49:12] vclr INFO: Train: [ 14]/[ 220/ 287] BT=5.960/6.093 Loss=12.991 6.178 0.547 1.565 4.701/12.774
[07/27 03:50:14] vclr INFO: Train: [ 14]/[ 230/ 287] BT=6.555/6.100 Loss=12.468 6.404 0.346 1.355 4.363/12.774
[07/27 03:51:16] vclr INFO: Train: [ 14]/[ 240/ 287] BT=5.685/6.104 Loss=13.493 6.550 0.495 1.556 4.891/12.784
[07/27 03:52:19] vclr INFO: Train: [ 14]/[ 250/ 287] BT=5.346/6.111 Loss=12.130 5.830 0.590 1.264 4.446/12.769
[07/27 03:53:19] vclr INFO: Train: [ 14]/[ 260/ 287] BT=5.962/6.107 Loss=13.064 6.800 0.222 1.298 4.744/12.764
[07/27 03:54:23] vclr INFO: Train: [ 14]/[ 270/ 287] BT=6.702/6.116 Loss=12.695 6.382 0.428 1.360 4.525/12.764
[07/27 03:55:24] vclr INFO: Train: [ 14]/[ 280/ 287] BT=6.351/6.117 Loss=13.238 6.863 0.319 1.309 4.747/12.762
[07/27 03:56:04] vclr INFO: epoch 14, total time 1762.24, loss=12.757448402431368
[07/27 03:56:04] vclr INFO: ==> Saving...
[07/27 03:57:21] vclr INFO: Train: [ 15]/[   0/ 287] BT=73.916/73.916 Loss=12.519 6.159 0.285 1.454 4.621/12.519
[07/27 03:58:18] vclr INFO: Train: [ 15]/[  10/ 287] BT=5.527/11.918 Loss=12.964 6.512 0.331 1.821 4.300/12.528
[07/27 03:59:13] vclr INFO: Train: [ 15]/[  20/ 287] BT=5.104/8.885 Loss=13.108 6.483 0.400 1.686 4.539/12.503
[07/27 04:00:09] vclr INFO: Train: [ 15]/[  30/ 287] BT=6.029/7.817 Loss=13.597 6.148 0.471 2.463 4.515/12.649
[07/27 04:01:11] vclr INFO: Train: [ 15]/[  40/ 287] BT=7.808/7.409 Loss=13.595 6.645 0.324 1.762 4.864/12.685
[07/27 04:02:16] vclr INFO: Train: [ 15]/[  50/ 287] BT=6.582/7.241 Loss=13.595 6.913 0.343 1.739 4.599/12.677
[07/27 04:03:19] vclr INFO: Train: [ 15]/[  60/ 287] BT=6.285/7.089 Loss=13.470 6.577 0.359 1.609 4.925/12.781
[07/27 04:04:23] vclr INFO: Train: [ 15]/[  70/ 287] BT=6.759/6.986 Loss=13.304 6.072 0.421 2.023 4.788/12.819
[07/27 04:05:25] vclr INFO: Train: [ 15]/[  80/ 287] BT=5.537/6.892 Loss=12.237 5.952 0.204 1.595 4.486/12.806
[07/27 04:06:28] vclr INFO: Train: [ 15]/[  90/ 287] BT=6.320/6.828 Loss=13.570 6.501 0.290 1.794 4.985/12.782
[07/27 04:07:33] vclr INFO: Train: [ 15]/[ 100/ 287] BT=6.803/6.796 Loss=12.355 6.272 0.310 1.245 4.529/12.769
[07/27 04:08:34] vclr INFO: Train: [ 15]/[ 110/ 287] BT=6.425/6.728 Loss=12.667 6.407 0.300 1.494 4.466/12.724
[07/27 04:09:44] vclr INFO: Train: [ 15]/[ 120/ 287] BT=6.686/6.755 Loss=12.836 6.260 0.653 1.575 4.348/12.742
[07/27 04:11:00] vclr INFO: Train: [ 15]/[ 130/ 287] BT=7.010/6.815 Loss=11.813 5.545 0.738 1.440 4.089/12.731
[07/27 04:12:14] vclr INFO: Train: [ 15]/[ 140/ 287] BT=8.315/6.859 Loss=11.628 5.788 0.456 1.200 4.185/12.724
[07/27 04:13:26] vclr INFO: Train: [ 15]/[ 150/ 287] BT=7.376/6.885 Loss=12.074 5.760 0.523 1.404 4.388/12.712
[07/27 04:14:40] vclr INFO: Train: [ 15]/[ 160/ 287] BT=7.101/6.911 Loss=12.832 6.356 0.309 1.422 4.745/12.696
[07/27 04:15:53] vclr INFO: Train: [ 15]/[ 170/ 287] BT=7.925/6.936 Loss=12.798 5.963 0.449 1.811 4.575/12.701
[07/27 04:17:05] vclr INFO: Train: [ 15]/[ 180/ 287] BT=7.424/6.952 Loss=11.679 5.387 0.388 1.702 4.202/12.687
[07/27 04:18:18] vclr INFO: Train: [ 15]/[ 190/ 287] BT=7.660/6.971 Loss=12.426 5.547 0.680 1.727 4.472/12.691
[07/27 04:19:34] vclr INFO: Train: [ 15]/[ 200/ 287] BT=7.430/7.000 Loss=12.315 5.959 0.543 1.293 4.520/12.691
[07/27 04:20:47] vclr INFO: Train: [ 15]/[ 210/ 287] BT=7.531/7.016 Loss=13.849 6.145 0.362 2.715 4.628/12.716
[07/27 04:22:01] vclr INFO: Train: [ 15]/[ 220/ 287] BT=7.973/7.033 Loss=12.193 5.557 0.592 1.654 4.390/12.727
[07/27 04:23:15] vclr INFO: Train: [ 15]/[ 230/ 287] BT=7.650/7.048 Loss=11.812 5.613 0.487 1.388 4.323/12.720
[07/27 04:24:30] vclr INFO: Train: [ 15]/[ 240/ 287] BT=7.507/7.066 Loss=12.463 6.033 0.436 1.431 4.563/12.714
[07/27 04:25:43] vclr INFO: Train: [ 15]/[ 250/ 287] BT=6.757/7.078 Loss=13.066 6.465 0.470 1.503 4.627/12.708
[07/27 04:26:56] vclr INFO: Train: [ 15]/[ 260/ 287] BT=6.928/7.083 Loss=13.393 6.380 0.343 1.868 4.802/12.699
[07/27 04:28:07] vclr INFO: Train: [ 15]/[ 270/ 287] BT=7.528/7.087 Loss=13.220 6.120 0.647 1.755 4.698/12.719
[07/27 04:29:19] vclr INFO: Train: [ 15]/[ 280/ 287] BT=7.780/7.090 Loss=13.209 6.328 0.796 1.635 4.450/12.734
[07/27 04:30:06] vclr INFO: epoch 15, total time 2041.58, loss=12.735561108339954
[07/27 04:30:06] vclr INFO: ==> Saving...
[07/27 04:31:38] vclr INFO: Train: [ 16]/[   0/ 287] BT=90.373/90.373 Loss=12.161 5.729 0.343 1.460 4.628/12.161
[07/27 04:32:54] vclr INFO: Train: [ 16]/[  10/ 287] BT=7.915/15.111 Loss=12.097 5.973 0.409 1.357 4.358/12.414
[07/27 04:34:07] vclr INFO: Train: [ 16]/[  20/ 287] BT=7.792/11.418 Loss=12.860 6.114 0.398 2.002 4.346/12.520
[07/27 04:35:18] vclr INFO: Train: [ 16]/[  30/ 287] BT=7.332/10.029 Loss=12.223 5.781 0.544 1.451 4.446/12.468
[07/27 04:36:34] vclr INFO: Train: [ 16]/[  40/ 287] BT=6.872/9.421 Loss=12.639 5.995 0.396 1.554 4.694/12.454
[07/27 04:37:44] vclr INFO: Train: [ 16]/[  50/ 287] BT=6.826/8.961 Loss=12.252 5.767 0.347 1.775 4.363/12.465
[07/27 04:38:54] vclr INFO: Train: [ 16]/[  60/ 287] BT=6.739/8.641 Loss=12.089 5.589 0.490 1.973 4.038/12.439
[07/27 04:40:02] vclr INFO: Train: [ 16]/[  70/ 287] BT=6.609/8.371 Loss=12.186 5.698 0.414 1.796 4.278/12.434
[07/27 04:41:14] vclr INFO: Train: [ 16]/[  80/ 287] BT=7.406/8.227 Loss=12.442 5.855 0.843 1.337 4.407/12.398
[07/27 04:42:23] vclr INFO: Train: [ 16]/[  90/ 287] BT=7.133/8.086 Loss=12.616 5.846 0.418 1.690 4.662/12.389
[07/27 04:43:37] vclr INFO: Train: [ 16]/[ 100/ 287] BT=7.571/8.016 Loss=12.027 5.619 0.386 1.560 4.462/12.389
[07/27 04:44:52] vclr INFO: Train: [ 16]/[ 110/ 287] BT=7.384/7.971 Loss=13.925 6.610 0.571 1.782 4.961/12.399
[07/27 04:46:04] vclr INFO: Train: [ 16]/[ 120/ 287] BT=7.530/7.906 Loss=12.908 6.511 0.353 1.291 4.753/12.411
[07/27 04:47:13] vclr INFO: Train: [ 16]/[ 130/ 287] BT=6.923/7.828 Loss=13.608 6.767 0.455 1.527 4.859/12.429
[07/27 04:48:20] vclr INFO: Train: [ 16]/[ 140/ 287] BT=5.652/7.750 Loss=13.242 6.587 0.392 1.448 4.815/12.445
[07/27 04:49:33] vclr INFO: Train: [ 16]/[ 150/ 287] BT=7.470/7.722 Loss=12.700 6.031 0.377 1.407 4.885/12.464
[07/27 04:50:45] vclr INFO: Train: [ 16]/[ 160/ 287] BT=7.933/7.685 Loss=12.996 6.149 0.288 2.011 4.547/12.474
[07/27 04:51:57] vclr INFO: Train: [ 16]/[ 170/ 287] BT=7.668/7.658 Loss=12.455 6.219 0.387 1.650 4.199/12.479
[07/27 04:53:08] vclr INFO: Train: [ 16]/[ 180/ 287] BT=6.340/7.630 Loss=13.179 6.431 0.319 1.497 4.933/12.483
[07/27 04:54:21] vclr INFO: Train: [ 16]/[ 190/ 287] BT=7.160/7.612 Loss=13.058 6.333 0.264 1.416 5.046/12.484
[07/27 04:55:30] vclr INFO: Train: [ 16]/[ 200/ 287] BT=6.688/7.574 Loss=12.221 6.412 0.271 1.398 4.141/12.486
[07/27 04:56:42] vclr INFO: Train: [ 16]/[ 210/ 287] BT=6.652/7.556 Loss=13.033 6.467 0.240 1.597 4.729/12.486
[07/27 04:57:59] vclr INFO: Train: [ 16]/[ 220/ 287] BT=8.379/7.565 Loss=13.177 6.367 0.620 2.074 4.115/12.489
[07/27 04:59:09] vclr INFO: Train: [ 16]/[ 230/ 287] BT=7.547/7.541 Loss=13.118 6.804 0.471 1.461 4.382/12.493
[07/27 05:00:18] vclr INFO: Train: [ 16]/[ 240/ 287] BT=7.343/7.513 Loss=12.149 5.672 0.333 1.813 4.331/12.497
[07/27 05:01:33] vclr INFO: Train: [ 16]/[ 250/ 287] BT=8.046/7.512 Loss=11.895 5.850 0.447 1.325 4.273/12.501
[07/27 05:02:43] vclr INFO: Train: [ 16]/[ 260/ 287] BT=6.663/7.494 Loss=12.683 6.434 0.388 1.404 4.456/12.504
[07/27 05:03:55] vclr INFO: Train: [ 16]/[ 270/ 287] BT=6.874/7.482 Loss=14.276 6.797 0.484 1.512 5.482/12.510
[07/27 05:05:02] vclr INFO: Train: [ 16]/[ 280/ 287] BT=6.443/7.454 Loss=12.273 5.968 0.306 1.498 4.501/12.505
[07/27 05:05:50] vclr INFO: epoch 16, total time 2144.50, loss=12.505404631850611
[07/27 05:05:50] vclr INFO: ==> Saving...
[07/27 05:07:14] vclr INFO: Train: [ 17]/[   0/ 287] BT=81.288/81.288 Loss=12.076 5.878 0.451 1.592 4.156/12.076
[07/27 05:08:30] vclr INFO: Train: [ 17]/[  10/ 287] BT=7.913/14.237 Loss=12.795 6.235 0.295 1.588 4.677/12.363
[07/27 05:09:43] vclr INFO: Train: [ 17]/[  20/ 287] BT=7.051/10.965 Loss=12.834 6.168 0.279 1.486 4.901/12.509
[07/27 05:10:57] vclr INFO: Train: [ 17]/[  30/ 287] BT=6.901/9.803 Loss=12.405 6.159 0.273 1.468 4.505/12.520
[07/27 05:12:13] vclr INFO: Train: [ 17]/[  40/ 287] BT=7.394/9.261 Loss=12.189 5.545 0.603 1.788 4.253/12.567
[07/27 05:13:28] vclr INFO: Train: [ 17]/[  50/ 287] BT=8.118/8.914 Loss=12.036 5.695 0.491 1.407 4.442/12.528
[07/27 05:14:44] vclr INFO: Train: [ 17]/[  60/ 287] BT=7.542/8.698 Loss=12.762 6.079 0.350 1.719 4.614/12.511
[07/27 05:15:57] vclr INFO: Train: [ 17]/[  70/ 287] BT=7.006/8.512 Loss=12.601 6.074 0.357 1.518 4.652/12.508
[07/27 05:17:13] vclr INFO: Train: [ 17]/[  80/ 287] BT=7.665/8.392 Loss=11.971 5.418 0.511 1.633 4.409/12.496
[07/27 05:18:30] vclr INFO: Train: [ 17]/[  90/ 287] BT=8.058/8.322 Loss=13.324 6.358 0.554 1.526 4.887/12.490
[07/27 05:19:44] vclr INFO: Train: [ 17]/[ 100/ 287] BT=7.172/8.232 Loss=12.857 6.371 0.286 1.275 4.925/12.481
[07/27 05:21:03] vclr INFO: Train: [ 17]/[ 110/ 287] BT=7.804/8.201 Loss=13.019 6.075 0.367 2.102 4.474/12.468
[07/27 05:22:19] vclr INFO: Train: [ 17]/[ 120/ 287] BT=7.285/8.153 Loss=12.169 5.861 0.378 1.343 4.587/12.479
[07/27 05:23:35] vclr INFO: Train: [ 17]/[ 130/ 287] BT=7.989/8.110 Loss=11.819 5.938 0.384 1.406 4.092/12.477
[07/27 05:24:52] vclr INFO: Train: [ 17]/[ 140/ 287] BT=7.626/8.080 Loss=12.094 5.737 0.418 1.576 4.362/12.477
[07/27 05:26:07] vclr INFO: Train: [ 17]/[ 150/ 287] BT=7.834/8.041 Loss=12.853 6.174 0.371 1.923 4.385/12.492
[07/27 05:27:24] vclr INFO: Train: [ 17]/[ 160/ 287] BT=7.952/8.017 Loss=13.021 6.405 0.363 1.558 4.695/12.486
[07/27 05:28:40] vclr INFO: Train: [ 17]/[ 170/ 287] BT=8.468/7.993 Loss=12.379 6.195 0.298 1.484 4.401/12.484
[07/27 05:29:57] vclr INFO: Train: [ 17]/[ 180/ 287] BT=7.438/7.980 Loss=12.966 6.418 0.286 1.350 4.911/12.484
[07/27 05:31:12] vclr INFO: Train: [ 17]/[ 190/ 287] BT=7.785/7.954 Loss=12.633 6.094 0.516 1.601 4.423/12.481
[07/27 05:32:30] vclr INFO: Train: [ 17]/[ 200/ 287] BT=8.062/7.943 Loss=12.711 6.155 0.315 1.683 4.558/12.477
[07/27 05:33:47] vclr INFO: Train: [ 17]/[ 210/ 287] BT=7.587/7.932 Loss=12.438 6.175 0.313 1.564 4.385/12.487
[07/27 05:35:01] vclr INFO: Train: [ 17]/[ 220/ 287] BT=7.266/7.910 Loss=12.453 5.474 0.561 2.104 4.313/12.498
[07/27 05:36:16] vclr INFO: Train: [ 17]/[ 230/ 287] BT=6.392/7.891 Loss=11.968 5.459 0.570 2.034 3.905/12.495
[07/27 05:37:35] vclr INFO: Train: [ 17]/[ 240/ 287] BT=8.804/7.892 Loss=12.196 5.933 0.268 1.414 4.581/12.513
[07/27 05:38:53] vclr INFO: Train: [ 17]/[ 250/ 287] BT=7.677/7.887 Loss=11.867 5.548 0.354 1.553 4.412/12.509
[07/27 05:40:06] vclr INFO: Train: [ 17]/[ 260/ 287] BT=7.046/7.868 Loss=12.856 6.256 0.498 1.509 4.592/12.502
[07/27 05:41:20] vclr INFO: Train: [ 17]/[ 270/ 287] BT=7.394/7.851 Loss=12.150 5.537 0.610 1.724 4.278/12.501
[07/27 05:42:34] vclr INFO: Train: [ 17]/[ 280/ 287] BT=7.252/7.833 Loss=12.993 5.995 0.326 1.595 5.076/12.502
[07/27 05:43:23] vclr INFO: epoch 17, total time 2253.08, loss=12.508141710367768
[07/27 05:43:23] vclr INFO: ==> Saving...
[07/27 05:45:05] vclr INFO: Train: [ 18]/[   0/ 287] BT=99.452/99.452 Loss=12.848 6.127 0.383 1.593 4.745/12.848
[07/27 05:46:21] vclr INFO: Train: [ 18]/[  10/ 287] BT=6.828/15.884 Loss=12.489 6.278 0.435 1.609 4.168/12.566
[07/27 05:47:38] vclr INFO: Train: [ 18]/[  20/ 287] BT=7.449/12.010 Loss=13.064 6.172 0.402 1.706 4.783/12.527
[07/27 05:48:58] vclr INFO: Train: [ 18]/[  30/ 287] BT=8.310/10.715 Loss=12.289 5.962 0.385 1.489 4.453/12.501
[07/27 05:50:14] vclr INFO: Train: [ 18]/[  40/ 287] BT=6.984/9.949 Loss=12.664 6.150 0.405 1.442 4.668/12.508
[07/27 05:51:32] vclr INFO: Train: [ 18]/[  50/ 287] BT=7.997/9.539 Loss=12.911 5.947 0.757 1.534 4.672/12.527
[07/27 05:52:48] vclr INFO: Train: [ 18]/[  60/ 287] BT=7.734/9.220 Loss=12.591 5.984 0.434 1.676 4.497/12.497
[07/27 05:54:06] vclr INFO: Train: [ 18]/[  70/ 287] BT=8.288/9.021 Loss=13.081 6.400 0.364 1.549 4.767/12.513
[07/27 05:55:24] vclr INFO: Train: [ 18]/[  80/ 287] BT=7.648/8.867 Loss=11.812 5.492 0.476 1.568 4.276/12.492
[07/27 05:56:41] vclr INFO: Train: [ 18]/[  90/ 287] BT=7.741/8.734 Loss=13.085 6.899 0.340 1.300 4.547/12.481
[07/27 05:57:58] vclr INFO: Train: [ 18]/[ 100/ 287] BT=7.686/8.638 Loss=12.900 6.281 0.514 1.604 4.501/12.498
[07/27 05:59:18] vclr INFO: Train: [ 18]/[ 110/ 287] BT=8.363/8.575 Loss=11.490 5.388 0.507 1.404 4.191/12.486
[07/27 06:00:35] vclr INFO: Train: [ 18]/[ 120/ 287] BT=7.759/8.507 Loss=11.721 5.765 0.360 1.172 4.425/12.467
[07/27 06:01:53] vclr INFO: Train: [ 18]/[ 130/ 287] BT=7.368/8.451 Loss=12.735 6.157 0.446 1.699 4.433/12.473
[07/27 06:03:11] vclr INFO: Train: [ 18]/[ 140/ 287] BT=7.741/8.401 Loss=12.708 5.712 0.411 2.286 4.299/12.492
[07/27 06:04:28] vclr INFO: Train: [ 18]/[ 150/ 287] BT=8.097/8.357 Loss=12.590 5.855 0.597 1.583 4.555/12.507
[07/27 06:05:46] vclr INFO: Train: [ 18]/[ 160/ 287] BT=7.773/8.324 Loss=13.251 6.589 0.423 1.254 4.985/12.530
[07/27 06:07:07] vclr INFO: Train: [ 18]/[ 170/ 287] BT=7.670/8.309 Loss=13.006 6.456 0.317 1.639 4.595/12.527
[07/27 06:08:24] vclr INFO: Train: [ 18]/[ 180/ 287] BT=7.477/8.274 Loss=13.039 6.326 0.433 1.573 4.708/12.527
[07/27 06:09:41] vclr INFO: Train: [ 18]/[ 190/ 287] BT=8.063/8.247 Loss=12.626 6.057 0.374 1.891 4.304/12.533
[07/27 06:10:57] vclr INFO: Train: [ 18]/[ 200/ 287] BT=7.942/8.216 Loss=11.644 5.598 0.379 1.547 4.119/12.520
[07/27 06:12:15] vclr INFO: Train: [ 18]/[ 210/ 287] BT=7.853/8.196 Loss=12.384 5.997 0.456 1.534 4.397/12.525
[07/27 06:13:34] vclr INFO: Train: [ 18]/[ 220/ 287] BT=7.999/8.181 Loss=12.832 6.121 0.346 1.680 4.685/12.526
[07/27 06:14:51] vclr INFO: Train: [ 18]/[ 230/ 287] BT=7.114/8.161 Loss=12.839 6.153 0.361 1.795 4.529/12.525
[07/27 06:16:10] vclr INFO: Train: [ 18]/[ 240/ 287] BT=7.256/8.149 Loss=13.202 6.088 0.409 1.621 5.084/12.521
[07/27 06:17:28] vclr INFO: Train: [ 18]/[ 250/ 287] BT=7.652/8.136 Loss=13.213 6.464 0.299 1.511 4.939/12.529
[07/27 06:18:46] vclr INFO: Train: [ 18]/[ 260/ 287] BT=7.985/8.121 Loss=12.229 5.875 0.373 1.690 4.291/12.523
[07/27 06:20:05] vclr INFO: Train: [ 18]/[ 270/ 287] BT=7.968/8.115 Loss=12.590 6.030 0.348 1.363 4.849/12.517
[07/27 06:21:23] vclr INFO: Train: [ 18]/[ 280/ 287] BT=8.343/8.104 Loss=11.843 5.596 0.335 1.619 4.293/12.508
[07/27 06:22:14] vclr INFO: epoch 18, total time 2330.23, loss=12.50742213485133
[07/27 06:22:14] vclr INFO: ==> Saving...
[07/27 06:23:50] vclr INFO: Train: [ 19]/[   0/ 287] BT=93.973/93.973 Loss=11.639 5.816 0.528 1.259 4.037/11.639
[07/27 06:25:08] vclr INFO: Train: [ 19]/[  10/ 287] BT=8.672/15.677 Loss=11.747 5.805 0.311 1.496 4.134/12.188
[07/27 06:26:27] vclr INFO: Train: [ 19]/[  20/ 287] BT=8.611/11.961 Loss=11.630 5.780 0.404 1.273 4.173/12.354
[07/27 06:27:50] vclr INFO: Train: [ 19]/[  30/ 287] BT=8.216/10.765 Loss=12.245 5.681 0.594 1.638 4.332/12.349
[07/27 06:29:09] vclr INFO: Train: [ 19]/[  40/ 287] BT=7.881/10.066 Loss=12.802 6.549 0.238 1.289 4.726/12.363
[07/27 06:30:26] vclr INFO: Train: [ 19]/[  50/ 287] BT=8.448/9.610 Loss=11.757 5.790 0.400 1.381 4.186/12.354
[07/27 06:31:46] vclr INFO: Train: [ 19]/[  60/ 287] BT=7.796/9.352 Loss=11.590 5.617 0.397 1.353 4.224/12.384
[07/27 06:33:04] vclr INFO: Train: [ 19]/[  70/ 287] BT=6.732/9.127 Loss=12.918 6.244 0.197 1.612 4.865/12.432
[07/27 06:34:19] vclr INFO: Train: [ 19]/[  80/ 287] BT=8.505/8.924 Loss=12.311 5.531 0.798 1.819 4.162/12.431
[07/27 06:35:34] vclr INFO: Train: [ 19]/[  90/ 287] BT=6.989/8.765 Loss=12.049 5.767 0.340 1.938 4.004/12.466
[07/27 06:36:52] vclr INFO: Train: [ 19]/[ 100/ 287] BT=8.138/8.678 Loss=11.872 5.491 0.436 1.648 4.297/12.467
[07/27 06:38:12] vclr INFO: Train: [ 19]/[ 110/ 287] BT=7.471/8.616 Loss=12.293 5.857 0.489 1.380 4.566/12.458
[07/27 06:39:30] vclr INFO: Train: [ 19]/[ 120/ 287] BT=7.581/8.547 Loss=12.382 6.054 0.381 1.537 4.409/12.494
[07/27 06:40:49] vclr INFO: Train: [ 19]/[ 130/ 287] BT=8.528/8.500 Loss=11.482 5.749 0.336 1.450 3.947/12.486
[07/27 06:42:08] vclr INFO: Train: [ 19]/[ 140/ 287] BT=8.210/8.453 Loss=12.441 5.990 0.291 1.449 4.711/12.478
[07/27 06:43:25] vclr INFO: Train: [ 19]/[ 150/ 287] BT=7.202/8.405 Loss=12.090 6.103 0.381 1.540 4.067/12.470
[07/27 06:44:49] vclr INFO: Train: [ 19]/[ 160/ 287] BT=8.681/8.403 Loss=12.302 6.040 0.542 1.276 4.445/12.468
[07/27 06:46:08] vclr INFO: Train: [ 19]/[ 170/ 287] BT=7.309/8.373 Loss=11.712 5.418 0.489 1.773 4.031/12.473
[07/27 06:47:25] vclr INFO: Train: [ 19]/[ 180/ 287] BT=7.430/8.339 Loss=11.735 5.936 0.398 1.304 4.097/12.473
[07/27 06:48:42] vclr INFO: Train: [ 19]/[ 190/ 287] BT=8.253/8.305 Loss=12.325 5.710 0.406 2.091 4.118/12.476
[07/27 06:50:01] vclr INFO: Train: [ 19]/[ 200/ 287] BT=8.335/8.285 Loss=13.025 6.184 0.460 1.739 4.642/12.482
[07/27 06:51:21] vclr INFO: Train: [ 19]/[ 210/ 287] BT=7.660/8.271 Loss=12.265 5.813 0.401 1.855 4.196/12.469
[07/27 06:52:38] vclr INFO: Train: [ 19]/[ 220/ 287] BT=8.365/8.244 Loss=12.061 6.125 0.385 1.445 4.106/12.467
[07/27 06:53:56] vclr INFO: Train: [ 19]/[ 230/ 287] BT=8.241/8.224 Loss=11.387 5.421 0.581 1.551 3.834/12.445
[07/27 06:55:17] vclr INFO: Train: [ 19]/[ 240/ 287] BT=8.066/8.219 Loss=12.804 6.261 0.336 1.684 4.524/12.436
[07/27 06:56:37] vclr INFO: Train: [ 19]/[ 250/ 287] BT=7.636/8.212 Loss=12.486 5.841 0.556 1.963 4.126/12.436
[07/27 06:57:57] vclr INFO: Train: [ 19]/[ 260/ 287] BT=7.640/8.203 Loss=12.021 5.877 0.365 1.408 4.371/12.432
[07/27 06:59:15] vclr INFO: Train: [ 19]/[ 270/ 287] BT=6.933/8.187 Loss=11.378 5.478 0.398 1.321 4.180/12.426
[07/27 07:00:30] vclr INFO: Train: [ 19]/[ 280/ 287] BT=7.834/8.165 Loss=11.913 5.835 0.256 1.334 4.488/12.430
[07/27 07:01:21] vclr INFO: epoch 19, total time 2347.66, loss=12.43963363860127
[07/27 07:01:21] vclr INFO: ==> Saving...
[07/27 07:02:40] vclr INFO: Train: [ 20]/[   0/ 287] BT=76.590/76.590 Loss=11.785 5.485 0.683 1.370 4.248/11.785
[07/27 07:03:55] vclr INFO: Train: [ 20]/[  10/ 287] BT=7.487/13.809 Loss=13.093 6.500 0.332 1.578 4.683/12.285
[07/27 07:05:10] vclr INFO: Train: [ 20]/[  20/ 287] BT=7.765/10.811 Loss=11.947 6.225 0.317 1.456 3.949/12.276
[07/27 07:06:24] vclr INFO: Train: [ 20]/[  30/ 287] BT=7.838/9.708 Loss=12.224 6.068 0.383 1.527 4.246/12.260
[07/27 07:07:38] vclr INFO: Train: [ 20]/[  40/ 287] BT=7.798/9.148 Loss=11.888 5.890 0.351 1.404 4.243/12.279
[07/27 07:08:53] vclr INFO: Train: [ 20]/[  50/ 287] BT=7.576/8.819 Loss=12.048 5.792 0.319 1.531 4.405/12.279
[07/27 07:10:09] vclr INFO: Train: [ 20]/[  60/ 287] BT=7.658/8.611 Loss=12.126 5.955 0.322 1.668 4.181/12.258
[07/27 07:11:24] vclr INFO: Train: [ 20]/[  70/ 287] BT=7.911/8.466 Loss=12.368 6.113 0.407 1.431 4.417/12.302
[07/27 07:12:39] vclr INFO: Train: [ 20]/[  80/ 287] BT=7.460/8.341 Loss=12.733 6.286 0.359 1.273 4.815/12.299
[07/27 07:13:54] vclr INFO: Train: [ 20]/[  90/ 287] BT=6.839/8.244 Loss=12.749 6.253 0.399 1.488 4.609/12.306
[07/27 07:15:08] vclr INFO: Train: [ 20]/[ 100/ 287] BT=7.103/8.160 Loss=12.962 6.682 0.317 1.457 4.507/12.322
[07/27 07:16:22] vclr INFO: Train: [ 20]/[ 110/ 287] BT=7.618/8.099 Loss=12.889 6.166 0.439 1.430 4.854/12.331
[07/27 07:17:38] vclr INFO: Train: [ 20]/[ 120/ 287] BT=7.059/8.053 Loss=12.711 6.352 0.402 1.786 4.170/12.344
[07/27 07:18:52] vclr INFO: Train: [ 20]/[ 130/ 287] BT=7.760/8.007 Loss=13.129 6.361 0.320 2.014 4.434/12.359
[07/27 07:20:08] vclr INFO: Train: [ 20]/[ 140/ 287] BT=7.625/7.979 Loss=11.993 5.575 0.651 1.464 4.303/12.387
[07/27 07:21:24] vclr INFO: Train: [ 20]/[ 150/ 287] BT=6.975/7.948 Loss=12.994 6.446 0.276 1.532 4.741/12.383
[07/27 07:22:39] vclr INFO: Train: [ 20]/[ 160/ 287] BT=7.882/7.921 Loss=11.737 5.830 0.303 1.436 4.169/12.377
[07/27 07:23:53] vclr INFO: Train: [ 20]/[ 170/ 287] BT=7.586/7.891 Loss=13.042 6.086 0.527 1.485 4.943/12.391
[07/27 07:25:07] vclr INFO: Train: [ 20]/[ 180/ 287] BT=7.414/7.865 Loss=12.794 6.247 0.272 1.748 4.527/12.386
[07/27 07:26:22] vclr INFO: Train: [ 20]/[ 190/ 287] BT=7.160/7.846 Loss=11.929 5.629 0.460 1.619 4.221/12.402
[07/27 07:27:37] vclr INFO: Train: [ 20]/[ 200/ 287] BT=7.888/7.827 Loss=12.271 5.521 0.710 1.654 4.386/12.408
[07/27 07:28:52] vclr INFO: Train: [ 20]/[ 210/ 287] BT=7.273/7.812 Loss=11.826 5.949 0.277 1.501 4.099/12.405
[07/27 07:30:06] vclr INFO: Train: [ 20]/[ 220/ 287] BT=7.114/7.794 Loss=12.548 6.258 0.227 1.456 4.607/12.394
[07/27 07:31:20] vclr INFO: Train: [ 20]/[ 230/ 287] BT=7.166/7.779 Loss=11.547 5.878 0.415 1.234 4.021/12.396
[07/27 07:32:35] vclr INFO: Train: [ 20]/[ 240/ 287] BT=7.722/7.768 Loss=12.195 6.082 0.344 1.357 4.412/12.398
[07/27 07:33:51] vclr INFO: Train: [ 20]/[ 250/ 287] BT=7.502/7.758 Loss=13.836 6.531 0.272 1.846 5.186/12.406
[07/27 07:35:06] vclr INFO: Train: [ 20]/[ 260/ 287] BT=7.463/7.750 Loss=12.376 6.243 0.345 1.595 4.193/12.414
[07/27 07:36:21] vclr INFO: Train: [ 20]/[ 270/ 287] BT=7.550/7.741 Loss=12.834 6.096 0.367 2.077 4.294/12.407
[07/27 07:37:35] vclr INFO: Train: [ 20]/[ 280/ 287] BT=7.286/7.729 Loss=11.820 5.435 0.357 1.827 4.201/12.402
[07/27 07:38:25] vclr INFO: epoch 20, total time 2224.21, loss=12.397060709963277
[07/27 07:38:25] vclr INFO: ==> Saving...
[07/27 07:39:54] vclr INFO: Train: [ 21]/[   0/ 287] BT=82.636/82.636 Loss=12.104 6.082 0.510 1.321 4.190/12.104
[07/27 07:41:07] vclr INFO: Train: [ 21]/[  10/ 287] BT=7.219/14.123 Loss=13.810 6.542 0.341 1.710 5.217/12.263
[07/27 07:42:22] vclr INFO: Train: [ 21]/[  20/ 287] BT=7.447/10.946 Loss=11.603 5.626 0.488 1.512 3.978/12.367
[07/27 07:43:35] vclr INFO: Train: [ 21]/[  30/ 287] BT=7.611/9.794 Loss=11.226 5.428 0.435 1.288 4.074/12.292
[07/27 07:44:49] vclr INFO: Train: [ 21]/[  40/ 287] BT=7.401/9.207 Loss=12.948 6.374 0.347 1.484 4.743/12.344
[07/27 07:46:04] vclr INFO: Train: [ 21]/[  50/ 287] BT=7.641/8.862 Loss=10.971 5.042 0.410 1.501 4.018/12.333
[07/27 07:47:17] vclr INFO: Train: [ 21]/[  60/ 287] BT=7.195/8.612 Loss=11.485 5.633 0.311 1.357 4.185/12.336
[07/27 07:48:31] vclr INFO: Train: [ 21]/[  70/ 287] BT=7.414/8.443 Loss=11.687 5.671 0.381 1.620 4.015/12.352
[07/27 07:49:45] vclr INFO: Train: [ 21]/[  80/ 287] BT=7.460/8.313 Loss=12.827 5.928 0.436 2.085 4.378/12.349
[07/27 07:50:58] vclr INFO: Train: [ 21]/[  90/ 287] BT=7.258/8.203 Loss=12.846 5.950 0.514 1.780 4.602/12.410
[07/27 07:52:12] vclr INFO: Train: [ 21]/[ 100/ 287] BT=7.209/8.121 Loss=11.750 5.407 0.657 1.516 4.169/12.415
[07/27 07:53:26] vclr INFO: Train: [ 21]/[ 110/ 287] BT=7.296/8.054 Loss=12.961 6.264 0.369 1.436 4.892/12.425
[07/27 07:54:39] vclr INFO: Train: [ 21]/[ 120/ 287] BT=7.788/7.996 Loss=11.738 5.847 0.359 1.456 4.076/12.393
[07/27 07:55:53] vclr INFO: Train: [ 21]/[ 130/ 287] BT=7.782/7.948 Loss=11.665 5.683 0.424 1.346 4.211/12.381
[07/27 07:57:06] vclr INFO: Train: [ 21]/[ 140/ 287] BT=7.768/7.904 Loss=12.495 5.996 0.441 1.442 4.615/12.363
[07/27 07:58:20] vclr INFO: Train: [ 21]/[ 150/ 287] BT=7.647/7.869 Loss=12.835 5.905 0.458 2.138 4.334/12.352
[07/27 07:59:34] vclr INFO: Train: [ 21]/[ 160/ 287] BT=7.593/7.839 Loss=12.217 6.392 0.195 1.553 4.077/12.343
[07/27 08:00:47] vclr INFO: Train: [ 21]/[ 170/ 287] BT=7.364/7.809 Loss=12.100 5.439 0.685 1.575 4.401/12.346
[07/27 08:02:01] vclr INFO: Train: [ 21]/[ 180/ 287] BT=7.435/7.787 Loss=13.682 6.321 0.375 2.596 4.389/12.370
[07/27 08:03:15] vclr INFO: Train: [ 21]/[ 190/ 287] BT=7.182/7.765 Loss=12.388 6.307 0.340 1.502 4.240/12.383
[07/27 08:04:29] vclr INFO: Train: [ 21]/[ 200/ 287] BT=6.995/7.745 Loss=12.773 6.262 0.312 2.038 4.162/12.390
[07/27 08:05:43] vclr INFO: Train: [ 21]/[ 210/ 287] BT=7.200/7.730 Loss=12.210 6.312 0.310 1.177 4.411/12.407
[07/27 08:06:57] vclr INFO: Train: [ 21]/[ 220/ 287] BT=7.353/7.718 Loss=12.682 5.875 0.481 1.843 4.484/12.401
[07/27 08:08:12] vclr INFO: Train: [ 21]/[ 230/ 287] BT=7.800/7.705 Loss=14.008 6.906 0.581 1.894 4.627/12.423
[07/27 08:09:25] vclr INFO: Train: [ 21]/[ 240/ 287] BT=7.544/7.689 Loss=12.050 5.778 0.292 1.545 4.435/12.427
[07/27 08:10:38] vclr INFO: Train: [ 21]/[ 250/ 287] BT=7.653/7.672 Loss=13.361 6.429 0.445 1.866 4.621/12.439
[07/27 08:11:51] vclr INFO: Train: [ 21]/[ 260/ 287] BT=7.592/7.660 Loss=11.536 5.734 0.309 1.291 4.202/12.447
[07/27 08:13:06] vclr INFO: Train: [ 21]/[ 270/ 287] BT=7.227/7.653 Loss=12.320 5.803 0.371 2.045 4.101/12.460
[07/27 08:14:18] vclr INFO: Train: [ 21]/[ 280/ 287] BT=6.865/7.637 Loss=12.585 5.854 0.347 1.915 4.469/12.465
[07/27 08:15:07] vclr INFO: epoch 21, total time 2201.48, loss=12.457505388957697
[07/27 08:15:07] vclr INFO: ==> Saving...
[07/27 08:16:11] vclr INFO: Train: [ 22]/[   0/ 287] BT=62.216/62.216 Loss=12.094 5.850 0.412 1.490 4.341/12.094
[07/27 08:17:27] vclr INFO: Train: [ 22]/[  10/ 287] BT=8.165/12.489 Loss=12.672 6.171 0.339 1.355 4.807/12.614
[07/27 08:18:39] vclr INFO: Train: [ 22]/[  20/ 287] BT=7.355/10.005 Loss=12.417 5.842 0.322 1.460 4.792/12.456
[07/27 08:19:50] vclr INFO: Train: [ 22]/[  30/ 287] BT=7.360/9.061 Loss=12.096 5.974 0.290 1.479 4.353/12.382
[07/27 08:21:04] vclr INFO: Train: [ 22]/[  40/ 287] BT=7.699/8.646 Loss=11.569 5.339 0.431 1.424 4.375/12.366
[07/27 08:22:16] vclr INFO: Train: [ 22]/[  50/ 287] BT=6.983/8.378 Loss=11.285 5.389 0.470 1.380 4.046/12.371
[07/27 08:23:30] vclr INFO: Train: [ 22]/[  60/ 287] BT=7.572/8.208 Loss=11.686 5.726 0.422 1.393 4.145/12.363
[07/27 08:24:44] vclr INFO: Train: [ 22]/[  70/ 287] BT=6.809/8.100 Loss=12.607 6.121 0.282 1.742 4.462/12.341
[07/27 08:26:00] vclr INFO: Train: [ 22]/[  80/ 287] BT=7.480/8.033 Loss=12.738 6.286 0.273 1.572 4.607/12.391
[07/27 08:27:15] vclr INFO: Train: [ 22]/[  90/ 287] BT=7.681/7.974 Loss=12.037 5.455 0.547 1.947 4.088/12.412
[07/27 08:28:29] vclr INFO: Train: [ 22]/[ 100/ 287] BT=7.922/7.923 Loss=13.016 6.213 0.478 1.650 4.674/12.403
[07/27 08:29:43] vclr INFO: Train: [ 22]/[ 110/ 287] BT=7.777/7.876 Loss=11.958 5.786 0.367 1.613 4.191/12.392
[07/27 08:30:58] vclr INFO: Train: [ 22]/[ 120/ 287] BT=7.937/7.846 Loss=12.870 6.294 0.329 1.504 4.743/12.386
[07/27 08:32:15] vclr INFO: Train: [ 22]/[ 130/ 287] BT=7.921/7.828 Loss=12.934 6.182 0.244 1.839 4.668/12.400
[07/27 08:33:32] vclr INFO: Train: [ 22]/[ 140/ 287] BT=7.955/7.819 Loss=12.483 5.955 0.429 1.556 4.543/12.405
[07/27 08:34:47] vclr INFO: Train: [ 22]/[ 150/ 287] BT=7.934/7.803 Loss=13.652 6.353 0.354 2.425 4.520/12.427
[07/27 08:36:02] vclr INFO: Train: [ 22]/[ 160/ 287] BT=7.374/7.784 Loss=12.676 6.468 0.414 1.538 4.256/12.416
[07/27 08:37:17] vclr INFO: Train: [ 22]/[ 170/ 287] BT=7.412/7.767 Loss=11.875 5.459 0.540 1.766 4.110/12.411
[07/27 08:38:33] vclr INFO: Train: [ 22]/[ 180/ 287] BT=7.379/7.757 Loss=12.646 6.202 0.408 1.443 4.593/12.393
[07/27 08:39:48] vclr INFO: Train: [ 22]/[ 190/ 287] BT=7.459/7.740 Loss=12.751 6.289 0.219 1.364 4.878/12.397
[07/27 08:41:03] vclr INFO: Train: [ 22]/[ 200/ 287] BT=8.284/7.729 Loss=12.137 6.135 0.577 1.343 4.082/12.397
[07/27 08:42:19] vclr INFO: Train: [ 22]/[ 210/ 287] BT=7.666/7.722 Loss=12.246 5.724 0.646 1.773 4.104/12.407
[07/27 08:43:35] vclr INFO: Train: [ 22]/[ 220/ 287] BT=7.628/7.721 Loss=12.323 5.524 0.555 2.300 3.943/12.402
[07/27 08:44:49] vclr INFO: Train: [ 22]/[ 230/ 287] BT=7.268/7.707 Loss=12.450 6.107 0.804 1.416 4.124/12.403
[07/27 08:46:06] vclr INFO: Train: [ 22]/[ 240/ 287] BT=7.861/7.705 Loss=12.504 6.174 0.393 1.483 4.455/12.410
[07/27 08:47:21] vclr INFO: Train: [ 22]/[ 250/ 287] BT=8.017/7.698 Loss=12.739 5.999 0.317 1.307 5.116/12.415
[07/27 08:48:36] vclr INFO: Train: [ 22]/[ 260/ 287] BT=7.675/7.690 Loss=12.224 5.503 0.454 2.029 4.238/12.440
[07/27 08:49:51] vclr INFO: Train: [ 22]/[ 270/ 287] BT=7.717/7.683 Loss=13.455 6.332 0.407 2.255 4.460/12.458
[07/27 08:51:07] vclr INFO: Train: [ 22]/[ 280/ 287] BT=7.132/7.678 Loss=11.978 5.766 0.316 1.538 4.357/12.464
[07/27 08:51:57] vclr INFO: epoch 22, total time 2209.83, loss=12.46290031127398
[07/27 08:51:57] vclr INFO: ==> Saving...
[07/27 08:53:08] vclr INFO: Train: [ 23]/[   0/ 287] BT=69.072/69.072 Loss=12.052 5.723 0.403 1.354 4.572/12.052
[07/27 08:54:29] vclr INFO: Train: [ 23]/[  10/ 287] BT=8.426/13.651 Loss=12.344 5.708 0.462 2.004 4.171/12.371
[07/27 08:55:45] vclr INFO: Train: [ 23]/[  20/ 287] BT=7.571/10.771 Loss=11.567 5.744 0.434 1.308 4.081/12.331
[07/27 08:57:08] vclr INFO: Train: [ 23]/[  30/ 287] BT=11.746/9.968 Loss=13.134 6.415 0.426 1.695 4.598/12.279
[07/27 08:59:02] vclr INFO: Train: [ 23]/[  40/ 287] BT=11.812/10.330 Loss=12.499 5.629 0.440 1.732 4.698/12.336
[07/27 09:00:58] vclr INFO: Train: [ 23]/[  50/ 287] BT=11.685/10.569 Loss=12.293 6.333 0.273 1.261 4.426/12.369
[07/27 09:02:53] vclr INFO: Train: [ 23]/[  60/ 287] BT=10.957/10.720 Loss=12.582 6.078 0.313 1.792 4.399/12.390
[07/27 09:04:48] vclr INFO: Train: [ 23]/[  70/ 287] BT=10.974/10.833 Loss=12.588 6.233 0.245 1.585 4.524/12.337
[07/27 09:06:41] vclr INFO: Train: [ 23]/[  80/ 287] BT=11.064/10.897 Loss=12.565 5.696 0.423 1.912 4.534/12.368
[07/27 09:08:37] vclr INFO: Train: [ 23]/[  90/ 287] BT=11.030/10.966 Loss=13.470 6.625 0.403 1.701 4.740/12.353
[07/27 09:10:32] vclr INFO: Train: [ 23]/[ 100/ 287] BT=11.976/11.027 Loss=12.287 6.004 0.237 1.791 4.255/12.341
[07/27 09:12:27] vclr INFO: Train: [ 23]/[ 110/ 287] BT=11.761/11.066 Loss=12.933 5.956 0.586 1.768 4.623/12.360
[07/27 09:14:24] vclr INFO: Train: [ 23]/[ 120/ 287] BT=11.668/11.117 Loss=12.437 5.980 0.319 1.348 4.790/12.351
[07/27 09:16:18] vclr INFO: Train: [ 23]/[ 130/ 287] BT=11.103/11.138 Loss=13.313 6.131 0.491 1.855 4.837/12.352
[07/27 09:18:10] vclr INFO: Train: [ 23]/[ 140/ 287] BT=11.252/11.145 Loss=12.330 5.949 0.504 1.470 4.408/12.350
[07/27 09:20:04] vclr INFO: Train: [ 23]/[ 150/ 287] BT=11.156/11.161 Loss=12.006 5.813 0.300 1.488 4.405/12.340
[07/27 09:21:59] vclr INFO: Train: [ 23]/[ 160/ 287] BT=12.168/11.182 Loss=12.040 5.774 0.240 1.526 4.500/12.357
[07/27 09:23:55] vclr INFO: Train: [ 23]/[ 170/ 287] BT=12.974/11.208 Loss=12.627 5.841 0.508 1.414 4.865/12.342
[07/27 09:25:49] vclr INFO: Train: [ 23]/[ 180/ 287] BT=11.089/11.215 Loss=12.237 5.888 0.296 1.441 4.611/12.325
[07/27 09:27:44] vclr INFO: Train: [ 23]/[ 190/ 287] BT=11.545/11.232 Loss=11.912 5.735 0.421 1.496 4.260/12.327
[07/27 09:29:39] vclr INFO: Train: [ 23]/[ 200/ 287] BT=11.447/11.245 Loss=11.946 5.847 0.367 1.492 4.240/12.328
[07/27 09:31:34] vclr INFO: Train: [ 23]/[ 210/ 287] BT=11.653/11.257 Loss=12.117 6.042 0.233 1.451 4.391/12.330
[07/27 09:33:26] vclr INFO: Train: [ 23]/[ 220/ 287] BT=10.808/11.255 Loss=12.320 5.822 0.376 1.433 4.689/12.322
[07/27 09:35:21] vclr INFO: Train: [ 23]/[ 230/ 287] BT=11.112/11.265 Loss=11.831 5.751 0.394 1.438 4.248/12.318
[07/27 09:37:14] vclr INFO: Train: [ 23]/[ 240/ 287] BT=11.125/11.268 Loss=12.225 6.175 0.347 1.446 4.256/12.311
[07/27 09:39:10] vclr INFO: Train: [ 23]/[ 250/ 287] BT=11.130/11.278 Loss=12.194 6.344 0.221 1.248 4.381/12.307
[07/27 09:41:04] vclr INFO: Train: [ 23]/[ 260/ 287] BT=11.825/11.286 Loss=12.409 5.921 0.578 1.775 4.134/12.322
[07/27 09:43:02] vclr INFO: Train: [ 23]/[ 270/ 287] BT=11.464/11.303 Loss=12.892 6.824 0.229 1.218 4.622/12.320
[07/27 09:44:57] vclr INFO: Train: [ 23]/[ 280/ 287] BT=11.114/11.309 Loss=12.726 6.433 0.322 1.392 4.580/12.315
[07/27 09:46:13] vclr INFO: epoch 23, total time 3256.63, loss=12.321136109089602
[07/27 09:46:13] vclr INFO: ==> Saving...
[07/27 09:47:51] vclr INFO: Train: [ 24]/[   0/ 287] BT=95.466/95.466 Loss=12.020 5.721 0.568 1.478 4.254/12.020
[07/27 09:49:40] vclr INFO: Train: [ 24]/[  10/ 287] BT=10.896/18.558 Loss=11.709 5.683 0.302 1.508 4.216/12.263
[07/27 09:51:30] vclr INFO: Train: [ 24]/[  20/ 287] BT=11.009/14.967 Loss=11.710 5.734 0.238 1.431 4.307/12.310
[07/27 09:53:21] vclr INFO: Train: [ 24]/[  30/ 287] BT=11.292/13.713 Loss=13.019 6.474 0.181 1.674 4.689/12.340
[07/27 09:55:10] vclr INFO: Train: [ 24]/[  40/ 287] BT=10.857/13.040 Loss=12.579 5.889 0.333 1.534 4.823/12.378
[07/27 09:57:02] vclr INFO: Train: [ 24]/[  50/ 287] BT=10.926/12.675 Loss=12.201 6.133 0.284 1.179 4.605/12.383
[07/27 09:58:53] vclr INFO: Train: [ 24]/[  60/ 287] BT=11.385/12.413 Loss=12.215 5.930 0.279 1.425 4.581/12.359
[07/27 10:00:42] vclr INFO: Train: [ 24]/[  70/ 287] BT=10.913/12.207 Loss=12.654 6.184 0.524 1.457 4.490/12.399
[07/27 10:02:32] vclr INFO: Train: [ 24]/[  80/ 287] BT=11.582/12.047 Loss=13.173 6.521 0.279 1.399 4.974/12.388
[07/27 10:04:21] vclr INFO: Train: [ 24]/[  90/ 287] BT=10.713/11.924 Loss=12.497 6.023 0.631 1.494 4.348/12.387
[07/27 10:06:11] vclr INFO: Train: [ 24]/[ 100/ 287] BT=10.626/11.838 Loss=12.982 6.532 0.384 1.390 4.676/12.379
[07/27 10:08:02] vclr INFO: Train: [ 24]/[ 110/ 287] BT=11.212/11.765 Loss=11.355 5.449 0.387 1.537 3.982/12.366
[07/27 10:09:51] vclr INFO: Train: [ 24]/[ 120/ 287] BT=11.478/11.699 Loss=11.868 5.770 0.206 1.390 4.502/12.357
[07/27 10:11:41] vclr INFO: Train: [ 24]/[ 130/ 287] BT=10.588/11.640 Loss=12.878 6.243 0.375 1.383 4.876/12.369
[07/27 10:13:31] vclr INFO: Train: [ 24]/[ 140/ 287] BT=11.239/11.600 Loss=12.744 6.260 0.252 1.700 4.531/12.367
[07/27 10:15:22] vclr INFO: Train: [ 24]/[ 150/ 287] BT=10.507/11.562 Loss=13.271 6.426 0.347 2.079 4.419/12.395
[07/27 10:17:11] vclr INFO: Train: [ 24]/[ 160/ 287] BT=10.966/11.524 Loss=13.160 6.407 0.258 1.764 4.731/12.393
[07/27 10:19:00] vclr INFO: Train: [ 24]/[ 170/ 287] BT=10.957/11.484 Loss=12.517 6.196 0.266 1.386 4.668/12.397
[07/27 10:20:49] vclr INFO: Train: [ 24]/[ 180/ 287] BT=10.714/11.454 Loss=11.969 5.676 0.445 1.510 4.338/12.394
[07/27 10:22:38] vclr INFO: Train: [ 24]/[ 190/ 287] BT=11.130/11.425 Loss=11.716 5.420 0.429 1.438 4.429/12.400
[07/27 10:24:27] vclr INFO: Train: [ 24]/[ 200/ 287] BT=10.240/11.399 Loss=12.257 6.117 0.260 1.523 4.358/12.393
[07/27 10:26:14] vclr INFO: Train: [ 24]/[ 210/ 287] BT=10.928/11.367 Loss=12.380 6.060 0.214 1.324 4.782/12.379
[07/27 10:28:01] vclr INFO: Train: [ 24]/[ 220/ 287] BT=10.834/11.337 Loss=12.702 6.065 0.426 1.578 4.634/12.366
[07/27 10:29:47] vclr INFO: Train: [ 24]/[ 230/ 287] BT=10.683/11.302 Loss=13.235 6.905 0.171 1.361 4.798/12.358
[07/27 10:31:34] vclr INFO: Train: [ 24]/[ 240/ 287] BT=10.735/11.278 Loss=13.101 6.569 0.221 1.363 4.947/12.351
[07/27 10:33:21] vclr INFO: Train: [ 24]/[ 250/ 287] BT=10.907/11.256 Loss=11.462 5.579 0.338 1.290 4.255/12.353
[07/27 10:35:06] vclr INFO: Train: [ 24]/[ 260/ 287] BT=10.008/11.228 Loss=12.540 6.121 0.295 1.999 4.125/12.366
[07/27 10:36:51] vclr INFO: Train: [ 24]/[ 270/ 287] BT=10.175/11.199 Loss=11.959 5.731 0.558 1.481 4.189/12.375
[07/27 10:38:36] vclr INFO: Train: [ 24]/[ 280/ 287] BT=9.997/11.176 Loss=11.797 5.938 0.387 1.286 4.186/12.379
[07/27 10:39:46] vclr INFO: epoch 24, total time 3212.16, loss=12.383129319247468
[07/27 10:39:46] vclr INFO: ==> Saving...
[07/27 10:41:22] vclr INFO: Train: [ 25]/[   0/ 287] BT=92.876/92.876 Loss=12.370 5.646 0.558 1.859 4.308/12.370
[07/27 10:43:05] vclr INFO: Train: [ 25]/[  10/ 287] BT=10.226/17.809 Loss=11.786 5.647 0.341 1.306 4.493/12.778
[07/27 10:44:48] vclr INFO: Train: [ 25]/[  20/ 287] BT=10.166/14.252 Loss=12.560 5.974 0.383 1.843 4.359/12.632
[07/27 10:46:30] vclr INFO: Train: [ 25]/[  30/ 287] BT=10.258/12.936 Loss=12.592 6.364 0.298 1.403 4.528/12.773
[07/27 10:48:14] vclr INFO: Train: [ 25]/[  40/ 287] BT=10.244/12.320 Loss=11.728 5.578 0.346 1.436 4.368/12.750
[07/27 10:49:59] vclr INFO: Train: [ 25]/[  50/ 287] BT=10.206/11.953 Loss=11.612 5.613 0.519 1.256 4.225/12.699
[07/27 10:51:41] vclr INFO: Train: [ 25]/[  60/ 287] BT=9.805/11.671 Loss=12.323 5.835 0.431 1.656 4.402/12.686
[07/27 10:53:23] vclr INFO: Train: [ 25]/[  70/ 287] BT=10.255/11.459 Loss=12.404 5.786 0.517 1.584 4.517/12.609
[07/27 10:55:06] vclr INFO: Train: [ 25]/[  80/ 287] BT=9.832/11.316 Loss=12.013 5.499 0.406 1.814 4.294/12.593
[07/27 10:56:46] vclr INFO: Train: [ 25]/[  90/ 287] BT=9.705/11.180 Loss=13.802 6.650 0.266 1.975 4.911/12.591
[07/27 10:58:30] vclr INFO: Train: [ 25]/[ 100/ 287] BT=10.327/11.098 Loss=12.166 5.794 0.365 1.518 4.489/12.559
[07/27 11:00:13] vclr INFO: Train: [ 25]/[ 110/ 287] BT=10.099/11.024 Loss=12.032 5.913 0.468 1.319 4.332/12.539
[07/27 11:01:56] vclr INFO: Train: [ 25]/[ 120/ 287] BT=10.199/10.965 Loss=12.675 6.611 0.168 1.399 4.497/12.520
[07/27 11:03:39] vclr INFO: Train: [ 25]/[ 130/ 287] BT=10.309/10.913 Loss=12.239 5.968 0.378 1.414 4.480/12.500
[07/27 11:05:21] vclr INFO: Train: [ 25]/[ 140/ 287] BT=10.583/10.863 Loss=12.300 5.991 0.296 1.274 4.739/12.484
[07/27 11:07:03] vclr INFO: Train: [ 25]/[ 150/ 287] BT=9.813/10.820 Loss=12.556 5.943 0.588 1.534 4.491/12.479
[07/27 11:08:45] vclr INFO: Train: [ 25]/[ 160/ 287] BT=10.273/10.785 Loss=12.550 6.005 0.218 1.651 4.675/12.471
[07/27 11:10:26] vclr INFO: Train: [ 25]/[ 170/ 287] BT=10.421/10.745 Loss=12.108 5.926 0.384 1.616 4.182/12.461
[07/27 11:12:12] vclr INFO: Train: [ 25]/[ 180/ 287] BT=10.341/10.732 Loss=11.188 5.269 0.468 1.540 3.911/12.443
[07/27 11:13:54] vclr INFO: Train: [ 25]/[ 190/ 287] BT=10.556/10.707 Loss=12.458 5.807 0.329 1.593 4.730/12.425
[07/27 11:15:38] vclr INFO: Train: [ 25]/[ 200/ 287] BT=10.769/10.690 Loss=12.826 6.162 0.407 1.661 4.595/12.426
[07/27 11:17:22] vclr INFO: Train: [ 25]/[ 210/ 287] BT=10.424/10.677 Loss=12.570 6.249 0.353 1.316 4.652/12.424
[07/27 11:19:07] vclr INFO: Train: [ 25]/[ 220/ 287] BT=11.069/10.669 Loss=12.271 6.092 0.277 1.251 4.652/12.429
[07/27 11:20:49] vclr INFO: Train: [ 25]/[ 230/ 287] BT=10.013/10.648 Loss=12.088 5.646 0.414 1.956 4.073/12.425
[07/27 11:22:31] vclr INFO: Train: [ 25]/[ 240/ 287] BT=9.958/10.632 Loss=11.977 5.826 0.249 1.421 4.481/12.419
[07/27 11:24:14] vclr INFO: Train: [ 25]/[ 250/ 287] BT=10.129/10.618 Loss=11.993 6.007 0.365 1.439 4.182/12.404
[07/27 11:25:56] vclr INFO: Train: [ 25]/[ 260/ 287] BT=10.172/10.602 Loss=12.388 6.291 0.383 1.391 4.324/12.390
[07/27 11:27:40] vclr INFO: Train: [ 25]/[ 270/ 287] BT=10.860/10.595 Loss=12.120 5.669 0.345 1.680 4.425/12.397
[07/27 11:29:25] vclr INFO: Train: [ 25]/[ 280/ 287] BT=10.122/10.589 Loss=12.159 5.693 0.506 1.496 4.465/12.390
[07/27 11:30:30] vclr INFO: epoch 25, total time 3044.55, loss=12.382720129830497
[07/27 11:30:30] vclr INFO: ==> Saving...
[07/27 11:32:15] vclr INFO: Train: [ 26]/[   0/ 287] BT=101.982/101.982 Loss=12.783 5.995 0.260 1.558 4.971/12.783
[07/27 11:33:58] vclr INFO: Train: [ 26]/[  10/ 287] BT=10.431/18.615 Loss=12.057 5.996 0.356 1.289 4.416/12.368
[07/27 11:35:41] vclr INFO: Train: [ 26]/[  20/ 287] BT=10.478/14.668 Loss=12.108 5.841 0.276 1.608 4.383/12.240
[07/27 11:37:25] vclr INFO: Train: [ 26]/[  30/ 287] BT=10.316/13.308 Loss=12.111 5.862 0.297 1.701 4.251/12.161
[07/27 11:39:09] vclr INFO: Train: [ 26]/[  40/ 287] BT=10.405/12.587 Loss=12.122 6.079 0.349 1.295 4.399/12.103
[07/27 11:40:53] vclr INFO: Train: [ 26]/[  50/ 287] BT=9.725/12.154 Loss=11.725 5.791 0.383 1.316 4.236/12.098
[07/27 11:42:37] vclr INFO: Train: [ 26]/[  60/ 287] BT=10.092/11.868 Loss=11.821 5.704 0.379 1.459 4.279/12.151
[07/27 11:44:21] vclr INFO: Train: [ 26]/[  70/ 287] BT=10.249/11.662 Loss=12.044 5.510 0.427 1.515 4.591/12.169
[07/27 11:46:05] vclr INFO: Train: [ 26]/[  80/ 287] BT=10.631/11.507 Loss=13.150 6.240 0.695 1.394 4.822/12.162
[07/27 11:47:52] vclr INFO: Train: [ 26]/[  90/ 287] BT=10.435/11.413 Loss=13.260 6.274 0.453 1.946 4.587/12.174
[07/27 11:49:37] vclr INFO: Train: [ 26]/[ 100/ 287] BT=10.043/11.322 Loss=11.454 5.045 0.513 1.748 4.148/12.134
[07/27 11:51:20] vclr INFO: Train: [ 26]/[ 110/ 287] BT=10.846/11.237 Loss=12.093 5.773 0.320 1.559 4.441/12.156
[07/27 11:53:05] vclr INFO: Train: [ 26]/[ 120/ 287] BT=10.705/11.172 Loss=12.140 5.665 0.418 1.357 4.700/12.173
[07/27 11:54:47] vclr INFO: Train: [ 26]/[ 130/ 287] BT=9.833/11.099 Loss=12.364 6.029 0.324 1.248 4.764/12.171
[07/27 11:56:29] vclr INFO: Train: [ 26]/[ 140/ 287] BT=9.858/11.038 Loss=11.964 5.744 0.383 1.564 4.274/12.179
[07/27 11:58:13] vclr INFO: Train: [ 26]/[ 150/ 287] BT=10.417/10.993 Loss=13.464 6.566 0.253 1.602 5.044/12.196
[07/27 11:59:57] vclr INFO: Train: [ 26]/[ 160/ 287] BT=10.780/10.957 Loss=12.351 5.803 0.446 1.591 4.511/12.205
[07/27 12:01:40] vclr INFO: Train: [ 26]/[ 170/ 287] BT=9.719/10.920 Loss=12.296 5.873 0.294 1.610 4.520/12.210
[07/27 12:03:24] vclr INFO: Train: [ 26]/[ 180/ 287] BT=11.574/10.889 Loss=12.247 5.867 0.444 1.585 4.350/12.207
[07/27 12:05:08] vclr INFO: Train: [ 26]/[ 190/ 287] BT=10.352/10.866 Loss=12.902 6.358 0.508 1.550 4.485/12.209
[07/27 12:06:52] vclr INFO: Train: [ 26]/[ 200/ 287] BT=10.523/10.839 Loss=13.339 6.726 0.377 1.780 4.457/12.224
[07/27 12:08:36] vclr INFO: Train: [ 26]/[ 210/ 287] BT=10.350/10.819 Loss=11.845 5.482 0.355 1.567 4.441/12.235
[07/27 12:10:18] vclr INFO: Train: [ 26]/[ 220/ 287] BT=10.136/10.794 Loss=12.551 6.146 0.288 1.546 4.571/12.240
[07/27 12:12:03] vclr INFO: Train: [ 26]/[ 230/ 287] BT=10.596/10.779 Loss=12.512 5.633 0.510 1.683 4.685/12.242
[07/27 12:13:46] vclr INFO: Train: [ 26]/[ 240/ 287] BT=9.663/10.760 Loss=12.322 6.040 0.308 1.648 4.325/12.257
[07/27 12:15:31] vclr INFO: Train: [ 26]/[ 250/ 287] BT=10.294/10.748 Loss=12.620 5.730 0.409 2.071 4.411/12.272
[07/27 12:17:14] vclr INFO: Train: [ 26]/[ 260/ 287] BT=10.396/10.734 Loss=13.290 6.383 0.301 1.812 4.792/12.281
[07/27 12:18:57] vclr INFO: Train: [ 26]/[ 270/ 287] BT=10.162/10.717 Loss=12.562 6.265 0.338 1.298 4.661/12.278
[07/27 12:20:40] vclr INFO: Train: [ 26]/[ 280/ 287] BT=10.246/10.703 Loss=11.542 5.536 0.518 1.351 4.137/12.295
[07/27 12:21:49] vclr INFO: epoch 26, total time 3078.82, loss=12.299631341409185
[07/27 12:21:49] vclr INFO: ==> Saving...
[07/27 12:23:19] vclr INFO: Train: [ 27]/[   0/ 287] BT=86.898/86.898 Loss=12.466 5.730 0.516 1.683 4.537/12.466
[07/27 12:25:07] vclr INFO: Train: [ 27]/[  10/ 287] BT=11.505/17.705 Loss=12.442 6.097 0.292 1.430 4.623/12.403
[07/27 12:26:55] vclr INFO: Train: [ 27]/[  20/ 287] BT=10.275/14.403 Loss=11.760 5.381 0.411 1.816 4.152/12.329
[07/27 12:28:46] vclr INFO: Train: [ 27]/[  30/ 287] BT=10.949/13.342 Loss=13.597 6.095 0.407 2.513 4.583/12.416
[07/27 12:30:31] vclr INFO: Train: [ 27]/[  40/ 287] BT=10.859/12.656 Loss=12.672 6.097 0.422 1.630 4.522/12.425
[07/27 12:32:26] vclr INFO: Train: [ 27]/[  50/ 287] BT=10.853/12.427 Loss=11.687 5.531 0.422 1.463 4.270/12.390
[07/27 12:34:15] vclr INFO: Train: [ 27]/[  60/ 287] BT=10.280/12.182 Loss=12.509 6.054 0.303 1.370 4.783/12.398
[07/27 12:36:05] vclr INFO: Train: [ 27]/[  70/ 287] BT=11.300/12.009 Loss=12.867 6.123 0.259 1.677 4.808/12.371
[07/27 12:37:55] vclr INFO: Train: [ 27]/[  80/ 287] BT=10.165/11.885 Loss=11.371 5.304 0.507 1.355 4.205/12.331
[07/27 12:39:45] vclr INFO: Train: [ 27]/[  90/ 287] BT=10.389/11.785 Loss=13.429 6.855 0.295 1.577 4.703/12.333
[07/27 12:41:34] vclr INFO: Train: [ 27]/[ 100/ 287] BT=11.264/11.701 Loss=12.208 5.747 0.514 1.776 4.171/12.338
[07/27 12:43:25] vclr INFO: Train: [ 27]/[ 110/ 287] BT=11.083/11.644 Loss=12.546 6.131 0.524 1.427 4.464/12.335
[07/27 12:45:13] vclr INFO: Train: [ 27]/[ 120/ 287] BT=10.467/11.575 Loss=12.145 6.167 0.294 1.287 4.397/12.308
[07/27 12:47:00] vclr INFO: Train: [ 27]/[ 130/ 287] BT=10.975/11.512 Loss=11.609 5.219 0.520 1.426 4.444/12.301
[07/27 12:48:48] vclr INFO: Train: [ 27]/[ 140/ 287] BT=10.708/11.457 Loss=11.961 5.926 0.320 1.399 4.316/12.291
[07/27 12:50:35] vclr INFO: Train: [ 27]/[ 150/ 287] BT=10.484/11.411 Loss=11.415 5.394 0.522 1.532 3.966/12.277
[07/27 12:52:24] vclr INFO: Train: [ 27]/[ 160/ 287] BT=10.518/11.376 Loss=12.101 5.924 0.505 1.444 4.228/12.284
[07/27 12:54:10] vclr INFO: Train: [ 27]/[ 170/ 287] BT=10.701/11.334 Loss=12.051 5.932 0.315 1.448 4.355/12.271
[07/27 12:55:59] vclr INFO: Train: [ 27]/[ 180/ 287] BT=10.774/11.310 Loss=12.417 6.133 0.205 1.752 4.327/12.265
[07/27 12:57:45] vclr INFO: Train: [ 27]/[ 190/ 287] BT=10.366/11.269 Loss=11.659 5.523 0.341 1.507 4.288/12.261
[07/27 12:59:34] vclr INFO: Train: [ 27]/[ 200/ 287] BT=11.058/11.253 Loss=11.504 5.654 0.271 1.494 4.085/12.256
[07/27 13:01:24] vclr INFO: Train: [ 27]/[ 210/ 287] BT=10.199/11.239 Loss=12.862 6.165 0.460 1.588 4.648/12.251
[07/27 13:03:12] vclr INFO: Train: [ 27]/[ 220/ 287] BT=10.563/11.223 Loss=11.889 5.515 0.537 1.598 4.238/12.242
[07/27 13:05:01] vclr INFO: Train: [ 27]/[ 230/ 287] BT=9.930/11.206 Loss=12.985 6.179 0.435 1.672 4.698/12.250
[07/27 13:06:48] vclr INFO: Train: [ 27]/[ 240/ 287] BT=10.623/11.185 Loss=11.928 5.341 0.663 1.575 4.349/12.242
[07/27 13:08:36] vclr INFO: Train: [ 27]/[ 250/ 287] BT=10.420/11.172 Loss=11.810 5.749 0.373 1.381 4.308/12.245
[07/27 13:10:22] vclr INFO: Train: [ 27]/[ 260/ 287] BT=10.542/11.150 Loss=11.858 5.578 0.505 1.587 4.187/12.250
[07/27 13:12:09] vclr INFO: Train: [ 27]/[ 270/ 287] BT=10.280/11.132 Loss=12.358 5.955 0.355 1.839 4.209/12.258
[07/27 13:13:59] vclr INFO: Train: [ 27]/[ 280/ 287] BT=10.923/11.128 Loss=12.461 5.838 0.460 1.684 4.479/12.261
[07/27 13:15:15] vclr INFO: epoch 27, total time 3206.27, loss=12.261247285982458
[07/27 13:15:15] vclr INFO: ==> Saving...
[07/27 13:16:57] vclr INFO: Train: [ 28]/[   0/ 287] BT=98.789/98.789 Loss=13.363 6.742 0.334 1.414 4.873/13.363
[07/27 13:18:48] vclr INFO: Train: [ 28]/[  10/ 287] BT=12.022/18.996 Loss=12.388 6.195 0.484 1.587 4.122/12.498
[07/27 13:20:35] vclr INFO: Train: [ 28]/[  20/ 287] BT=10.895/15.088 Loss=12.099 5.908 0.429 1.359 4.403/12.561
[07/27 13:22:23] vclr INFO: Train: [ 28]/[  30/ 287] BT=10.526/13.688 Loss=13.423 6.577 0.352 1.534 4.960/12.445
[07/27 13:24:12] vclr INFO: Train: [ 28]/[  40/ 287] BT=11.562/13.014 Loss=11.079 5.020 0.568 1.382 4.109/12.371
[07/27 13:26:00] vclr INFO: Train: [ 28]/[  50/ 287] BT=11.184/12.572 Loss=12.479 5.906 0.362 1.627 4.584/12.402
[07/27 13:27:48] vclr INFO: Train: [ 28]/[  60/ 287] BT=10.784/12.289 Loss=12.535 6.074 0.437 1.694 4.330/12.364
[07/27 13:29:34] vclr INFO: Train: [ 28]/[  70/ 287] BT=10.921/12.055 Loss=11.900 5.473 0.512 1.378 4.537/12.305
[07/27 13:31:21] vclr INFO: Train: [ 28]/[  80/ 287] BT=10.890/11.878 Loss=12.317 5.776 0.535 1.581 4.425/12.268
[07/27 13:33:09] vclr INFO: Train: [ 28]/[  90/ 287] BT=11.549/11.762 Loss=12.226 5.999 0.432 1.350 4.446/12.254
[07/27 13:34:57] vclr INFO: Train: [ 28]/[ 100/ 287] BT=11.642/11.671 Loss=12.190 6.007 0.276 1.490 4.417/12.245
[07/27 13:36:46] vclr INFO: Train: [ 28]/[ 110/ 287] BT=10.847/11.599 Loss=12.512 6.077 0.376 1.543 4.516/12.247
[07/27 13:38:33] vclr INFO: Train: [ 28]/[ 120/ 287] BT=10.221/11.520 Loss=12.345 5.668 0.374 1.900 4.402/12.220
[07/27 13:40:18] vclr INFO: Train: [ 28]/[ 130/ 287] BT=10.504/11.447 Loss=12.314 5.947 0.331 1.733 4.304/12.242
[07/27 13:42:06] vclr INFO: Train: [ 28]/[ 140/ 287] BT=11.119/11.398 Loss=13.186 6.578 0.197 1.482 4.929/12.240
[07/27 13:43:52] vclr INFO: Train: [ 28]/[ 150/ 287] BT=10.391/11.345 Loss=13.040 6.319 0.259 1.575 4.887/12.256
[07/27 13:45:38] vclr INFO: Train: [ 28]/[ 160/ 287] BT=10.518/11.303 Loss=12.913 6.477 0.244 1.352 4.839/12.258
[07/27 13:47:27] vclr INFO: Train: [ 28]/[ 170/ 287] BT=10.479/11.276 Loss=11.682 5.719 0.365 1.353 4.245/12.255
[07/27 13:49:14] vclr INFO: Train: [ 28]/[ 180/ 287] BT=10.861/11.248 Loss=12.526 5.885 0.404 1.721 4.517/12.259
[07/27 13:51:03] vclr INFO: Train: [ 28]/[ 190/ 287] BT=10.838/11.230 Loss=11.994 5.302 0.567 1.515 4.609/12.268
[07/27 13:52:52] vclr INFO: Train: [ 28]/[ 200/ 287] BT=10.658/11.209 Loss=11.667 5.258 0.584 1.647 4.178/12.265
[07/27 13:54:41] vclr INFO: Train: [ 28]/[ 210/ 287] BT=10.915/11.194 Loss=12.431 6.047 0.370 1.565 4.449/12.265
[07/27 13:56:26] vclr INFO: Train: [ 28]/[ 220/ 287] BT=10.495/11.167 Loss=12.360 6.204 0.279 1.454 4.424/12.263
[07/27 13:58:15] vclr INFO: Train: [ 28]/[ 230/ 287] BT=10.733/11.151 Loss=12.660 6.260 0.320 1.416 4.665/12.267
[07/27 14:00:04] vclr INFO: Train: [ 28]/[ 240/ 287] BT=10.685/11.144 Loss=11.475 5.501 0.369 1.644 3.962/12.259
[07/27 14:01:51] vclr INFO: Train: [ 28]/[ 250/ 287] BT=10.302/11.126 Loss=12.303 6.026 0.412 1.201 4.665/12.264
[07/27 14:03:38] vclr INFO: Train: [ 28]/[ 260/ 287] BT=10.500/11.110 Loss=13.096 6.033 0.223 2.471 4.368/12.286
[07/27 14:05:25] vclr INFO: Train: [ 28]/[ 270/ 287] BT=10.834/11.095 Loss=11.754 5.494 0.425 1.523 4.312/12.288
[07/27 14:07:13] vclr INFO: Train: [ 28]/[ 280/ 287] BT=11.042/11.082 Loss=11.981 5.873 0.353 1.229 4.527/12.295
[07/27 14:08:23] vclr INFO: epoch 28, total time 3187.75, loss=12.285496914428286
[07/27 14:08:23] vclr INFO: ==> Saving...
[07/27 14:10:12] vclr INFO: Train: [ 29]/[   0/ 287] BT=105.412/105.412 Loss=12.385 5.941 0.550 1.527 4.367/12.385
[07/27 14:11:51] vclr INFO: Train: [ 29]/[  10/ 287] BT=9.747/18.524 Loss=11.824 5.563 0.476 1.532 4.252/12.547
[07/27 14:13:29] vclr INFO: Train: [ 29]/[  20/ 287] BT=9.750/14.410 Loss=12.991 6.338 0.473 1.430 4.750/12.335
[07/27 14:15:10] vclr INFO: Train: [ 29]/[  30/ 287] BT=9.789/12.999 Loss=13.099 6.303 0.696 1.551 4.548/12.363
[07/27 14:16:47] vclr INFO: Train: [ 29]/[  40/ 287] BT=9.513/12.212 Loss=12.118 5.892 0.403 1.546 4.278/12.283
[07/27 14:18:29] vclr INFO: Train: [ 29]/[  50/ 287] BT=10.406/11.804 Loss=12.223 5.734 0.358 1.778 4.352/12.235
[07/27 14:20:09] vclr INFO: Train: [ 29]/[  60/ 287] BT=10.441/11.515 Loss=12.240 5.974 0.466 1.365 4.434/12.281
[07/27 14:21:46] vclr INFO: Train: [ 29]/[  70/ 287] BT=9.796/11.259 Loss=12.532 6.453 0.207 1.364 4.509/12.269
[07/27 14:23:28] vclr INFO: Train: [ 29]/[  80/ 287] BT=10.132/11.130 Loss=11.507 5.617 0.621 1.101 4.168/12.258
[07/27 14:25:08] vclr INFO: Train: [ 29]/[  90/ 287] BT=9.904/11.004 Loss=11.902 5.513 0.424 1.579 4.387/12.273
[07/27 14:26:49] vclr INFO: Train: [ 29]/[ 100/ 287] BT=10.169/10.910 Loss=11.904 5.745 0.450 1.430 4.280/12.280
[07/27 14:28:28] vclr INFO: Train: [ 29]/[ 110/ 287] BT=10.194/10.821 Loss=12.517 6.611 0.119 1.199 4.588/12.267
[07/27 14:30:06] vclr INFO: Train: [ 29]/[ 120/ 287] BT=9.239/10.736 Loss=12.529 6.189 0.415 1.471 4.454/12.271
[07/27 14:31:46] vclr INFO: Train: [ 29]/[ 130/ 287] BT=9.744/10.678 Loss=11.570 5.621 0.321 1.539 4.089/12.257
[07/27 14:33:25] vclr INFO: Train: [ 29]/[ 140/ 287] BT=11.388/10.625 Loss=12.126 6.049 0.345 1.430 4.302/12.273
[07/27 14:35:03] vclr INFO: Train: [ 29]/[ 150/ 287] BT=9.655/10.571 Loss=12.497 5.828 0.550 1.626 4.492/12.265
[07/27 14:36:42] vclr INFO: Train: [ 29]/[ 160/ 287] BT=9.429/10.527 Loss=11.617 5.597 0.445 1.528 4.046/12.267
[07/27 14:38:23] vclr INFO: Train: [ 29]/[ 170/ 287] BT=9.879/10.505 Loss=12.194 5.460 0.489 1.845 4.399/12.262
[07/27 14:40:02] vclr INFO: Train: [ 29]/[ 180/ 287] BT=9.664/10.472 Loss=11.868 5.313 0.687 1.297 4.570/12.253
[07/27 14:41:43] vclr INFO: Train: [ 29]/[ 190/ 287] BT=9.804/10.452 Loss=12.538 5.877 0.333 2.022 4.306/12.258
[07/27 14:43:26] vclr INFO: Train: [ 29]/[ 200/ 287] BT=10.830/10.442 Loss=12.169 5.755 0.409 1.633 4.372/12.259
[07/27 14:45:04] vclr INFO: Train: [ 29]/[ 210/ 287] BT=9.195/10.415 Loss=11.555 5.712 0.330 1.312 4.201/12.247
[07/27 14:46:45] vclr INFO: Train: [ 29]/[ 220/ 287] BT=9.629/10.397 Loss=12.560 6.209 0.226 1.403 4.723/12.247
[07/27 14:48:24] vclr INFO: Train: [ 29]/[ 230/ 287] BT=10.139/10.378 Loss=12.750 6.301 0.212 1.395 4.842/12.256
[07/27 14:50:02] vclr INFO: Train: [ 29]/[ 240/ 287] BT=9.418/10.352 Loss=12.503 6.076 0.617 1.321 4.489/12.264
[07/27 14:51:41] vclr INFO: Train: [ 29]/[ 250/ 287] BT=9.918/10.335 Loss=12.022 5.839 0.554 1.412 4.217/12.255
[07/27 14:53:21] vclr INFO: Train: [ 29]/[ 260/ 287] BT=9.476/10.323 Loss=11.972 5.820 0.241 1.735 4.176/12.247
[07/27 14:54:58] vclr INFO: Train: [ 29]/[ 270/ 287] BT=9.693/10.300 Loss=11.378 5.126 0.545 1.647 4.060/12.243
[07/27 14:56:37] vclr INFO: Train: [ 29]/[ 280/ 287] BT=9.292/10.285 Loss=12.082 5.846 0.509 1.520 4.206/12.235
[07/27 14:57:45] vclr INFO: epoch 29, total time 2962.39, loss=12.228206667750555
[07/27 14:57:45] vclr INFO: ==> Saving...
[07/27 14:59:21] vclr INFO: Train: [ 30]/[   0/ 287] BT=93.695/93.695 Loss=12.006 5.826 0.583 1.278 4.320/12.006
[07/27 15:01:07] vclr INFO: Train: [ 30]/[  10/ 287] BT=11.036/18.148 Loss=12.309 6.313 0.269 1.335 4.393/11.994
[07/27 15:02:55] vclr INFO: Train: [ 30]/[  20/ 287] BT=10.168/14.642 Loss=12.538 5.857 0.357 1.567 4.757/12.103
[07/27 15:04:40] vclr INFO: Train: [ 30]/[  30/ 287] BT=10.595/13.306 Loss=12.960 6.436 0.272 1.556 4.696/12.222
[07/27 15:06:25] vclr INFO: Train: [ 30]/[  40/ 287] BT=10.541/12.624 Loss=11.663 5.381 0.517 1.676 4.090/12.185
[07/27 15:08:10] vclr INFO: Train: [ 30]/[  50/ 287] BT=9.847/12.205 Loss=11.784 5.444 0.344 1.711 4.286/12.164
[07/27 15:09:57] vclr INFO: Train: [ 30]/[  60/ 287] BT=10.276/11.952 Loss=12.290 5.938 0.384 1.630 4.338/12.198
[07/27 15:11:44] vclr INFO: Train: [ 30]/[  70/ 287] BT=10.784/11.773 Loss=11.725 5.517 0.581 1.636 3.990/12.199
[07/27 15:13:29] vclr INFO: Train: [ 30]/[  80/ 287] BT=10.777/11.616 Loss=12.346 6.191 0.275 1.466 4.414/12.224
[07/27 15:15:15] vclr INFO: Train: [ 30]/[  90/ 287] BT=10.435/11.509 Loss=11.915 5.435 0.641 1.802 4.037/12.218
[07/27 15:17:01] vclr INFO: Train: [ 30]/[ 100/ 287] BT=11.004/11.414 Loss=14.326 6.817 0.427 1.873 5.209/12.248
[07/27 15:18:47] vclr INFO: Train: [ 30]/[ 110/ 287] BT=11.257/11.344 Loss=12.222 5.722 0.375 1.757 4.367/12.245
[07/27 15:20:31] vclr INFO: Train: [ 30]/[ 120/ 287] BT=10.862/11.270 Loss=11.652 5.400 0.390 1.345 4.519/12.236
[07/27 15:22:16] vclr INFO: Train: [ 30]/[ 130/ 287] BT=10.052/11.204 Loss=11.727 5.636 0.669 1.369 4.053/12.244
[07/27 15:23:57] vclr INFO: Train: [ 30]/[ 140/ 287] BT=9.524/11.129 Loss=11.298 5.656 0.436 1.206 4.001/12.238
[07/27 15:25:41] vclr INFO: Train: [ 30]/[ 150/ 287] BT=10.375/11.081 Loss=12.162 6.191 0.303 1.305 4.364/12.215
[07/27 15:27:27] vclr INFO: Train: [ 30]/[ 160/ 287] BT=10.743/11.054 Loss=12.411 6.146 0.396 1.249 4.620/12.219
[07/27 15:29:14] vclr INFO: Train: [ 30]/[ 170/ 287] BT=10.735/11.031 Loss=12.089 6.000 0.408 1.268 4.413/12.216
[07/27 15:31:00] vclr INFO: Train: [ 30]/[ 180/ 287] BT=11.139/11.005 Loss=13.130 6.348 0.268 1.561 4.952/12.217
[07/27 15:32:46] vclr INFO: Train: [ 30]/[ 190/ 287] BT=11.213/10.984 Loss=11.174 5.269 0.487 1.347 4.071/12.193
[07/27 15:34:38] vclr INFO: Train: [ 30]/[ 200/ 287] BT=11.351/10.997 Loss=11.791 6.146 0.189 1.126 4.330/12.190
[07/27 15:36:28] vclr INFO: Train: [ 30]/[ 210/ 287] BT=10.157/10.995 Loss=12.130 5.968 0.443 1.469 4.249/12.191
[07/27 15:38:15] vclr INFO: Train: [ 30]/[ 220/ 287] BT=10.719/10.981 Loss=13.311 6.780 0.273 1.592 4.666/12.199
[07/27 15:40:02] vclr INFO: Train: [ 30]/[ 230/ 287] BT=10.597/10.969 Loss=12.685 6.403 0.370 1.695 4.218/12.209
[07/27 15:41:48] vclr INFO: Train: [ 30]/[ 240/ 287] BT=10.195/10.954 Loss=11.942 6.103 0.356 1.298 4.185/12.201
[07/27 15:43:35] vclr INFO: Train: [ 30]/[ 250/ 287] BT=11.044/10.945 Loss=12.341 6.226 0.394 1.418 4.303/12.215
[07/27 15:45:23] vclr INFO: Train: [ 30]/[ 260/ 287] BT=10.700/10.941 Loss=11.445 5.542 0.491 1.342 4.070/12.213
[07/27 15:47:10] vclr INFO: Train: [ 30]/[ 270/ 287] BT=10.265/10.930 Loss=11.823 5.939 0.325 1.295 4.265/12.213
[07/27 15:48:57] vclr INFO: Train: [ 30]/[ 280/ 287] BT=11.230/10.922 Loss=11.248 5.382 0.594 1.435 3.836/12.217
[07/27 15:50:11] vclr INFO: epoch 30, total time 3145.34, loss=12.217245710020697
[07/27 15:50:11] vclr INFO: ==> Saving...
[07/27 15:52:07] vclr INFO: Train: [ 31]/[   0/ 287] BT=109.011/109.011 Loss=11.861 5.168 0.515 2.160 4.018/11.861
[07/27 15:53:54] vclr INFO: Train: [ 31]/[  10/ 287] BT=11.078/19.590 Loss=12.388 6.293 0.260 1.449 4.386/12.181
[07/27 15:55:38] vclr INFO: Train: [ 31]/[  20/ 287] BT=10.159/15.244 Loss=12.803 6.252 0.326 1.463 4.763/12.318
[07/27 15:57:23] vclr INFO: Train: [ 31]/[  30/ 287] BT=10.722/13.696 Loss=13.076 6.043 0.402 2.306 4.324/12.518
[07/27 15:59:08] vclr INFO: Train: [ 31]/[  40/ 287] BT=9.968/12.933 Loss=12.129 6.123 0.266 1.459 4.281/12.493
[07/27 16:00:52] vclr INFO: Train: [ 31]/[  50/ 287] BT=10.409/12.436 Loss=12.185 6.298 0.247 1.313 4.327/12.511
[07/27 16:02:35] vclr INFO: Train: [ 31]/[  60/ 287] BT=10.353/12.082 Loss=11.629 5.586 0.389 1.351 4.303/12.422
[07/27 16:04:18] vclr INFO: Train: [ 31]/[  70/ 287] BT=9.947/11.827 Loss=11.990 5.640 0.308 1.528 4.515/12.360
[07/27 16:05:59] vclr INFO: Train: [ 31]/[  80/ 287] BT=10.414/11.621 Loss=11.955 5.630 0.552 1.456 4.317/12.324
[07/27 16:07:44] vclr INFO: Train: [ 31]/[  90/ 287] BT=10.728/11.491 Loss=11.906 5.535 0.455 1.455 4.462/12.296
[07/27 16:09:26] vclr INFO: Train: [ 31]/[ 100/ 287] BT=10.093/11.366 Loss=11.953 5.303 0.580 1.628 4.441/12.302
[07/27 16:11:08] vclr INFO: Train: [ 31]/[ 110/ 287] BT=10.399/11.260 Loss=11.158 5.595 0.343 1.213 4.007/12.268
[07/27 16:12:50] vclr INFO: Train: [ 31]/[ 120/ 287] BT=10.915/11.176 Loss=12.097 6.107 0.376 1.403 4.212/12.245
[07/27 16:14:30] vclr INFO: Train: [ 31]/[ 130/ 287] BT=9.279/11.086 Loss=11.414 5.358 0.447 1.357 4.252/12.235
[07/27 16:16:13] vclr INFO: Train: [ 31]/[ 140/ 287] BT=10.312/11.029 Loss=11.659 5.783 0.315 1.229 4.332/12.223
[07/27 16:17:54] vclr INFO: Train: [ 31]/[ 150/ 287] BT=9.705/10.966 Loss=12.874 6.263 0.575 1.681 4.355/12.231
[07/27 16:19:36] vclr INFO: Train: [ 31]/[ 160/ 287] BT=10.006/10.917 Loss=12.729 6.090 0.338 1.558 4.744/12.224
[07/27 16:21:19] vclr INFO: Train: [ 31]/[ 170/ 287] BT=10.367/10.885 Loss=11.981 5.728 0.359 1.429 4.466/12.219
[07/27 16:23:01] vclr INFO: Train: [ 31]/[ 180/ 287] BT=10.155/10.847 Loss=11.883 5.814 0.392 1.528 4.150/12.221
[07/27 16:24:45] vclr INFO: Train: [ 31]/[ 190/ 287] BT=10.063/10.821 Loss=11.974 5.648 0.463 1.438 4.425/12.209
[07/27 16:26:26] vclr INFO: Train: [ 31]/[ 200/ 287] BT=10.367/10.786 Loss=12.857 6.325 0.183 2.129 4.219/12.201
[07/27 16:28:11] vclr INFO: Train: [ 31]/[ 210/ 287] BT=9.761/10.772 Loss=11.976 5.923 0.369 1.376 4.309/12.190
[07/27 16:29:55] vclr INFO: Train: [ 31]/[ 220/ 287] BT=10.472/10.756 Loss=12.179 5.766 0.233 1.377 4.803/12.182
[07/27 16:31:38] vclr INFO: Train: [ 31]/[ 230/ 287] BT=9.467/10.735 Loss=12.374 5.898 0.517 1.673 4.286/12.177
[07/27 16:33:18] vclr INFO: Train: [ 31]/[ 240/ 287] BT=10.019/10.703 Loss=12.329 6.379 0.279 1.326 4.345/12.166
[07/27 16:35:00] vclr INFO: Train: [ 31]/[ 250/ 287] BT=10.818/10.684 Loss=12.078 6.185 0.269 1.338 4.286/12.163
[07/27 16:36:43] vclr INFO: Train: [ 31]/[ 260/ 287] BT=10.523/10.670 Loss=11.768 5.680 0.296 1.707 4.085/12.157
[07/27 16:38:26] vclr INFO: Train: [ 31]/[ 270/ 287] BT=10.084/10.657 Loss=11.662 5.512 0.350 1.449 4.350/12.154
[07/27 16:40:08] vclr INFO: Train: [ 31]/[ 280/ 287] BT=10.414/10.641 Loss=11.822 5.530 0.331 1.744 4.217/12.154
[07/27 16:41:16] vclr INFO: epoch 31, total time 3065.09, loss=12.158086966139098
[07/27 16:41:16] vclr INFO: ==> Saving...
[07/27 16:42:58] vclr INFO: Train: [ 32]/[   0/ 287] BT=99.211/99.211 Loss=11.798 5.874 0.304 1.488 4.132/11.798
[07/27 16:44:36] vclr INFO: Train: [ 32]/[  10/ 287] BT=9.726/17.922 Loss=11.717 5.712 0.376 1.478 4.151/12.124
[07/27 16:46:20] vclr INFO: Train: [ 32]/[  20/ 287] BT=9.968/14.367 Loss=12.414 5.888 0.459 1.805 4.262/12.287
[07/27 16:48:03] vclr INFO: Train: [ 32]/[  30/ 287] BT=10.037/13.039 Loss=12.917 6.286 0.212 2.096 4.323/12.486
[07/27 16:49:44] vclr INFO: Train: [ 32]/[  40/ 287] BT=10.149/12.332 Loss=11.634 5.769 0.233 1.160 4.471/12.412
[07/27 16:51:28] vclr INFO: Train: [ 32]/[  50/ 287] BT=10.670/11.948 Loss=12.283 5.953 0.311 1.370 4.650/12.405
[07/27 16:53:11] vclr INFO: Train: [ 32]/[  60/ 287] BT=10.253/11.677 Loss=12.140 5.493 0.402 1.711 4.534/12.387
[07/27 16:54:54] vclr INFO: Train: [ 32]/[  70/ 287] BT=9.985/11.485 Loss=12.270 5.725 0.493 1.731 4.321/12.357
[07/27 16:56:35] vclr INFO: Train: [ 32]/[  80/ 287] BT=9.348/11.314 Loss=11.859 5.937 0.349 1.245 4.328/12.326
[07/27 16:58:15] vclr INFO: Train: [ 32]/[  90/ 287] BT=10.429/11.168 Loss=11.268 5.444 0.388 1.370 4.066/12.305
[07/27 16:59:58] vclr INFO: Train: [ 32]/[ 100/ 287] BT=9.677/11.081 Loss=11.554 5.692 0.381 1.328 4.153/12.284
[07/27 17:01:41] vclr INFO: Train: [ 32]/[ 110/ 287] BT=10.549/11.018 Loss=11.912 5.708 0.597 1.542 4.065/12.281
[07/27 17:03:24] vclr INFO: Train: [ 32]/[ 120/ 287] BT=10.380/10.952 Loss=12.158 6.158 0.500 1.199 4.301/12.252
[07/27 17:04:59] vclr INFO: Train: [ 32]/[ 130/ 287] BT=9.903/10.845 Loss=13.377 6.409 0.305 1.689 4.974/12.250
[07/27 17:06:39] vclr INFO: Train: [ 32]/[ 140/ 287] BT=10.608/10.787 Loss=12.687 6.054 0.493 1.862 4.278/12.242
[07/27 17:08:25] vclr INFO: Train: [ 32]/[ 150/ 287] BT=10.420/10.769 Loss=13.008 6.088 0.423 1.979 4.517/12.236
[07/27 17:10:05] vclr INFO: Train: [ 32]/[ 160/ 287] BT=10.434/10.727 Loss=12.028 5.497 0.490 2.082 3.959/12.241
[07/27 17:11:47] vclr INFO: Train: [ 32]/[ 170/ 287] BT=9.678/10.695 Loss=12.835 5.869 0.226 1.866 4.874/12.259
[07/27 17:13:34] vclr INFO: Train: [ 32]/[ 180/ 287] BT=10.630/10.691 Loss=12.696 5.957 0.614 1.363 4.763/12.264
[07/27 17:15:38] vclr INFO: Train: [ 32]/[ 190/ 287] BT=13.402/10.785 Loss=13.396 6.312 0.518 1.998 4.568/12.268
[07/27 17:17:52] vclr INFO: Train: [ 32]/[ 200/ 287] BT=13.542/10.911 Loss=11.928 5.802 0.314 1.502 4.309/12.259
[07/27 17:20:06] vclr INFO: Train: [ 32]/[ 210/ 287] BT=13.053/11.033 Loss=11.359 5.480 0.452 1.556 3.870/12.241
[07/27 17:22:19] vclr INFO: Train: [ 32]/[ 220/ 287] BT=12.048/11.135 Loss=13.092 6.394 0.201 1.611 4.887/12.237
[07/27 17:24:32] vclr INFO: Train: [ 32]/[ 230/ 287] BT=12.950/11.229 Loss=12.166 5.722 0.522 1.410 4.512/12.224
[07/27 17:26:35] vclr INFO: Train: [ 32]/[ 240/ 287] BT=10.279/11.271 Loss=11.819 5.564 0.485 1.537 4.233/12.221
[07/27 17:28:32] vclr INFO: Train: [ 32]/[ 250/ 287] BT=11.600/11.291 Loss=12.616 6.175 0.511 1.612 4.318/12.221
[07/27 17:30:26] vclr INFO: Train: [ 32]/[ 260/ 287] BT=11.398/11.294 Loss=11.467 5.661 0.282 1.370 4.154/12.205
[07/27 17:32:18] vclr INFO: Train: [ 32]/[ 270/ 287] BT=11.320/11.290 Loss=11.824 6.094 0.201 1.204 4.325/12.201
[07/27 17:34:13] vclr INFO: Train: [ 32]/[ 280/ 287] BT=10.800/11.298 Loss=11.695 5.264 0.621 1.669 4.140/12.193
[07/27 17:35:29] vclr INFO: epoch 32, total time 3252.80, loss=12.193134008799696
[07/27 17:35:29] vclr INFO: ==> Saving...
[07/27 17:37:19] vclr INFO: Train: [ 33]/[   0/ 287] BT=106.578/106.578 Loss=11.768 5.660 0.333 1.594 4.180/11.768
[07/27 17:39:15] vclr INFO: Train: [ 33]/[  10/ 287] BT=12.234/20.198 Loss=12.060 5.386 0.675 1.864 4.135/11.983
[07/27 17:41:08] vclr INFO: Train: [ 33]/[  20/ 287] BT=11.433/15.991 Loss=12.647 6.384 0.381 1.615 4.266/12.174
[07/27 17:43:07] vclr INFO: Train: [ 33]/[  30/ 287] BT=11.990/14.670 Loss=12.298 5.468 0.374 1.946 4.510/12.145
[07/27 17:45:03] vclr INFO: Train: [ 33]/[  40/ 287] BT=10.131/13.921 Loss=11.544 5.687 0.350 1.482 4.025/12.087
[07/27 17:46:57] vclr INFO: Train: [ 33]/[  50/ 287] BT=11.539/13.424 Loss=11.608 5.111 0.665 1.487 4.345/12.057
[07/27 17:48:49] vclr INFO: Train: [ 33]/[  60/ 287] BT=11.259/13.061 Loss=11.308 5.458 0.422 1.252 4.176/12.030
[07/27 17:50:44] vclr INFO: Train: [ 33]/[  70/ 287] BT=11.847/12.843 Loss=12.006 5.791 0.428 1.451 4.336/11.993
[07/27 17:52:36] vclr INFO: Train: [ 33]/[  80/ 287] BT=11.710/12.634 Loss=12.092 5.885 0.312 1.394 4.501/11.996
[07/27 17:54:27] vclr INFO: Train: [ 33]/[  90/ 287] BT=11.758/12.470 Loss=11.591 5.392 0.579 1.709 3.911/12.016
[07/27 17:56:17] vclr INFO: Train: [ 33]/[ 100/ 287] BT=10.422/12.325 Loss=11.287 5.563 0.394 1.284 4.045/12.015
[07/27 17:58:09] vclr INFO: Train: [ 33]/[ 110/ 287] BT=11.550/12.223 Loss=11.902 5.767 0.433 1.663 4.039/12.007
[07/27 18:00:02] vclr INFO: Train: [ 33]/[ 120/ 287] BT=12.172/12.144 Loss=11.023 5.521 0.312 1.200 3.990/11.997
[07/27 18:01:54] vclr INFO: Train: [ 33]/[ 130/ 287] BT=10.316/12.075 Loss=12.383 6.282 0.343 1.368 4.391/11.998
[07/27 18:03:48] vclr INFO: Train: [ 33]/[ 140/ 287] BT=10.971/12.028 Loss=12.127 5.744 0.622 1.320 4.441/11.994
[07/27 18:05:43] vclr INFO: Train: [ 33]/[ 150/ 287] BT=10.508/11.992 Loss=12.713 6.058 0.369 1.993 4.294/12.005
[07/27 18:07:41] vclr INFO: Train: [ 33]/[ 160/ 287] BT=13.055/11.975 Loss=11.432 5.542 0.364 1.581 3.945/12.006
[07/27 18:09:36] vclr INFO: Train: [ 33]/[ 170/ 287] BT=11.541/11.950 Loss=11.704 5.602 0.411 1.437 4.254/11.997
[07/27 18:11:24] vclr INFO: Train: [ 33]/[ 180/ 287] BT=11.364/11.889 Loss=12.198 6.094 0.267 1.441 4.394/12.000
[07/27 18:13:15] vclr INFO: Train: [ 33]/[ 190/ 287] BT=10.793/11.847 Loss=12.306 6.196 0.280 1.532 4.298/12.004
[07/27 18:15:08] vclr INFO: Train: [ 33]/[ 200/ 287] BT=11.266/11.820 Loss=11.463 5.447 0.369 1.547 4.100/12.000
[07/27 18:17:01] vclr INFO: Train: [ 33]/[ 210/ 287] BT=10.680/11.792 Loss=11.591 5.789 0.232 1.430 4.141/12.024
[07/27 18:18:52] vclr INFO: Train: [ 33]/[ 220/ 287] BT=11.376/11.763 Loss=12.581 6.030 0.316 2.109 4.126/12.030
[07/27 18:20:43] vclr INFO: Train: [ 33]/[ 230/ 287] BT=10.463/11.733 Loss=12.911 6.126 0.365 1.746 4.674/12.035
[07/27 18:22:36] vclr INFO: Train: [ 33]/[ 240/ 287] BT=10.053/11.713 Loss=12.392 6.086 0.414 1.696 4.196/12.032
[07/27 18:24:29] vclr INFO: Train: [ 33]/[ 250/ 287] BT=10.771/11.698 Loss=11.933 6.028 0.335 1.355 4.215/12.037
[07/27 18:26:19] vclr INFO: Train: [ 33]/[ 260/ 287] BT=10.219/11.674 Loss=11.741 5.440 0.396 1.942 3.964/12.037
[07/27 18:28:11] vclr INFO: Train: [ 33]/[ 270/ 287] BT=10.009/11.655 Loss=12.630 6.055 0.188 1.643 4.744/12.035
[07/27 18:30:04] vclr INFO: Train: [ 33]/[ 280/ 287] BT=12.257/11.643 Loss=12.704 6.496 0.260 1.416 4.533/12.044
[07/27 18:31:25] vclr INFO: epoch 33, total time 3356.71, loss=12.042991405580102
[07/27 18:31:25] vclr INFO: ==> Saving...
[07/27 18:33:13] vclr INFO: Train: [ 34]/[   0/ 287] BT=103.999/103.999 Loss=11.412 5.327 0.334 1.518 4.234/11.412
[07/27 18:35:07] vclr INFO: Train: [ 34]/[  10/ 287] BT=11.829/19.827 Loss=12.064 6.016 0.389 1.580 4.079/11.892
[07/27 18:36:53] vclr INFO: Train: [ 34]/[  20/ 287] BT=11.754/15.418 Loss=11.738 5.950 0.355 1.229 4.204/11.972
[07/27 18:38:40] vclr INFO: Train: [ 34]/[  30/ 287] BT=12.042/13.905 Loss=11.584 5.423 0.518 1.507 4.136/12.027
[07/27 18:40:35] vclr INFO: Train: [ 34]/[  40/ 287] BT=11.338/13.328 Loss=11.534 5.200 0.316 1.925 4.093/12.021
[07/27 18:42:26] vclr INFO: Train: [ 34]/[  50/ 287] BT=10.732/12.877 Loss=12.069 6.087 0.409 1.270 4.303/12.041
[07/27 18:44:11] vclr INFO: Train: [ 34]/[  60/ 287] BT=10.524/12.499 Loss=12.421 5.543 0.692 1.442 4.743/11.976
[07/27 18:46:04] vclr INFO: Train: [ 34]/[  70/ 287] BT=11.762/12.329 Loss=12.112 5.922 0.309 1.689 4.192/11.966
[07/27 18:47:53] vclr INFO: Train: [ 34]/[  80/ 287] BT=10.474/12.145 Loss=12.304 6.332 0.426 1.513 4.032/11.994
[07/27 18:49:50] vclr INFO: Train: [ 34]/[  90/ 287] BT=12.002/12.096 Loss=11.589 5.601 0.649 1.265 4.074/11.949
[07/27 18:51:55] vclr INFO: Train: [ 34]/[ 100/ 287] BT=12.185/12.145 Loss=11.920 5.748 0.370 1.362 4.439/11.915
[07/27 18:54:08] vclr INFO: Train: [ 34]/[ 110/ 287] BT=12.856/12.246 Loss=12.694 6.385 0.313 1.376 4.620/11.920
[07/27 18:56:14] vclr INFO: Train: [ 34]/[ 120/ 287] BT=13.033/12.276 Loss=12.240 5.853 0.341 1.619 4.427/11.929
[07/27 18:58:20] vclr INFO: Train: [ 34]/[ 130/ 287] BT=12.784/12.301 Loss=11.856 5.476 0.399 1.815 4.166/11.932
[07/27 19:00:25] vclr INFO: Train: [ 34]/[ 140/ 287] BT=13.048/12.314 Loss=12.152 6.378 0.257 1.391 4.126/11.933
[07/27 19:02:38] vclr INFO: Train: [ 34]/[ 150/ 287] BT=12.673/12.376 Loss=12.192 6.029 0.417 1.420 4.327/11.940
[07/27 19:04:26] vclr INFO: Train: [ 34]/[ 160/ 287] BT=8.620/12.282 Loss=11.323 5.205 0.435 1.514 4.169/11.932
[07/27 19:06:08] vclr INFO: Train: [ 34]/[ 170/ 287] BT=7.977/12.160 Loss=12.439 6.171 0.267 1.555 4.446/11.948
[07/27 19:07:40] vclr INFO: Train: [ 34]/[ 180/ 287] BT=8.959/11.995 Loss=12.252 5.867 0.284 1.815 4.286/11.939
[07/27 19:09:12] vclr INFO: Train: [ 34]/[ 190/ 287] BT=10.624/11.849 Loss=11.223 5.479 0.420 1.356 3.969/11.959
[07/27 19:10:56] vclr INFO: Train: [ 34]/[ 200/ 287] BT=10.318/11.776 Loss=11.808 5.702 0.434 1.651 4.020/11.953
[07/27 19:12:30] vclr INFO: Train: [ 34]/[ 210/ 287] BT=10.560/11.666 Loss=12.211 5.812 0.371 1.599 4.429/11.953
[07/27 19:14:03] vclr INFO: Train: [ 34]/[ 220/ 287] BT=9.082/11.558 Loss=12.323 5.818 0.382 1.648 4.474/11.957
[07/27 19:15:34] vclr INFO: Train: [ 34]/[ 230/ 287] BT=9.666/11.451 Loss=11.781 5.857 0.219 1.502 4.203/11.953
[07/27 19:17:10] vclr INFO: Train: [ 34]/[ 240/ 287] BT=8.859/11.374 Loss=11.417 5.573 0.431 1.404 4.010/11.967
[07/27 19:18:44] vclr INFO: Train: [ 34]/[ 250/ 287] BT=9.765/11.294 Loss=11.597 5.818 0.405 1.262 4.112/11.966
[07/27 19:20:13] vclr INFO: Train: [ 34]/[ 260/ 287] BT=8.805/11.206 Loss=12.269 5.885 0.267 1.750 4.367/11.963
[07/27 19:21:44] vclr INFO: Train: [ 34]/[ 270/ 287] BT=9.470/11.126 Loss=12.193 5.922 0.258 1.843 4.170/11.957
[07/27 19:23:16] vclr INFO: Train: [ 34]/[ 280/ 287] BT=8.984/11.057 Loss=12.363 5.410 0.547 1.867 4.539/11.961
[07/27 19:24:31] vclr INFO: epoch 34, total time 3185.79, loss=11.963379976226062
[07/27 19:24:31] vclr INFO: ==> Saving...
[07/27 19:26:23] vclr INFO: Train: [ 35]/[   0/ 287] BT=108.170/108.170 Loss=12.212 6.324 0.292 1.484 4.113/12.212
[07/27 19:27:55] vclr INFO: Train: [ 35]/[  10/ 287] BT=10.060/18.221 Loss=11.182 5.571 0.238 1.435 3.939/11.895
[07/27 19:29:23] vclr INFO: Train: [ 35]/[  20/ 287] BT=7.887/13.750 Loss=11.963 5.563 0.446 1.727 4.227/11.791
[07/27 19:30:52] vclr INFO: Train: [ 35]/[  30/ 287] BT=8.405/12.174 Loss=12.311 6.248 0.344 1.332 4.386/11.805
[07/27 19:32:20] vclr INFO: Train: [ 35]/[  40/ 287] BT=8.039/11.346 Loss=12.845 6.432 0.415 1.573 4.424/11.846
[07/27 19:33:47] vclr INFO: Train: [ 35]/[  50/ 287] BT=8.961/10.829 Loss=12.072 6.262 0.225 1.240 4.345/11.854
[07/27 19:35:14] vclr INFO: Train: [ 35]/[  60/ 287] BT=7.680/10.473 Loss=11.488 5.600 0.318 1.704 3.865/11.885
[07/27 19:36:41] vclr INFO: Train: [ 35]/[  70/ 287] BT=9.500/10.227 Loss=12.166 6.246 0.346 1.365 4.209/11.882
[07/27 19:38:10] vclr INFO: Train: [ 35]/[  80/ 287] BT=8.883/10.065 Loss=11.725 5.692 0.342 1.758 3.934/11.880
[07/27 19:39:39] vclr INFO: Train: [ 35]/[  90/ 287] BT=8.588/9.943 Loss=11.872 5.830 0.275 1.702 4.065/11.876
[07/27 19:41:08] vclr INFO: Train: [ 35]/[ 100/ 287] BT=8.630/9.837 Loss=11.578 5.336 0.794 1.519 3.929/11.857
[07/27 19:42:43] vclr INFO: Train: [ 35]/[ 110/ 287] BT=10.306/9.801 Loss=11.747 5.982 0.249 1.448 4.068/11.872
[07/27 19:44:26] vclr INFO: Train: [ 35]/[ 120/ 287] BT=9.995/9.842 Loss=12.376 6.362 0.219 1.542 4.253/11.891
[07/27 19:45:59] vclr INFO: Train: [ 35]/[ 130/ 287] BT=10.840/9.807 Loss=11.912 5.914 0.461 1.328 4.209/11.903
[07/27 19:47:32] vclr INFO: Train: [ 35]/[ 140/ 287] BT=10.281/9.771 Loss=11.080 5.441 0.478 1.336 3.825/11.896
[07/27 19:49:21] vclr INFO: Train: [ 35]/[ 150/ 287] BT=8.017/9.842 Loss=11.933 5.907 0.395 1.246 4.386/11.895
[07/27 19:51:07] vclr INFO: Train: [ 35]/[ 160/ 287] BT=11.537/9.891 Loss=11.721 5.898 0.468 1.436 3.919/11.910
[07/27 19:53:00] vclr INFO: Train: [ 35]/[ 170/ 287] BT=10.728/9.975 Loss=11.911 5.863 0.371 1.429 4.247/11.923
[07/27 19:54:50] vclr INFO: Train: [ 35]/[ 180/ 287] BT=8.837/10.028 Loss=12.139 6.065 0.247 1.574 4.253/11.917
[07/27 19:56:29] vclr INFO: Train: [ 35]/[ 190/ 287] BT=9.568/10.025 Loss=13.149 6.619 0.263 1.230 5.036/11.932
[07/27 19:58:01] vclr INFO: Train: [ 35]/[ 200/ 287] BT=9.093/9.981 Loss=12.045 5.918 0.428 1.549 4.151/11.949
[07/27 19:59:38] vclr INFO: Train: [ 35]/[ 210/ 287] BT=10.042/9.967 Loss=12.969 6.389 0.512 1.877 4.192/11.947
[07/27 20:01:27] vclr INFO: Train: [ 35]/[ 220/ 287] BT=9.555/10.009 Loss=12.621 6.382 0.346 1.545 4.348/11.950
[07/27 20:03:08] vclr INFO: Train: [ 35]/[ 230/ 287] BT=9.703/10.015 Loss=10.798 4.998 0.383 1.539 3.878/11.945
[07/27 20:04:36] vclr INFO: Train: [ 35]/[ 240/ 287] BT=8.835/9.965 Loss=12.067 6.249 0.222 1.254 4.342/11.944
[07/27 20:06:12] vclr INFO: Train: [ 35]/[ 250/ 287] BT=10.310/9.948 Loss=11.845 5.724 0.341 1.610 4.169/11.951
[07/27 20:07:43] vclr INFO: Train: [ 35]/[ 260/ 287] BT=8.640/9.917 Loss=13.050 6.703 0.223 1.547 4.577/11.954
[07/27 20:09:10] vclr INFO: Train: [ 35]/[ 270/ 287] BT=7.586/9.873 Loss=11.491 5.672 0.413 1.468 3.938/11.952
[07/27 20:10:39] vclr INFO: Train: [ 35]/[ 280/ 287] BT=7.808/9.836 Loss=12.304 6.412 0.201 1.506 4.185/11.949
[07/27 20:11:54] vclr INFO: epoch 35, total time 2843.27, loss=11.952290382119422
[07/27 20:11:54] vclr INFO: ==> Saving...
[07/27 20:14:00] vclr INFO: Train: [ 36]/[   0/ 287] BT=122.296/122.296 Loss=11.738 5.793 0.550 1.235 4.161/11.738
[07/27 20:15:25] vclr INFO: Train: [ 36]/[  10/ 287] BT=8.316/18.866 Loss=11.178 5.213 0.452 1.498 4.016/11.789
[07/27 20:16:53] vclr INFO: Train: [ 36]/[  20/ 287] BT=8.440/14.088 Loss=11.780 5.605 0.387 1.758 4.030/11.788
[07/27 20:18:17] vclr INFO: Train: [ 36]/[  30/ 287] BT=9.597/12.246 Loss=12.677 6.072 0.344 1.697 4.564/11.895
[07/27 20:19:48] vclr INFO: Train: [ 36]/[  40/ 287] BT=8.611/11.469 Loss=11.653 5.596 0.368 1.319 4.370/11.891
[07/27 20:21:13] vclr INFO: Train: [ 36]/[  50/ 287] BT=7.355/10.899 Loss=12.435 6.392 0.303 1.556 4.185/11.938
[07/27 20:22:32] vclr INFO: Train: [ 36]/[  60/ 287] BT=8.686/10.401 Loss=10.941 5.175 0.410 1.492 3.864/11.900
[07/27 20:23:48] vclr INFO: Train: [ 36]/[  70/ 287] BT=7.885/10.010 Loss=12.129 5.936 0.472 1.376 4.344/11.888
[07/27 20:25:04] vclr INFO: Train: [ 36]/[  80/ 287] BT=8.061/9.713 Loss=12.141 6.201 0.216 1.396 4.328/11.897
[07/27 20:26:24] vclr INFO: Train: [ 36]/[  90/ 287] BT=7.444/9.518 Loss=12.088 5.537 0.366 1.969 4.216/11.886
[07/27 20:27:43] vclr INFO: Train: [ 36]/[ 100/ 287] BT=8.098/9.360 Loss=12.006 5.969 0.218 1.643 4.176/11.886
[07/27 20:29:04] vclr INFO: Train: [ 36]/[ 110/ 287] BT=7.692/9.246 Loss=12.075 5.756 0.287 1.825 4.207/11.901
[07/27 20:30:26] vclr INFO: Train: [ 36]/[ 120/ 287] BT=8.724/9.164 Loss=13.036 6.877 0.399 1.321 4.439/11.915
[07/27 20:31:44] vclr INFO: Train: [ 36]/[ 130/ 287] BT=8.148/9.060 Loss=11.973 6.085 0.244 1.553 4.092/11.921
[07/27 20:33:07] vclr INFO: Train: [ 36]/[ 140/ 287] BT=9.850/9.005 Loss=11.408 5.534 0.310 1.648 3.916/11.907
[07/27 20:34:27] vclr INFO: Train: [ 36]/[ 150/ 287] BT=8.091/8.938 Loss=10.937 5.206 0.389 1.454 3.887/11.892
[07/27 20:35:46] vclr INFO: Train: [ 36]/[ 160/ 287] BT=8.078/8.871 Loss=12.429 6.078 0.329 1.438 4.584/11.894
[07/27 20:37:04] vclr INFO: Train: [ 36]/[ 170/ 287] BT=7.711/8.808 Loss=11.742 5.941 0.473 1.383 3.944/11.879
[07/27 20:38:22] vclr INFO: Train: [ 36]/[ 180/ 287] BT=7.525/8.755 Loss=13.205 6.628 0.193 1.751 4.633/11.910
[07/27 20:39:37] vclr INFO: Train: [ 36]/[ 190/ 287] BT=7.534/8.688 Loss=11.884 5.920 0.362 1.449 4.152/11.914
[07/27 20:40:54] vclr INFO: Train: [ 36]/[ 200/ 287] BT=7.436/8.637 Loss=12.221 6.008 0.483 1.447 4.282/11.905
[07/27 20:42:11] vclr INFO: Train: [ 36]/[ 210/ 287] BT=7.907/8.595 Loss=11.471 5.452 0.386 1.612 4.021/11.896
[07/27 20:43:29] vclr INFO: Train: [ 36]/[ 220/ 287] BT=7.532/8.560 Loss=12.378 6.333 0.343 1.342 4.360/11.888
[07/27 20:44:49] vclr INFO: Train: [ 36]/[ 230/ 287] BT=7.311/8.534 Loss=12.891 5.782 0.503 1.927 4.679/11.884
[07/27 20:46:13] vclr INFO: Train: [ 36]/[ 240/ 287] BT=7.622/8.528 Loss=10.794 4.890 0.418 1.734 3.752/11.881
[07/27 20:47:30] vclr INFO: Train: [ 36]/[ 250/ 287] BT=7.568/8.496 Loss=11.206 5.479 0.379 1.660 3.687/11.880
[07/27 20:48:53] vclr INFO: Train: [ 36]/[ 260/ 287] BT=9.993/8.489 Loss=11.664 5.853 0.493 1.231 4.087/11.883
[07/27 20:50:14] vclr INFO: Train: [ 36]/[ 270/ 287] BT=7.369/8.473 Loss=11.787 5.659 0.540 1.719 3.869/11.881
[07/27 20:51:34] vclr INFO: Train: [ 36]/[ 280/ 287] BT=7.788/8.457 Loss=11.257 5.279 0.485 1.733 3.760/11.867
[07/27 20:53:00] vclr INFO: epoch 36, total time 2465.60, loss=11.867742608233195
[07/27 20:53:00] vclr INFO: ==> Saving...
[07/27 20:55:17] vclr INFO: Train: [ 37]/[   0/ 287] BT=131.099/131.099 Loss=11.649 5.719 0.312 1.525 4.094/11.649
[07/27 20:56:38] vclr INFO: Train: [ 37]/[  10/ 287] BT=8.069/19.279 Loss=12.002 6.062 0.463 1.426 4.051/11.486
[07/28 18:50:28] vclr INFO: Full config saved to ./video-contrastive-learning/results\config.json
[07/28 18:50:28] vclr INFO: length of training dataset: 4592
[07/28 18:50:30] vclr INFO: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_inter): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_intra): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
  (fc_order_classifier): Linear(in_features=768, out_features=4, bias=True)
  (fc_tsn): Head(
    (0): Linear(in_features=2048, out_features=2048, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=2048, out_features=128, bias=True)
  )
)
[07/28 18:50:30] vclr INFO: Training
[07/28 18:55:58] vclr INFO: Train: [  1]/[   0/ 287] BT=327.687/327.687 Loss=11.930 5.653 0.625 1.441 4.211/11.930
[07/28 18:56:44] vclr INFO: Train: [  1]/[  10/ 287] BT=4.325/33.961 Loss=10.569 4.859 0.617 1.440 3.653/11.576
[07/28 18:57:25] vclr INFO: Train: [  1]/[  20/ 287] BT=3.668/19.759 Loss=12.289 6.290 0.207 1.479 4.313/11.660
[07/28 18:58:09] vclr INFO: Train: [  1]/[  30/ 287] BT=3.300/14.801 Loss=13.193 6.811 0.415 1.406 4.560/11.644
[07/28 18:58:51] vclr INFO: Train: [  1]/[  40/ 287] BT=4.088/12.198 Loss=10.784 5.502 0.409 1.451 3.422/11.515
[07/28 18:59:29] vclr INFO: Train: [  1]/[  50/ 287] BT=4.201/10.567 Loss=10.561 5.254 0.412 1.284 3.611/11.496
[07/28 19:00:13] vclr INFO: Train: [  1]/[  60/ 287] BT=3.816/9.543 Loss=12.024 5.917 0.412 1.459 4.236/11.578
[07/28 19:00:56] vclr INFO: Train: [  1]/[  70/ 287] BT=4.015/8.809 Loss=12.058 6.026 0.476 1.405 4.151/11.581
[07/28 19:01:37] vclr INFO: Train: [  1]/[  80/ 287] BT=4.450/8.227 Loss=12.069 5.617 0.492 1.481 4.478/11.573
[07/28 19:02:18] vclr INFO: Train: [  1]/[  90/ 287] BT=4.259/7.778 Loss=11.451 5.552 0.503 1.400 3.996/11.615
[07/28 19:03:00] vclr INFO: Train: [  1]/[ 100/ 287] BT=3.941/7.425 Loss=11.864 5.375 0.775 1.418 4.296/11.620
[07/28 19:03:42] vclr INFO: Train: [  1]/[ 110/ 287] BT=3.764/7.128 Loss=11.263 5.403 0.554 1.456 3.850/11.654
[07/28 19:04:22] vclr INFO: Train: [  1]/[ 120/ 287] BT=3.872/6.876 Loss=12.921 6.648 0.418 1.353 4.502/11.715
[07/28 19:05:05] vclr INFO: Train: [  1]/[ 130/ 287] BT=4.817/6.677 Loss=13.045 6.929 0.282 1.275 4.559/11.763
[07/28 19:05:45] vclr INFO: Train: [  1]/[ 140/ 287] BT=3.771/6.489 Loss=12.698 6.591 0.352 1.448 4.307/11.819
[07/28 19:06:26] vclr INFO: Train: [  1]/[ 150/ 287] BT=4.246/6.329 Loss=11.820 5.746 0.543 1.346 4.185/11.865
[07/28 19:07:07] vclr INFO: Train: [  1]/[ 160/ 287] BT=3.733/6.190 Loss=11.196 5.424 0.533 1.396 3.843/11.903
[07/28 19:07:50] vclr INFO: Train: [  1]/[ 170/ 287] BT=4.324/6.078 Loss=12.613 6.411 0.464 1.343 4.396/11.945
[07/28 19:08:32] vclr INFO: Train: [  1]/[ 180/ 287] BT=3.590/5.975 Loss=12.876 6.656 0.438 1.368 4.414/11.985
[07/28 19:09:13] vclr INFO: Train: [  1]/[ 190/ 287] BT=3.979/5.879 Loss=12.186 5.975 0.480 1.430 4.301/12.024
[07/28 19:09:56] vclr INFO: Train: [  1]/[ 200/ 287] BT=4.721/5.800 Loss=12.545 6.506 0.330 1.339 4.370/12.057
[07/28 19:10:50] vclr INFO: Train: [  1]/[ 210/ 287] BT=5.696/5.781 Loss=12.436 6.038 0.562 1.457 4.378/12.086
[07/28 19:11:46] vclr INFO: Train: [  1]/[ 220/ 287] BT=6.005/5.771 Loss=14.202 7.010 0.369 1.565 5.257/12.107
[07/28 19:12:42] vclr INFO: Train: [  1]/[ 230/ 287] BT=5.544/5.763 Loss=13.027 6.366 0.475 1.346 4.841/12.141
[07/28 19:13:43] vclr INFO: Train: [  1]/[ 240/ 287] BT=5.964/5.777 Loss=12.997 6.473 0.406 1.288 4.829/12.177
[07/28 19:14:44] vclr INFO: Train: [  1]/[ 250/ 287] BT=7.068/5.790 Loss=12.958 6.657 0.470 1.410 4.420/12.206
[07/28 19:15:45] vclr INFO: Train: [  1]/[ 260/ 287] BT=6.212/5.801 Loss=13.073 6.412 0.510 1.417 4.735/12.242
[07/28 19:16:36] vclr INFO: Train: [  1]/[ 270/ 287] BT=3.872/5.777 Loss=14.410 7.202 0.509 1.501 5.199/12.281
[07/28 19:17:18] vclr INFO: Train: [  1]/[ 280/ 287] BT=4.083/5.721 Loss=13.596 6.698 0.489 1.431 4.979/12.307
[07/28 19:17:59] vclr INFO: epoch 1, total time 1648.35, loss=12.32064892439892
[07/28 19:17:59] vclr INFO: ==> Saving...
[07/28 19:19:58] vclr INFO: Train: [  2]/[   0/ 287] BT=111.489/111.489 Loss=12.736 6.642 0.387 1.260 4.447/12.736
[07/28 19:20:44] vclr INFO: Train: [  2]/[  10/ 287] BT=4.062/14.345 Loss=13.779 7.256 0.265 1.519 4.739/13.266
